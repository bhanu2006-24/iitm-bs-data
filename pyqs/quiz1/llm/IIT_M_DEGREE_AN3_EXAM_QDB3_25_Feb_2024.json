{
    "component": "Quizpractise/PractiseQuestionPaper",
    "props": {
        "errors": {},
        "auth": {
            "user": null
        },
        "flash": {
            "error": []
        },
        "banner": null,
        "file_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "file_do_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "question_paper": {
            "id": 271,
            "group_id": 13,
            "exam_id": 1,
            "total_score": "1.00",
            "duration": 4,
            "created_at": "2025-07-09T14:38:35.000000Z",
            "updated_at": "2025-07-09T14:38:35.000000Z",
            "question_paper_name": "IIT M DEGREE AN3 EXAM QDB3 25 Feb 2024",
            "question_paper_description": "2024 Feb25: IIT M AN3 EXAM QDB3",
            "uuid": "22d0b861-cfe",
            "year": 2024,
            "is_new": 0,
            "exam": {
                "id": 1,
                "exam_name": "Quiz 1",
                "created_at": "2024-10-16T08:08:51.000000Z",
                "updated_at": "2024-10-16T08:08:51.000000Z",
                "uuid": "9251bc3a-e33e-45e0-bcf0-b16a0ea5b5fa",
                "en_id": "eyJpdiI6InRIOW16Y3k0SzM2Yy9mNWc5QkJNYWc9PSIsInZhbHVlIjoiYXhFSFcyRUdFQncvekE2T3pxaStCdz09IiwibWFjIjoiMmE3NGYwZTkwZGY2NmNmNjQxNDBmODE4NzM0NzA1MTM0Yjk4YWVjOTA5OWRiNDY5MTQxNTcwZDgyZDhmYjA3NyIsInRhZyI6IiJ9"
            },
            "questions": [
                {
                    "id": 86500,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 182,
                    "question_text_1": "THIS IS QUESTION PAPER FOR THE SUBJECT <b>\"DEGREE LEVEL : INTRODUCTION TO LARGE </b>LANGUAGE MODELS (COMPUTER BASED EXAM)\"<b> </b> ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740036,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "0755ad51be5bea623a3a34e5f846330a",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "10c49dd1-94cd-444f-83dc-e796710d5d59",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "THIS IS QUESTION PAPER FOR THE SUBJECT <b>\"DEGREE LEVEL : INTRODUCTION TO LARGE </b>LANGUAGE MODELS (COMPUTER BASED EXAM)\"<b> </b> ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 230810,
                            "question_id": 86500,
                            "option_text": "YES",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475214,
                            "option_image_url": null
                        },
                        {
                            "id": 230811,
                            "question_id": 86500,
                            "option_text": "NO",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475215,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86501,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 183,
                    "question_text_1": null,
                    "question_image_1": "Dhp4wEOLR2CPH5yXypdu8m87ImwViU8NH7k4TtZiggV11KjJKw.png",
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740037,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "879518845badb6e17f750bf488fa62c4",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "971079d7-81cd-4162-996b-f7705f21ce5c",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/Dhp4wEOLR2CPH5yXypdu8m87ImwViU8NH7k4TtZiggV11KjJKw.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 230812,
                            "question_id": 86501,
                            "option_text": "Useful Data has been mentioned above.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475216,
                            "option_image_url": null
                        },
                        {
                            "id": 230813,
                            "question_id": 86501,
                            "option_text": "This data attachment is just for a reference & not for an evaluation.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475217,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86502,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 0,
                    "question_text_1": "  Based on the above data, answer the given subquestions.",
                    "question_image_1": "5grEo96tzuGY0GZ27iCi80t0bQvs3rAWe0M2GfV4J04DmZtcxL.png",
                    "question_type": "COMPREHENSION",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740038,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 0,
                    "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "55faf4ec-5e7e-4886-a36a-4f02e235f3b0",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/5grEo96tzuGY0GZ27iCi80t0bQvs3rAWe0M2GfV4J04DmZtcxL.png"
                    ],
                    "question_texts": [
                        "  Based on the above data, answer the given subquestions."
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 86503,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 184,
                    "question_text_1": null,
                    "question_image_1": "zUvd8JjmuLnMqkkbB7NwnnvUj3aPXsiSh6vcZdLRq37ZveMbAt.png",
                    "question_type": "SA",
                    "total_mark": "3.00",
                    "value_start": "-0.73",
                    "value_end": "-0.63",
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740039,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "db280026989a0964ec6de8b6596b2bc8",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86502,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "16850e4d-c73e-4e31-aa1b-c8238e78e029",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/zUvd8JjmuLnMqkkbB7NwnnvUj3aPXsiSh6vcZdLRq37ZveMbAt.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86502,
                        "exam_id": 1,
                        "question_paper_id": 271,
                        "question_number": 0,
                        "question_text_1": "  Based on the above data, answer the given subquestions.",
                        "question_image_1": "5grEo96tzuGY0GZ27iCi80t0bQvs3rAWe0M2GfV4J04DmZtcxL.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:36.000000Z",
                        "updated_at": "2025-07-09T14:38:36.000000Z",
                        "question_num_long": 640653740038,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": null,
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "55faf4ec-5e7e-4886-a36a-4f02e235f3b0",
                        "question_image_url": [
                            "/question_images/5grEo96tzuGY0GZ27iCi80t0bQvs3rAWe0M2GfV4J04DmZtcxL.png"
                        ],
                        "question_texts": [
                            "  Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86504,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 185,
                    "question_text_1": null,
                    "question_image_1": "QL3WYgw3eC2jumYgnApdZTgUtKjvS3bOYk5eKhATtsHE9XLf9E.png",
                    "question_type": "SA",
                    "total_mark": "2.00",
                    "value_start": "0.13",
                    "value_end": "0.20",
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740040,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "47251b27284ff0fd8465b535b64a7561",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86502,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "cb0e2310-2483-4992-a2e7-6a18d521a627",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/QL3WYgw3eC2jumYgnApdZTgUtKjvS3bOYk5eKhATtsHE9XLf9E.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86502,
                        "exam_id": 1,
                        "question_paper_id": 271,
                        "question_number": 0,
                        "question_text_1": "  Based on the above data, answer the given subquestions.",
                        "question_image_1": "5grEo96tzuGY0GZ27iCi80t0bQvs3rAWe0M2GfV4J04DmZtcxL.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:36.000000Z",
                        "updated_at": "2025-07-09T14:38:36.000000Z",
                        "question_num_long": 640653740038,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": null,
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "55faf4ec-5e7e-4886-a36a-4f02e235f3b0",
                        "question_image_url": [
                            "/question_images/5grEo96tzuGY0GZ27iCi80t0bQvs3rAWe0M2GfV4J04DmZtcxL.png"
                        ],
                        "question_texts": [
                            "  Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86505,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 186,
                    "question_text_1": "  ",
                    "question_image_1": "ydg6r9pvTNhgYmltDH20JohSiG609Xnek9KDM2Zdpoc0Ul6Ikw.png",
                    "question_type": "SA",
                    "total_mark": "3.00",
                    "value_start": "-0.06",
                    "value_end": "-0.035",
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740041,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "1e6f5c6db58c42a34291a8f65c218cc2",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": "  ",
                    "question_image_2": "WZmSXR816zr5jMSmYXATau55BocbMw4MRTpPN7qOsfKujQDvTU.png",
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86502,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "817a7da5-69fe-4ea3-a8e4-bd920bd78cee",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/ydg6r9pvTNhgYmltDH20JohSiG609Xnek9KDM2Zdpoc0Ul6Ikw.png",
                        "/question_images/WZmSXR816zr5jMSmYXATau55BocbMw4MRTpPN7qOsfKujQDvTU.png"
                    ],
                    "question_texts": [
                        "  ",
                        "  "
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86502,
                        "exam_id": 1,
                        "question_paper_id": 271,
                        "question_number": 0,
                        "question_text_1": "  Based on the above data, answer the given subquestions.",
                        "question_image_1": "5grEo96tzuGY0GZ27iCi80t0bQvs3rAWe0M2GfV4J04DmZtcxL.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:36.000000Z",
                        "updated_at": "2025-07-09T14:38:36.000000Z",
                        "question_num_long": 640653740038,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": null,
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "55faf4ec-5e7e-4886-a36a-4f02e235f3b0",
                        "question_image_url": [
                            "/question_images/5grEo96tzuGY0GZ27iCi80t0bQvs3rAWe0M2GfV4J04DmZtcxL.png"
                        ],
                        "question_texts": [
                            "  Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86506,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 187,
                    "question_text_1": "  Which token in the sequence was given the highest score?",
                    "question_image_1": "x6Px8XedTBIy9ACTjg7iWRHEGNXU4ONjbhR5CDd4Ii8R29ltCY.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740042,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "74aac5fb1d155f670e159221f4867322",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86502,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "7a81e15f-c631-4cff-9052-6654d18ccd32",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/x6Px8XedTBIy9ACTjg7iWRHEGNXU4ONjbhR5CDd4Ii8R29ltCY.png"
                    ],
                    "question_texts": [
                        "  Which token in the sequence was given the highest score?"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 230814,
                            "question_id": 86506,
                            "option_text": "G",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475221,
                            "option_image_url": null
                        },
                        {
                            "id": 230815,
                            "question_id": 86506,
                            "option_text": "C",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475222,
                            "option_image_url": null
                        },
                        {
                            "id": 230816,
                            "question_id": 86506,
                            "option_text": "T",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475223,
                            "option_image_url": null
                        },
                        {
                            "id": 230817,
                            "question_id": 86506,
                            "option_text": "A",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475224,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": {
                        "id": 86502,
                        "exam_id": 1,
                        "question_paper_id": 271,
                        "question_number": 0,
                        "question_text_1": "  Based on the above data, answer the given subquestions.",
                        "question_image_1": "5grEo96tzuGY0GZ27iCi80t0bQvs3rAWe0M2GfV4J04DmZtcxL.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:36.000000Z",
                        "updated_at": "2025-07-09T14:38:36.000000Z",
                        "question_num_long": 640653740038,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": null,
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "55faf4ec-5e7e-4886-a36a-4f02e235f3b0",
                        "question_image_url": [
                            "/question_images/5grEo96tzuGY0GZ27iCi80t0bQvs3rAWe0M2GfV4J04DmZtcxL.png"
                        ],
                        "question_texts": [
                            "  Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86507,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 0,
                    "question_text_1": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                    "question_image_1": "U2XSJEjuNZPo0DR7mRIOgwHRtLZy5lNWSYfYOmaDPrgs3XWiDT.png",
                    "question_type": "COMPREHENSION",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740043,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 0,
                    "hash": "b4f53b55bd4053fdbc148717005f92dc",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions.",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "e95184ae-39b2-44ad-895c-86b1f11b1a29",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/U2XSJEjuNZPo0DR7mRIOgwHRtLZy5lNWSYfYOmaDPrgs3XWiDT.png"
                    ],
                    "question_texts": [
                        "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                        "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions."
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 86508,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 188,
                    "question_text_1": "Suppose the number of learnable parameters in the source input embedding layer is 1600, how many parameters are there in the positional embedding layer of the source language? Assume the positional embeddings are learnable.",
                    "question_image_1": null,
                    "question_type": "SA",
                    "total_mark": "3.00",
                    "value_start": "512",
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740044,
                    "answer_type": "Equal",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "fbdef4442a06712234387dc3c8a0863e",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86507,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "91c5e518-a5eb-4447-88b2-4381c17b3f87",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Suppose the number of learnable parameters in the source input embedding layer is 1600, how many parameters are there in the positional embedding layer of the source language? Assume the positional embeddings are learnable."
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86507,
                        "exam_id": 1,
                        "question_paper_id": 271,
                        "question_number": 0,
                        "question_text_1": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                        "question_image_1": "U2XSJEjuNZPo0DR7mRIOgwHRtLZy5lNWSYfYOmaDPrgs3XWiDT.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:36.000000Z",
                        "updated_at": "2025-07-09T14:38:36.000000Z",
                        "question_num_long": 640653740043,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "b4f53b55bd4053fdbc148717005f92dc",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions.",
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "e95184ae-39b2-44ad-895c-86b1f11b1a29",
                        "question_image_url": [
                            "/question_images/U2XSJEjuNZPo0DR7mRIOgwHRtLZy5lNWSYfYOmaDPrgs3XWiDT.png"
                        ],
                        "question_texts": [
                            "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                            "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86509,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 189,
                    "question_text_1": "How many parameters are there in the multi-head attention layer of the encoder (exclude the parameters in the WO matrix used for linear transformation)?",
                    "question_image_1": null,
                    "question_type": "SA",
                    "total_mark": "3.00",
                    "value_start": "768",
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740045,
                    "answer_type": "Equal",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "db0aba9af3053fffa483e446c414d77e",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86507,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "d7b5e9c2-8058-4b2b-bdec-4c40a7f3ef69",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "How many parameters are there in the multi-head attention layer of the encoder (exclude the parameters in the WO matrix used for linear transformation)?"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86507,
                        "exam_id": 1,
                        "question_paper_id": 271,
                        "question_number": 0,
                        "question_text_1": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                        "question_image_1": "U2XSJEjuNZPo0DR7mRIOgwHRtLZy5lNWSYfYOmaDPrgs3XWiDT.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:36.000000Z",
                        "updated_at": "2025-07-09T14:38:36.000000Z",
                        "question_num_long": 640653740043,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "b4f53b55bd4053fdbc148717005f92dc",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions.",
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "e95184ae-39b2-44ad-895c-86b1f11b1a29",
                        "question_image_url": [
                            "/question_images/U2XSJEjuNZPo0DR7mRIOgwHRtLZy5lNWSYfYOmaDPrgs3XWiDT.png"
                        ],
                        "question_texts": [
                            "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                            "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86510,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 190,
                    "question_text_1": null,
                    "question_image_1": "5Qfv06T57wWENGdH8BuPLgZEiwKTJZwINYIe1PicUEeZENs1iR.png",
                    "question_type": "MCQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740046,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "4e1dcad3ae0fbcca9e350d40c8e59354",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "1b61b9a0-b087-44b1-ba60-cddabc5b3683",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/5Qfv06T57wWENGdH8BuPLgZEiwKTJZwINYIe1PicUEeZENs1iR.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 230818,
                            "question_id": 86510,
                            "option_text": "",
                            "option_image": "Eu4r711ATxZWPxsaVPl7Iqqo0cu5YSYU0vjashQybwhellDuuF.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475227,
                            "option_image_url": "app/option_images/Eu4r711ATxZWPxsaVPl7Iqqo0cu5YSYU0vjashQybwhellDuuF.png"
                        },
                        {
                            "id": 230819,
                            "question_id": 86510,
                            "option_text": "",
                            "option_image": "yBMLTpbYTdlmWnKwGvmoB9fAFaTSM2J1Bz26lPhhu69Z297uKv.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475228,
                            "option_image_url": "app/option_images/yBMLTpbYTdlmWnKwGvmoB9fAFaTSM2J1Bz26lPhhu69Z297uKv.png"
                        },
                        {
                            "id": 230820,
                            "question_id": 86510,
                            "option_text": "",
                            "option_image": "pVHtvT873E47x2bVAnFGcp8gjopStLoMuE3UtZXL2AWOQHo5d2.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475229,
                            "option_image_url": "app/option_images/pVHtvT873E47x2bVAnFGcp8gjopStLoMuE3UtZXL2AWOQHo5d2.png"
                        },
                        {
                            "id": 230821,
                            "question_id": 86510,
                            "option_text": "",
                            "option_image": "jrjeAC9Pi6Bbc1PcPDezpyHPEkFCBIhjnWy5mtlD4ATCQ7V1jH.png",
                            "score": "4.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475230,
                            "option_image_url": "app/option_images/jrjeAC9Pi6Bbc1PcPDezpyHPEkFCBIhjnWy5mtlD4ATCQ7V1jH.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86511,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 191,
                    "question_text_1": "Suppose we run the pre-trained GPT model in an autoregressive fashion for generating a text sequence. Then, the statement that \u201c We do not need to add the positional embedding to the predicted tokens at every time step\u201d is",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740052,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "ec668461cafeb3a93dad8f1c92fed4ca",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "c8e276c9-6772-4315-ac03-62e2ee416fe5",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Suppose we run the pre-trained GPT model in an autoregressive fashion for generating a text sequence. Then, the statement that \u201c We do not need to add the positional embedding to the predicted tokens at every time step\u201d is"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 230822,
                            "question_id": 86511,
                            "option_text": "TRUE",
                            "option_image": "",
                            "score": "4.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475243,
                            "option_image_url": null
                        },
                        {
                            "id": 230823,
                            "question_id": 86511,
                            "option_text": "FALSE",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475244,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86512,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 192,
                    "question_text_1": "Suppose we have a dataset for machine translation tasks with thousands of samples. Suppose a team considers training the transformer model. The model could be trained in two approaches  A\u00a0\u00a0Autoregressive training B\u00a0\u00a0Teacher forcing  Choose the correct statements",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740047,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "14a4f2d99a97c32710b4bd1a61d324d9",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "519c82fd-610b-40da-9971-4667bcc2f72b",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Suppose we have a dataset for machine translation tasks with thousands of samples. Suppose a team considers training the transformer model. The model could be trained in two approaches  A\u00a0\u00a0Autoregressive training B\u00a0\u00a0Teacher forcing  Choose the correct statements"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 230824,
                            "question_id": 86512,
                            "option_text": "Approach A helps the model to converge faster than approach B",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475231,
                            "option_image_url": null
                        },
                        {
                            "id": 230825,
                            "question_id": 86512,
                            "option_text": "Approach B helps the model converge faster than approach A",
                            "option_image": "",
                            "score": "1.500",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475232,
                            "option_image_url": null
                        },
                        {
                            "id": 230826,
                            "question_id": 86512,
                            "option_text": "One can start the training with approach B first and then switch to approach Aafter some training steps",
                            "option_image": "",
                            "score": "1.500",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475233,
                            "option_image_url": null
                        },
                        {
                            "id": 230827,
                            "question_id": 86512,
                            "option_text": "Once the training starts with approach B and then switching to approach Aafter some training steps can not be done",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475234,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86513,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 193,
                    "question_text_1": null,
                    "question_image_1": "cwudJtwrnfldtUdoYMv9qEYr0WU7UWHb7wuWUxVuleYzEI7tns.png",
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740048,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "00547d1b3306051b6ea045fe95b522ac",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "8c5d3818-a054-4ca9-9c54-15ea3947bb5a",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/cwudJtwrnfldtUdoYMv9qEYr0WU7UWHb7wuWUxVuleYzEI7tns.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 230828,
                            "question_id": 86513,
                            "option_text": "512",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475235,
                            "option_image_url": null
                        },
                        {
                            "id": 230829,
                            "question_id": 86513,
                            "option_text": "1024",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475236,
                            "option_image_url": null
                        },
                        {
                            "id": 230830,
                            "question_id": 86513,
                            "option_text": "1536",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475237,
                            "option_image_url": null
                        },
                        {
                            "id": 230831,
                            "question_id": 86513,
                            "option_text": "2048",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475238,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86514,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 194,
                    "question_text_1": "Choose the correct statements",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740053,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "c81e5b7c036479d23a47ad2af2a15160",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "16d4d4d8-b7d1-4aa3-9bee-17933d160c57",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Choose the correct statements"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 230832,
                            "question_id": 86514,
                            "option_text": "All the parameters of the GPT model are randomly initialized during pre-training",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475245,
                            "option_image_url": null
                        },
                        {
                            "id": 230833,
                            "question_id": 86514,
                            "option_text": "The model minimizes the CLM objective during fine-tuning to improve theperformance",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475246,
                            "option_image_url": null
                        },
                        {
                            "id": 230834,
                            "question_id": 86514,
                            "option_text": "All parameters of the GPT model are randomly initialized during fine-tuning",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475247,
                            "option_image_url": null
                        },
                        {
                            "id": 230835,
                            "question_id": 86514,
                            "option_text": "In general, fine-tuning requires a dataset with labels",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475248,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86515,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 195,
                    "question_text_1": null,
                    "question_image_1": "OzWPaMcgw5SMsjlzQN8SEtwZTvYITkOY1cYPRoHVqHRpB0LT2c.png",
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740054,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "17ec149c7e8e1d1e40792fbc1f17db14",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "18abb7d3-ceea-495f-aa66-ca47acdc8cf3",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/OzWPaMcgw5SMsjlzQN8SEtwZTvYITkOY1cYPRoHVqHRpB0LT2c.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 230836,
                            "question_id": 86515,
                            "option_text": "Top-k sampling",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475249,
                            "option_image_url": null
                        },
                        {
                            "id": 230837,
                            "question_id": 86515,
                            "option_text": "Beam search with the beam size K = 2",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475250,
                            "option_image_url": null
                        },
                        {
                            "id": 230838,
                            "question_id": 86515,
                            "option_text": "Top-p sampling",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475251,
                            "option_image_url": null
                        },
                        {
                            "id": 230839,
                            "question_id": 86515,
                            "option_text": "Exhaustive search",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475252,
                            "option_image_url": null
                        },
                        {
                            "id": 230840,
                            "question_id": 86515,
                            "option_text": "None of these",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475253,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86516,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 0,
                    "question_text_1": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                    "question_image_1": "hEWDmErtXmgOwEK2eS1Yo6U8f28190z6GjHk0L8IODGSuI8DUN.png",
                    "question_type": "COMPREHENSION",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740049,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 0,
                    "hash": "791e0ce80e66c4f270c3568b2953163a",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions.",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "55cb8b02-0b1b-4fdf-9e6b-b0b26e9d027e",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/hEWDmErtXmgOwEK2eS1Yo6U8f28190z6GjHk0L8IODGSuI8DUN.png"
                    ],
                    "question_texts": [
                        "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                        "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions."
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 86517,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 196,
                    "question_text_1": "The attention score matrix given in Table 1 is appropriate for the causal language modelling task",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740050,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "21d94beb8e9d646be00b756861f076c8",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86516,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "fa85eb65-00ec-4fb0-a47a-d44ae47434ed",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "The attention score matrix given in Table 1 is appropriate for the causal language modelling task"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 230841,
                            "question_id": 86517,
                            "option_text": "True",
                            "option_image": "",
                            "score": "4.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475239,
                            "option_image_url": null
                        },
                        {
                            "id": 230842,
                            "question_id": 86517,
                            "option_text": "False",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475240,
                            "option_image_url": null
                        },
                        {
                            "id": 230843,
                            "question_id": 86517,
                            "option_text": "Insufficient information",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475241,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": {
                        "id": 86516,
                        "exam_id": 1,
                        "question_paper_id": 271,
                        "question_number": 0,
                        "question_text_1": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                        "question_image_1": "hEWDmErtXmgOwEK2eS1Yo6U8f28190z6GjHk0L8IODGSuI8DUN.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:36.000000Z",
                        "updated_at": "2025-07-09T14:38:36.000000Z",
                        "question_num_long": 640653740049,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "791e0ce80e66c4f270c3568b2953163a",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions.",
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "55cb8b02-0b1b-4fdf-9e6b-b0b26e9d027e",
                        "question_image_url": [
                            "/question_images/hEWDmErtXmgOwEK2eS1Yo6U8f28190z6GjHk0L8IODGSuI8DUN.png"
                        ],
                        "question_texts": [
                            "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                            "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86518,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 197,
                    "question_text_1": "Assume the time step starts from t = 0 and ends at t = 6. Suppose the model is at time step t = 4, what is the attention value assigned for the word \u201cis\u201d?",
                    "question_image_1": null,
                    "question_type": "SA",
                    "total_mark": "4.00",
                    "value_start": "0",
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740051,
                    "answer_type": "Equal",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "27ab2ee264aa09f00a8c213026d4fb47",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86516,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "b7be9b1c-06e6-46b1-a80d-48d039a64f1d",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Assume the time step starts from t = 0 and ends at t = 6. Suppose the model is at time step t = 4, what is the attention value assigned for the word \u201cis\u201d?"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86516,
                        "exam_id": 1,
                        "question_paper_id": 271,
                        "question_number": 0,
                        "question_text_1": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                        "question_image_1": "hEWDmErtXmgOwEK2eS1Yo6U8f28190z6GjHk0L8IODGSuI8DUN.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:36.000000Z",
                        "updated_at": "2025-07-09T14:38:36.000000Z",
                        "question_num_long": 640653740049,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "791e0ce80e66c4f270c3568b2953163a",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions.",
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "55cb8b02-0b1b-4fdf-9e6b-b0b26e9d027e",
                        "question_image_url": [
                            "/question_images/hEWDmErtXmgOwEK2eS1Yo6U8f28190z6GjHk0L8IODGSuI8DUN.png"
                        ],
                        "question_texts": [
                            "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                            "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86519,
                    "exam_id": 1,
                    "question_paper_id": 271,
                    "question_number": 198,
                    "question_text_1": "The statement that \u201cthe Next Sentence Prediction (NSP) task requires the BERT model to run autoregressively given the first sentence( or segment)\u201d is",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:36.000000Z",
                    "updated_at": "2025-07-09T14:38:36.000000Z",
                    "question_num_long": 640653740055,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "1bc44d48f8638d8cd65a97cb58ae75b6",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "7d214d34-2b7c-450a-a369-37db040106f9",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "The statement that \u201cthe Next Sentence Prediction (NSP) task requires the BERT model to run autoregressively given the first sentence( or segment)\u201d is"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 230844,
                            "question_id": 86519,
                            "option_text": "TRUE",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475254,
                            "option_image_url": null
                        },
                        {
                            "id": 230845,
                            "question_id": 86519,
                            "option_text": "FALSE",
                            "option_image": "",
                            "score": "3.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:36.000000Z",
                            "updated_at": "2025-07-09T14:38:36.000000Z",
                            "option_number": 6406532475255,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                }
            ]
        },
        "summary": "### Summary of Core Topics, Concepts, Principles, and Equations\n\n#### Core Topics and Concepts:\n1. **Large Language Models (LLMs)**\n   - Introduction to LLMs\n   - Transformer Architecture\n   - Encoder and Decoder Layers\n   - Attention Mechanisms\n\n2. **Embeddings and Positional Encoding**\n   - Vocabulary and Embedding Matrices\n   - Positional Encoding Schemes\n   - Sinusoidal Positional Encoding\n\n3. **Attention Mechanisms**\n   - Self-Attention\n   - Multi-Head Attention\n   - Cross-Attention\n   - Attention Scores and Weights\n\n4. **Transformer Configurations**\n   - Number of Heads\n   - Model Dimensions (\\(d_{model}\\), \\(d_{ff}\\), \\(d_q\\), \\(d_k\\), \\(d_v\\))\n   - Context Window Length\n\n5. **Training and Decoding Strategies**\n   - Autoregressive Training\n   - Teacher Forcing\n   - Top-k Sampling\n   - Beam Search\n   - Top-p Sampling\n   - Exhaustive Search\n\n6. **Model Initialization and Fine-Tuning**\n   - Parameter Initialization\n   - Fine-Tuning with Labeled Data\n\n7. **Special Tokens and Context Length**\n   - Special Tokens ([BOS], [EOS])\n   - Context Length and Attention Matrices\n\n#### Important Equations:\n1. **Positional Encoding:**\n   \\[\n   P(pos, 2i) = \\sin\\left(\\frac{pos}{10^{(2i/d_{model})}}\\right)\n   \\]\n   \\[\n   P(pos, 2i + 1) = \\cos\\left(\\frac{pos}{10^{(2i/d_{model})}}\\right)\n   \\]\n\n2. **Attention Mechanism:**\n   \\[\n   \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n   \\]\n\n3. **Model Dimensions:**\n   \\[\n   d_{ff} = 4 \\times d_{model}\n   \\]\n   \\[\n   d_q = d_k = d_v = \\frac{d_{model}}{n_h}\n   \\]\n\n#### Knowledge Graph :\n\n```mermaid\ngraph TD;\n    A[Large Language Models] --> B[Transformer Architecture]\n    B --> C[Encoder and Decoder Layers]\n    B --> D[Attention Mechanisms]\n    C --> E[Embeddings and Positional Encoding]\n    D --> F[Self-Attention]\n    D --> G[Multi-Head Attention]\n    D --> H[Cross-Attention]\n    E --> I[Vocabulary and Embedding Matrices]\n    E --> J[Positional Encoding Schemes]\n    A --> K[Training and Decoding Strategies]\n    K --> L[Autoregressive Training]\n    K --> M[Teacher Forcing]\n    K --> N[Top-k Sampling]\n    K --> O[Beam Search]\n    K --> P[Top-p Sampling]\n    K --> Q[Exhaustive Search]\n    A --> R[Model Initialization and Fine-Tuning]\n    R --> S[Parameter Initialization]\n    R --> T[Fine-Tuning with Labeled Data]\n    A --> U[Special Tokens and Context Length]\n    U --> V[Special Tokens]\n    U --> W[Context Length and Attention Matrices]\n```\n\nThis knowledge graph visually maps the relationships between the core topics and concepts essential for the exam. The graph is structured to show the hierarchical relationships and dependencies between different topics.",
        "total_score": "50.00"
    },
    "url": "/question-paper/practise/85/22d0b861-cfe",
    "version": "ee3d5d44299e610bd137ea6200db5ac2"
}