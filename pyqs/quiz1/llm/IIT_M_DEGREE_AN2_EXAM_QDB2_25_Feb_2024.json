{
    "component": "Quizpractise/PractiseQuestionPaper",
    "props": {
        "errors": {},
        "auth": {
            "user": null
        },
        "flash": {
            "error": []
        },
        "banner": null,
        "file_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "file_do_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "question_paper": {
            "id": 269,
            "group_id": 13,
            "exam_id": 1,
            "total_score": "1.00",
            "duration": 4,
            "created_at": "2025-07-09T14:38:22.000000Z",
            "updated_at": "2025-07-09T14:38:22.000000Z",
            "question_paper_name": "IIT M DEGREE AN2 EXAM QDB2 25 Feb 2024",
            "question_paper_description": "2024 Feb25: IIT M AN2 EXAM QDB2",
            "uuid": "abae87da-b20",
            "year": 2024,
            "is_new": 0,
            "exam": {
                "id": 1,
                "exam_name": "Quiz 1",
                "created_at": "2024-10-16T08:08:51.000000Z",
                "updated_at": "2024-10-16T08:08:51.000000Z",
                "uuid": "9251bc3a-e33e-45e0-bcf0-b16a0ea5b5fa",
                "en_id": "eyJpdiI6InhOOGdrVzdiaS83dG9xU0x3NGxxWGc9PSIsInZhbHVlIjoiRHhzWHplVndqT0h3bkJTSGJxcEJ2Zz09IiwibWFjIjoiNjYyZTJhNmJmNjg3ZDZiMmM5MWRkYTAyZDI2MjcxZmVkYWIxZjI2MzE4MDY2NWFmYjhhNTU2YzdiMjMzNTQ2YiIsInRhZyI6IiJ9"
            },
            "questions": [
                {
                    "id": 86011,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 182,
                    "question_text_1": "THIS IS QUESTION PAPER FOR THE SUBJECT <b>\"DEGREE LEVEL : INTRODUCTION TO LARGE </b>LANGUAGE MODELS (COMPUTER BASED EXAM)\"<b> </b> ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739754,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "6a7fbcb94da9b50c68c0f470a31bbc56",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "7241377e-fc79-4da5-a9d3-5ffe5426551f",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "THIS IS QUESTION PAPER FOR THE SUBJECT <b>\"DEGREE LEVEL : INTRODUCTION TO LARGE </b>LANGUAGE MODELS (COMPUTER BASED EXAM)\"<b> </b> ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 229723,
                            "question_id": 86011,
                            "option_text": "YES",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474501,
                            "option_image_url": null
                        },
                        {
                            "id": 229724,
                            "question_id": 86011,
                            "option_text": "NO",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474502,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86012,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 183,
                    "question_text_1": null,
                    "question_image_1": "QRyUfRVmHAyD7TzQTTUYHOrQjCe8xs5NIiaLE7ONp9ci8No1nt.png",
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739755,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "aac768536057e8d937d778cbf52a303b",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "75303909-c0ac-45a9-94b2-842df0327625",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/QRyUfRVmHAyD7TzQTTUYHOrQjCe8xs5NIiaLE7ONp9ci8No1nt.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 229725,
                            "question_id": 86012,
                            "option_text": "Useful Data has been mentioned above.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474503,
                            "option_image_url": null
                        },
                        {
                            "id": 229726,
                            "question_id": 86012,
                            "option_text": "This data attachment is just for a reference & not for an evaluation.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474504,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86013,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 0,
                    "question_text_1": "  Based on the above data, answer the given subquestions.",
                    "question_image_1": "F6wRaDjuLiFoORD3w36vu2bZDOvNLiIjzo0kf71XPFXdpWeAMs.png",
                    "question_type": "COMPREHENSION",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739756,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 0,
                    "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "a30a20aa-c24e-465a-b599-c7b3ed621db4",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/F6wRaDjuLiFoORD3w36vu2bZDOvNLiIjzo0kf71XPFXdpWeAMs.png"
                    ],
                    "question_texts": [
                        "  Based on the above data, answer the given subquestions."
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 86014,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 184,
                    "question_text_1": null,
                    "question_image_1": "wnu0bjJZP66GNXe0LEl5xbPlWgRYhOsMkjPf0ZfahpCveB9VJf.png",
                    "question_type": "SA",
                    "total_mark": "3.00",
                    "value_start": "-0.73",
                    "value_end": "-0.63",
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739757,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "db280026989a0964ec6de8b6596b2bc8",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86013,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "fd6d6ece-5395-4e12-99fd-60f97d6cd5f8",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/wnu0bjJZP66GNXe0LEl5xbPlWgRYhOsMkjPf0ZfahpCveB9VJf.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86013,
                        "exam_id": 1,
                        "question_paper_id": 269,
                        "question_number": 0,
                        "question_text_1": "  Based on the above data, answer the given subquestions.",
                        "question_image_1": "F6wRaDjuLiFoORD3w36vu2bZDOvNLiIjzo0kf71XPFXdpWeAMs.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:23.000000Z",
                        "updated_at": "2025-07-09T14:38:23.000000Z",
                        "question_num_long": 640653739756,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": null,
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "a30a20aa-c24e-465a-b599-c7b3ed621db4",
                        "question_image_url": [
                            "/question_images/F6wRaDjuLiFoORD3w36vu2bZDOvNLiIjzo0kf71XPFXdpWeAMs.png"
                        ],
                        "question_texts": [
                            "  Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86015,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 185,
                    "question_text_1": null,
                    "question_image_1": "WIcs4L41iyw0TI2Tll7RgPFmjqkUNo4Y4W1DF1lzeSlhXUceSN.png",
                    "question_type": "SA",
                    "total_mark": "2.00",
                    "value_start": "0.13",
                    "value_end": "0.20",
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739758,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "47251b27284ff0fd8465b535b64a7561",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86013,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "a2d0ed8e-06a9-4ad9-a0da-7f1f018eaf8f",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/WIcs4L41iyw0TI2Tll7RgPFmjqkUNo4Y4W1DF1lzeSlhXUceSN.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86013,
                        "exam_id": 1,
                        "question_paper_id": 269,
                        "question_number": 0,
                        "question_text_1": "  Based on the above data, answer the given subquestions.",
                        "question_image_1": "F6wRaDjuLiFoORD3w36vu2bZDOvNLiIjzo0kf71XPFXdpWeAMs.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:23.000000Z",
                        "updated_at": "2025-07-09T14:38:23.000000Z",
                        "question_num_long": 640653739756,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": null,
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "a30a20aa-c24e-465a-b599-c7b3ed621db4",
                        "question_image_url": [
                            "/question_images/F6wRaDjuLiFoORD3w36vu2bZDOvNLiIjzo0kf71XPFXdpWeAMs.png"
                        ],
                        "question_texts": [
                            "  Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86016,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 186,
                    "question_text_1": "  ",
                    "question_image_1": "8OiVYUkd443FsfECGslXjqvWoGj3bJT3EBgMhsy0SK0NVwAYEM.png",
                    "question_type": "SA",
                    "total_mark": "3.00",
                    "value_start": "-0.06",
                    "value_end": "-0.035",
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739759,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "1e6f5c6db58c42a34291a8f65c218cc2",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": "  ",
                    "question_image_2": "lzJzpOmhh4bcczJ7V4DRMIMpiXIRZbKtWwhhAVhkoSZTQzdJRv.png",
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86013,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "10c21345-78dc-407a-ba96-87ec1a7dc3cc",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/8OiVYUkd443FsfECGslXjqvWoGj3bJT3EBgMhsy0SK0NVwAYEM.png",
                        "/question_images/lzJzpOmhh4bcczJ7V4DRMIMpiXIRZbKtWwhhAVhkoSZTQzdJRv.png"
                    ],
                    "question_texts": [
                        "  ",
                        "  "
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86013,
                        "exam_id": 1,
                        "question_paper_id": 269,
                        "question_number": 0,
                        "question_text_1": "  Based on the above data, answer the given subquestions.",
                        "question_image_1": "F6wRaDjuLiFoORD3w36vu2bZDOvNLiIjzo0kf71XPFXdpWeAMs.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:23.000000Z",
                        "updated_at": "2025-07-09T14:38:23.000000Z",
                        "question_num_long": 640653739756,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": null,
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "a30a20aa-c24e-465a-b599-c7b3ed621db4",
                        "question_image_url": [
                            "/question_images/F6wRaDjuLiFoORD3w36vu2bZDOvNLiIjzo0kf71XPFXdpWeAMs.png"
                        ],
                        "question_texts": [
                            "  Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86017,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 187,
                    "question_text_1": "  Which token in the sequence was given the highest score?",
                    "question_image_1": "kZCwmCUJggW63iviCmZgPrcDruspJr9S9v7bvIuat8PYf3nAeK.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739760,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "bc0b137827282bb266ba08f92d972529",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86013,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "c6e7b26a-6439-4f3c-8115-53b7120171eb",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/kZCwmCUJggW63iviCmZgPrcDruspJr9S9v7bvIuat8PYf3nAeK.png"
                    ],
                    "question_texts": [
                        "  Which token in the sequence was given the highest score?"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 229727,
                            "question_id": 86017,
                            "option_text": "G",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474508,
                            "option_image_url": null
                        },
                        {
                            "id": 229728,
                            "question_id": 86017,
                            "option_text": "C",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474509,
                            "option_image_url": null
                        },
                        {
                            "id": 229729,
                            "question_id": 86017,
                            "option_text": "T",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474510,
                            "option_image_url": null
                        },
                        {
                            "id": 229730,
                            "question_id": 86017,
                            "option_text": "A",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474511,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": {
                        "id": 86013,
                        "exam_id": 1,
                        "question_paper_id": 269,
                        "question_number": 0,
                        "question_text_1": "  Based on the above data, answer the given subquestions.",
                        "question_image_1": "F6wRaDjuLiFoORD3w36vu2bZDOvNLiIjzo0kf71XPFXdpWeAMs.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:23.000000Z",
                        "updated_at": "2025-07-09T14:38:23.000000Z",
                        "question_num_long": 640653739756,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": null,
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "a30a20aa-c24e-465a-b599-c7b3ed621db4",
                        "question_image_url": [
                            "/question_images/F6wRaDjuLiFoORD3w36vu2bZDOvNLiIjzo0kf71XPFXdpWeAMs.png"
                        ],
                        "question_texts": [
                            "  Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86018,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 0,
                    "question_text_1": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                    "question_image_1": "LXV7r3DewFE6draDo2dnCBHYDxKZoCLohhRtqFPsBGmwVLyVZE.png",
                    "question_type": "COMPREHENSION",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739761,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 0,
                    "hash": "b4f53b55bd4053fdbc148717005f92dc",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions.",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "fd62fc8c-4b14-472f-8f5b-97cbb51f1828",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/LXV7r3DewFE6draDo2dnCBHYDxKZoCLohhRtqFPsBGmwVLyVZE.png"
                    ],
                    "question_texts": [
                        "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                        "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions."
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 86019,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 188,
                    "question_text_1": "Suppose the number of learnable parameters in the source input embedding layer is 1600, how many parameters are there in the positional embedding layer of the source language? Assume the positional embeddings are learnable.",
                    "question_image_1": null,
                    "question_type": "SA",
                    "total_mark": "3.00",
                    "value_start": "512",
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739762,
                    "answer_type": "Equal",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "fbdef4442a06712234387dc3c8a0863e",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86018,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "3b2024ff-4915-4187-bfeb-d5d1e8fb4bef",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Suppose the number of learnable parameters in the source input embedding layer is 1600, how many parameters are there in the positional embedding layer of the source language? Assume the positional embeddings are learnable."
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86018,
                        "exam_id": 1,
                        "question_paper_id": 269,
                        "question_number": 0,
                        "question_text_1": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                        "question_image_1": "LXV7r3DewFE6draDo2dnCBHYDxKZoCLohhRtqFPsBGmwVLyVZE.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:23.000000Z",
                        "updated_at": "2025-07-09T14:38:23.000000Z",
                        "question_num_long": 640653739761,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "b4f53b55bd4053fdbc148717005f92dc",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions.",
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "fd62fc8c-4b14-472f-8f5b-97cbb51f1828",
                        "question_image_url": [
                            "/question_images/LXV7r3DewFE6draDo2dnCBHYDxKZoCLohhRtqFPsBGmwVLyVZE.png"
                        ],
                        "question_texts": [
                            "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                            "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86020,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 189,
                    "question_text_1": "How many parameters are there in the multi-head attention layer of the encoder (exclude the parameters in the WO matrix used for linear transformation)?",
                    "question_image_1": null,
                    "question_type": "SA",
                    "total_mark": "3.00",
                    "value_start": "768",
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739763,
                    "answer_type": "Equal",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "db0aba9af3053fffa483e446c414d77e",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86018,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "af6628e9-f89c-4c89-81aa-3ed853cf0c92",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "How many parameters are there in the multi-head attention layer of the encoder (exclude the parameters in the WO matrix used for linear transformation)?"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86018,
                        "exam_id": 1,
                        "question_paper_id": 269,
                        "question_number": 0,
                        "question_text_1": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                        "question_image_1": "LXV7r3DewFE6draDo2dnCBHYDxKZoCLohhRtqFPsBGmwVLyVZE.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:23.000000Z",
                        "updated_at": "2025-07-09T14:38:23.000000Z",
                        "question_num_long": 640653739761,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "b4f53b55bd4053fdbc148717005f92dc",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions.",
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "fd62fc8c-4b14-472f-8f5b-97cbb51f1828",
                        "question_image_url": [
                            "/question_images/LXV7r3DewFE6draDo2dnCBHYDxKZoCLohhRtqFPsBGmwVLyVZE.png"
                        ],
                        "question_texts": [
                            "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                            "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86021,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 190,
                    "question_text_1": null,
                    "question_image_1": "uoeDaDVZun9EKbpkfi60UnSXWIAtq5extC7HWM8Zhv6fSkecMw.png",
                    "question_type": "MCQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739764,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "41dc2ef569c54cd48b374d12ab61899c",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "8ddcbd6f-b494-44c9-a8b8-a147dcc0c527",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/uoeDaDVZun9EKbpkfi60UnSXWIAtq5extC7HWM8Zhv6fSkecMw.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 229731,
                            "question_id": 86021,
                            "option_text": "",
                            "option_image": "ijkwpkauhfd3U6qjYRODn3WegQbldl5WlSkbI0Qh0a1P4wmxc5.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474514,
                            "option_image_url": "app/option_images/ijkwpkauhfd3U6qjYRODn3WegQbldl5WlSkbI0Qh0a1P4wmxc5.png"
                        },
                        {
                            "id": 229732,
                            "question_id": 86021,
                            "option_text": "",
                            "option_image": "MiRYy3izf9LicWmUCIlqUYLB5iJFgub36eVGxIzXCgg4rxGMo1.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474515,
                            "option_image_url": "app/option_images/MiRYy3izf9LicWmUCIlqUYLB5iJFgub36eVGxIzXCgg4rxGMo1.png"
                        },
                        {
                            "id": 229733,
                            "question_id": 86021,
                            "option_text": "",
                            "option_image": "vFUPdiifXVYXDPXklnq1y4NypZc6NvzETpGEXxv1JT3de7k17Z.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474516,
                            "option_image_url": "app/option_images/vFUPdiifXVYXDPXklnq1y4NypZc6NvzETpGEXxv1JT3de7k17Z.png"
                        },
                        {
                            "id": 229734,
                            "question_id": 86021,
                            "option_text": "",
                            "option_image": "4Ojf7ReOHoFFVSexF0ydIBFmNn54GoRuH7w8BNGDo34inPgGgS.png",
                            "score": "4.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474517,
                            "option_image_url": "app/option_images/4Ojf7ReOHoFFVSexF0ydIBFmNn54GoRuH7w8BNGDo34inPgGgS.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86022,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 191,
                    "question_text_1": "Suppose we run the pre-trained GPT model in an autoregressive fashion for generating a text sequence. Then, the statement that \u201c We do not need to add the positional embedding to the predicted tokens at every time step\u201d is",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739770,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "d42fa06272cca6daea4487bcaa1554cd",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "6b60a7e5-cfb3-4d6d-a24d-6087b1c44b6f",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Suppose we run the pre-trained GPT model in an autoregressive fashion for generating a text sequence. Then, the statement that \u201c We do not need to add the positional embedding to the predicted tokens at every time step\u201d is"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 229735,
                            "question_id": 86022,
                            "option_text": "TRUE",
                            "option_image": "",
                            "score": "4.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474530,
                            "option_image_url": null
                        },
                        {
                            "id": 229736,
                            "question_id": 86022,
                            "option_text": "FALSE",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474531,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86023,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 192,
                    "question_text_1": "Suppose we have a dataset for machine translation tasks with thousands of samples. Suppose a team considers training the transformer model. The model could be trained in two approaches  A\u00a0\u00a0Autoregressive training B\u00a0\u00a0Teacher forcing  Choose the correct statements",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739765,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "ba4dc0559b3588cababe54dfe89f3ccd",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "2fe053c3-1a3d-4345-9f2a-15babdef3e5c",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Suppose we have a dataset for machine translation tasks with thousands of samples. Suppose a team considers training the transformer model. The model could be trained in two approaches  A\u00a0\u00a0Autoregressive training B\u00a0\u00a0Teacher forcing  Choose the correct statements"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 229737,
                            "question_id": 86023,
                            "option_text": "Approach A helps the model to converge faster than approach B",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474518,
                            "option_image_url": null
                        },
                        {
                            "id": 229738,
                            "question_id": 86023,
                            "option_text": "Approach B helps the model converge faster than approach A",
                            "option_image": "",
                            "score": "1.500",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474519,
                            "option_image_url": null
                        },
                        {
                            "id": 229739,
                            "question_id": 86023,
                            "option_text": "One can start the training with approach B first and then switch to approach Aafter some training steps",
                            "option_image": "",
                            "score": "1.500",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474520,
                            "option_image_url": null
                        },
                        {
                            "id": 229740,
                            "question_id": 86023,
                            "option_text": "Once the training starts with approach B and then switching to approach Aafter some training steps can not be done",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474521,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86024,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 193,
                    "question_text_1": null,
                    "question_image_1": "smxQcobz4L4EJBKInMNx7NMaAk1VJy48KD22W8C8Q0NMKO6jMt.png",
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739766,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "a62a842fb81a50ef7407ac51af46ee78",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "9e09b8a3-db0d-4e7c-ad1c-b0e091685139",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/smxQcobz4L4EJBKInMNx7NMaAk1VJy48KD22W8C8Q0NMKO6jMt.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 229741,
                            "question_id": 86024,
                            "option_text": "512",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474522,
                            "option_image_url": null
                        },
                        {
                            "id": 229742,
                            "question_id": 86024,
                            "option_text": "1024",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474523,
                            "option_image_url": null
                        },
                        {
                            "id": 229743,
                            "question_id": 86024,
                            "option_text": "1536",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474524,
                            "option_image_url": null
                        },
                        {
                            "id": 229744,
                            "question_id": 86024,
                            "option_text": "2048",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474525,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86025,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 194,
                    "question_text_1": "Choose the correct statements",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739771,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "9fcb2c4275e55535adcf18cd0244e08f",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "549f7707-22f8-496c-9d14-ad9207e2eb2f",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Choose the correct statements"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 229745,
                            "question_id": 86025,
                            "option_text": "All the parameters of the GPT model are randomly initialized during pre-training",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474532,
                            "option_image_url": null
                        },
                        {
                            "id": 229746,
                            "question_id": 86025,
                            "option_text": "The model minimizes the CLM objective during fine-tuning to improve theperformance",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474533,
                            "option_image_url": null
                        },
                        {
                            "id": 229747,
                            "question_id": 86025,
                            "option_text": "All parameters of the GPT model are randomly initialized during fine-tuning",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474534,
                            "option_image_url": null
                        },
                        {
                            "id": 229748,
                            "question_id": 86025,
                            "option_text": "In general, fine-tuning requires a dataset with labels",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474535,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86026,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 195,
                    "question_text_1": null,
                    "question_image_1": "sYp6DupSAE1U6GfutaTq9fQbnugXTqpCD6LfMO3sLFn6nO5OJc.png",
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739772,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "c5dbe8c7d4de7a5801ab8bfccfc178a0",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "2ddda20d-8975-42ec-9abe-95fd17ccbc86",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/sYp6DupSAE1U6GfutaTq9fQbnugXTqpCD6LfMO3sLFn6nO5OJc.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 229749,
                            "question_id": 86026,
                            "option_text": "Top-k sampling",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474536,
                            "option_image_url": null
                        },
                        {
                            "id": 229750,
                            "question_id": 86026,
                            "option_text": "Beam search with the beam size K = 2",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474537,
                            "option_image_url": null
                        },
                        {
                            "id": 229751,
                            "question_id": 86026,
                            "option_text": "Top-p sampling",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474538,
                            "option_image_url": null
                        },
                        {
                            "id": 229752,
                            "question_id": 86026,
                            "option_text": "Exhaustive search",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474539,
                            "option_image_url": null
                        },
                        {
                            "id": 229753,
                            "question_id": 86026,
                            "option_text": "None of these",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474540,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86027,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 0,
                    "question_text_1": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                    "question_image_1": "rwEWb2FFLQr2pFL460EqDF9u2LxFKqBU4mlSdkWs32geV3aVFk.png",
                    "question_type": "COMPREHENSION",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739767,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 0,
                    "hash": "791e0ce80e66c4f270c3568b2953163a",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions.",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "892a4c60-de34-4eb4-8511-61f83b7228b9",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/rwEWb2FFLQr2pFL460EqDF9u2LxFKqBU4mlSdkWs32geV3aVFk.png"
                    ],
                    "question_texts": [
                        "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                        "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions."
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 86028,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 196,
                    "question_text_1": "The attention score matrix given in Table 1 is appropriate for the causal language modelling task",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739768,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "1deade2a53f2a9fbaeca92c86a296317",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86027,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "c97d796d-5d6c-4d18-9762-a4722f5e3532",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "The attention score matrix given in Table 1 is appropriate for the causal language modelling task"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 229754,
                            "question_id": 86028,
                            "option_text": "True",
                            "option_image": "",
                            "score": "4.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474526,
                            "option_image_url": null
                        },
                        {
                            "id": 229755,
                            "question_id": 86028,
                            "option_text": "False",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474527,
                            "option_image_url": null
                        },
                        {
                            "id": 229756,
                            "question_id": 86028,
                            "option_text": "Insufficient information",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474528,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": {
                        "id": 86027,
                        "exam_id": 1,
                        "question_paper_id": 269,
                        "question_number": 0,
                        "question_text_1": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                        "question_image_1": "rwEWb2FFLQr2pFL460EqDF9u2LxFKqBU4mlSdkWs32geV3aVFk.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:23.000000Z",
                        "updated_at": "2025-07-09T14:38:23.000000Z",
                        "question_num_long": 640653739767,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "791e0ce80e66c4f270c3568b2953163a",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions.",
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "892a4c60-de34-4eb4-8511-61f83b7228b9",
                        "question_image_url": [
                            "/question_images/rwEWb2FFLQr2pFL460EqDF9u2LxFKqBU4mlSdkWs32geV3aVFk.png"
                        ],
                        "question_texts": [
                            "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                            "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86029,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 197,
                    "question_text_1": "Assume the time step starts from t = 0 and ends at t = 6. Suppose the model is at time step t = 4, what is the attention value assigned for the word \u201cis\u201d?",
                    "question_image_1": null,
                    "question_type": "SA",
                    "total_mark": "4.00",
                    "value_start": "0",
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739769,
                    "answer_type": "Equal",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "27ab2ee264aa09f00a8c213026d4fb47",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86027,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "ed6cef0b-2d47-4ebf-9aed-50cb09932f34",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Assume the time step starts from t = 0 and ends at t = 6. Suppose the model is at time step t = 4, what is the attention value assigned for the word \u201cis\u201d?"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86027,
                        "exam_id": 1,
                        "question_paper_id": 269,
                        "question_number": 0,
                        "question_text_1": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                        "question_image_1": "rwEWb2FFLQr2pFL460EqDF9u2LxFKqBU4mlSdkWs32geV3aVFk.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:23.000000Z",
                        "updated_at": "2025-07-09T14:38:23.000000Z",
                        "question_num_long": 640653739767,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "791e0ce80e66c4f270c3568b2953163a",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions.",
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "892a4c60-de34-4eb4-8511-61f83b7228b9",
                        "question_image_url": [
                            "/question_images/rwEWb2FFLQr2pFL460EqDF9u2LxFKqBU4mlSdkWs32geV3aVFk.png"
                        ],
                        "question_texts": [
                            "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                            "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86030,
                    "exam_id": 1,
                    "question_paper_id": 269,
                    "question_number": 198,
                    "question_text_1": "The statement that \u201cthe Next Sentence Prediction (NSP) task requires the BERT model to run autoregressively given the first sentence( or segment)\u201d is",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:23.000000Z",
                    "updated_at": "2025-07-09T14:38:23.000000Z",
                    "question_num_long": 640653739773,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "cdce8ae2ccdf28865369f88471c7457c",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "4f2c34a5-9b3a-46ed-a8bd-6ff5b05efe6f",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "The statement that \u201cthe Next Sentence Prediction (NSP) task requires the BERT model to run autoregressively given the first sentence( or segment)\u201d is"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 229757,
                            "question_id": 86030,
                            "option_text": "TRUE",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474541,
                            "option_image_url": null
                        },
                        {
                            "id": 229758,
                            "question_id": 86030,
                            "option_text": "FALSE",
                            "option_image": "",
                            "score": "3.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:23.000000Z",
                            "updated_at": "2025-07-09T14:38:23.000000Z",
                            "option_number": 6406532474542,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                }
            ]
        },
        "summary": "### Summary of Core Topics, Concepts, Principles, and Equations\n\n#### Core Topics and Concepts\n\n1. **Large Language Models (LLMs)**\n   - Introduction to LLMs\n   - Transformer Architecture\n   - Encoder and Decoder Layers\n   - Positional Encoding\n   - Attention Mechanisms\n\n2. **Embedding and Positional Encoding**\n   - Vocabulary and Embedding Matrix\n   - Positional Encoding Schemes\n   - Adding Positional Information to Tokens\n\n3. **Transformer Architecture**\n   - Multi-Head Attention\n   - Parameter Matrices (W_Q, W_K, W_V, W_O)\n   - Attention Scores and Representations\n\n4. **Training and Decoding Strategies**\n   - Autoregressive Training\n   - Teacher Forcing\n   - Decoding Strategies (Top-k sampling, Beam search, Top-p sampling, Exhaustive search)\n\n5. **Model Configurations and Parameters**\n   - Source and Target Vocabulary Size\n   - Maximum Sequence Length\n   - Context Window Length\n   - Number of Heads\n   - Model Dimensions (d_model, d_ff, d_q, d_k, d_v)\n\n6. **GPT and BERT Models**\n   - Causal Language Modeling\n   - Attention Matrices\n   - Next Sentence Prediction (NSP) Task\n\n#### Important Equations\n\n1. **Positional Encoding:**\n   \\[\n   P(pos, 2i) = \\sin\\left(\\frac{pos}{10^{(2i/d_{model})}}\\right)\n   \\]\n   \\[\n   P(pos, 2i + 1) = \\cos\\left(\\frac{pos}{10^{(2i/d_{model})}}\\right)\n   \\]\n\n2. **Model Dimensions:**\n   \\[\n   d_{ff} = 4 \\times d_{model}\n   \\]\n   \\[\n   d_q = d_k = d_v = \\frac{d_{model}}{n_h}\n   \\]\n\n3. **Attention Mechanisms:**\n   - Attention Score Calculation\n   - Linear Transformation Matrices (W_Q, W_K, W_V, W_O)\n\n### Knowledge Graph \n\n```mermaid\ngraph TD;\n    A[Large Language Models] --> B[Transformer Architecture];\n    A --> C[Embedding and Positional Encoding];\n    A --> D[Training and Decoding Strategies];\n    A --> E[Model Configurations and Parameters];\n    B --> F[Encoder and Decoder Layers];\n    B --> G[Multi-Head Attention];\n    B --> H[Parameter Matrices];\n    C --> I[Vocabulary and Embedding Matrix];\n    C --> J[Positional Encoding Schemes];\n    D --> K[Autoregressive Training];\n    D --> L[Teacher Forcing];\n    D --> M[Decoding Strategies];\n    E --> N[Source and Target Vocabulary Size];\n    E --> O[Maximum Sequence Length];\n    E --> P[Context Window Length];\n    E --> Q[Number of Heads];\n    E --> R[Model Dimensions];\n```\n\n```mermaid\ngraph TD;\n    S[GPT and BERT Models] --> T[Causal Language Modeling];\n    S --> U[Attention Matrices];\n    S --> V[Next Sentence Prediction];\n    T --> W[Attention Mechanisms];\n    U --> X[Attention Score Calculation];\n    V --> Y[NSP Task];\n```\n\nThis summary and knowledge graph should help you focus on the essential topics and understand the relationships between them for your exam preparation.",
        "total_score": "50.00"
    },
    "url": "/question-paper/practise/85/abae87da-b20",
    "version": "ee3d5d44299e610bd137ea6200db5ac2"
}