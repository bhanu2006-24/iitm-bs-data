{
    "component": "Quizpractise/PractiseQuestionPaper",
    "props": {
        "errors": {},
        "auth": {
            "user": null
        },
        "flash": {
            "error": []
        },
        "banner": null,
        "file_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "file_do_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "question_paper": {
            "id": 272,
            "group_id": 13,
            "exam_id": 1,
            "total_score": "1.00",
            "duration": 4,
            "created_at": "2025-07-09T14:38:42.000000Z",
            "updated_at": "2025-07-09T14:38:42.000000Z",
            "question_paper_name": "IIT M DEGREE AN4 EXAM QDB4 25 Feb 2024",
            "question_paper_description": "2024 Feb25: IIT M AN4 EXAM QDB4",
            "uuid": "c9a76912-84e",
            "year": 2024,
            "is_new": 0,
            "exam": {
                "id": 1,
                "exam_name": "Quiz 1",
                "created_at": "2024-10-16T08:08:51.000000Z",
                "updated_at": "2024-10-16T08:08:51.000000Z",
                "uuid": "9251bc3a-e33e-45e0-bcf0-b16a0ea5b5fa",
                "en_id": "eyJpdiI6ImpjN2w3M0cvR2R2MjdxQjFxVWN4bkE9PSIsInZhbHVlIjoiV2pYYXFKMng2MWp2RmM1Y25xUFlFUT09IiwibWFjIjoiMjAzYTVlMTEwN2UwZDE4ODEzOGVmZGU4YTU4NTkyNzY1MWZkNTAyOTBmZDAyM2YyMzYxMmRmZGYwZWVlYTBmMyIsInRhZyI6IiJ9"
            },
            "questions": [
                {
                    "id": 86782,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 182,
                    "question_text_1": "THIS IS QUESTION PAPER FOR THE SUBJECT <b>\"DEGREE LEVEL : INTRODUCTION TO LARGE </b>LANGUAGE MODELS (COMPUTER BASED EXAM)\"<b> </b> ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740318,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "c5824c31330bc2d9b577ddd7ae48a54e",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "cf8da744-ec26-49de-baa8-b17af4cab264",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "THIS IS QUESTION PAPER FOR THE SUBJECT <b>\"DEGREE LEVEL : INTRODUCTION TO LARGE </b>LANGUAGE MODELS (COMPUTER BASED EXAM)\"<b> </b> ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 231450,
                            "question_id": 86782,
                            "option_text": "YES",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475927,
                            "option_image_url": null
                        },
                        {
                            "id": 231451,
                            "question_id": 86782,
                            "option_text": "NO",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475928,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86783,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 183,
                    "question_text_1": null,
                    "question_image_1": "Ecwp2xKIhYmYBJgIjRDwAMjS75uqe0ULe54hV3WnqxszxitXcl.png",
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740319,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "8e6013ed6762c1c288f1431e29ceee19",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "9ee94fae-29e7-4021-936b-e56d30945e72",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/Ecwp2xKIhYmYBJgIjRDwAMjS75uqe0ULe54hV3WnqxszxitXcl.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 231452,
                            "question_id": 86783,
                            "option_text": "Useful Data has been mentioned above.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475929,
                            "option_image_url": null
                        },
                        {
                            "id": 231453,
                            "question_id": 86783,
                            "option_text": "This data attachment is just for a reference & not for an evaluation.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475930,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86784,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 0,
                    "question_text_1": "  Based on the above data, answer the given subquestions.",
                    "question_image_1": "VmlNwtu9RPhzbqPJP87xp7cCNm3InKShhMagbioeyEYgOiTNDy.png",
                    "question_type": "COMPREHENSION",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740320,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 0,
                    "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "73892a0e-ca40-4b2c-a157-84983b052d4a",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/VmlNwtu9RPhzbqPJP87xp7cCNm3InKShhMagbioeyEYgOiTNDy.png"
                    ],
                    "question_texts": [
                        "  Based on the above data, answer the given subquestions."
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 86785,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 184,
                    "question_text_1": null,
                    "question_image_1": "boRp12sbmgO07E4Ge8bx1HvVO5qa5B90kc6gABOeO92C8IZBW9.png",
                    "question_type": "SA",
                    "total_mark": "3.00",
                    "value_start": "-0.73",
                    "value_end": "-0.63",
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740321,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "db280026989a0964ec6de8b6596b2bc8",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86784,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "309eb944-2f92-43c8-859d-656a39064c07",
                    "solutions_count": 1,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/boRp12sbmgO07E4Ge8bx1HvVO5qa5B90kc6gABOeO92C8IZBW9.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86784,
                        "exam_id": 1,
                        "question_paper_id": 272,
                        "question_number": 0,
                        "question_text_1": "  Based on the above data, answer the given subquestions.",
                        "question_image_1": "VmlNwtu9RPhzbqPJP87xp7cCNm3InKShhMagbioeyEYgOiTNDy.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:44.000000Z",
                        "updated_at": "2025-07-09T14:38:44.000000Z",
                        "question_num_long": 640653740320,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": null,
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "73892a0e-ca40-4b2c-a157-84983b052d4a",
                        "question_image_url": [
                            "/question_images/VmlNwtu9RPhzbqPJP87xp7cCNm3InKShhMagbioeyEYgOiTNDy.png"
                        ],
                        "question_texts": [
                            "  Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86786,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 185,
                    "question_text_1": null,
                    "question_image_1": "eDghirdQZz8CKO5AYk4rvqoCiKN07ry7tTEN7LbsRocgplfwM3.png",
                    "question_type": "SA",
                    "total_mark": "2.00",
                    "value_start": "0.13",
                    "value_end": "0.20",
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740322,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "47251b27284ff0fd8465b535b64a7561",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86784,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "dc6ba66c-1440-4a32-aab7-411f3c8ae19a",
                    "solutions_count": 1,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/eDghirdQZz8CKO5AYk4rvqoCiKN07ry7tTEN7LbsRocgplfwM3.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86784,
                        "exam_id": 1,
                        "question_paper_id": 272,
                        "question_number": 0,
                        "question_text_1": "  Based on the above data, answer the given subquestions.",
                        "question_image_1": "VmlNwtu9RPhzbqPJP87xp7cCNm3InKShhMagbioeyEYgOiTNDy.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:44.000000Z",
                        "updated_at": "2025-07-09T14:38:44.000000Z",
                        "question_num_long": 640653740320,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": null,
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "73892a0e-ca40-4b2c-a157-84983b052d4a",
                        "question_image_url": [
                            "/question_images/VmlNwtu9RPhzbqPJP87xp7cCNm3InKShhMagbioeyEYgOiTNDy.png"
                        ],
                        "question_texts": [
                            "  Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86787,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 186,
                    "question_text_1": "  ",
                    "question_image_1": "iuXUz3ay0zhu6dvby50SFXDMBkBgU521NJuRs0t97rFWvdLteF.png",
                    "question_type": "SA",
                    "total_mark": "3.00",
                    "value_start": "-0.06",
                    "value_end": "-0.035",
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740323,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "1e6f5c6db58c42a34291a8f65c218cc2",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": "  ",
                    "question_image_2": "gILHUsKHq11PqFcqMqo8HlC9KBPf3b5Eld6tbcF3E4KJDK3gpd.png",
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86784,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "9481e3b3-5310-451a-984b-112c293c4388",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/iuXUz3ay0zhu6dvby50SFXDMBkBgU521NJuRs0t97rFWvdLteF.png",
                        "/question_images/gILHUsKHq11PqFcqMqo8HlC9KBPf3b5Eld6tbcF3E4KJDK3gpd.png"
                    ],
                    "question_texts": [
                        "  ",
                        "  "
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86784,
                        "exam_id": 1,
                        "question_paper_id": 272,
                        "question_number": 0,
                        "question_text_1": "  Based on the above data, answer the given subquestions.",
                        "question_image_1": "VmlNwtu9RPhzbqPJP87xp7cCNm3InKShhMagbioeyEYgOiTNDy.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:44.000000Z",
                        "updated_at": "2025-07-09T14:38:44.000000Z",
                        "question_num_long": 640653740320,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": null,
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "73892a0e-ca40-4b2c-a157-84983b052d4a",
                        "question_image_url": [
                            "/question_images/VmlNwtu9RPhzbqPJP87xp7cCNm3InKShhMagbioeyEYgOiTNDy.png"
                        ],
                        "question_texts": [
                            "  Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86788,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 187,
                    "question_text_1": "  Which token in the sequence was given the highest score?",
                    "question_image_1": "dWUpIXusibq05a5PwGcjsJumwuOfL0PMXOquMb0ef8Qk8m8r3V.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740324,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "e047b7806556d680cd42653b0b2669a7",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86784,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "163e22b3-aa0f-4431-9433-a1f769f4708e",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/dWUpIXusibq05a5PwGcjsJumwuOfL0PMXOquMb0ef8Qk8m8r3V.png"
                    ],
                    "question_texts": [
                        "  Which token in the sequence was given the highest score?"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 231454,
                            "question_id": 86788,
                            "option_text": "G",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475934,
                            "option_image_url": null
                        },
                        {
                            "id": 231455,
                            "question_id": 86788,
                            "option_text": "C",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475935,
                            "option_image_url": null
                        },
                        {
                            "id": 231456,
                            "question_id": 86788,
                            "option_text": "T",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475936,
                            "option_image_url": null
                        },
                        {
                            "id": 231457,
                            "question_id": 86788,
                            "option_text": "A",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475937,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": {
                        "id": 86784,
                        "exam_id": 1,
                        "question_paper_id": 272,
                        "question_number": 0,
                        "question_text_1": "  Based on the above data, answer the given subquestions.",
                        "question_image_1": "VmlNwtu9RPhzbqPJP87xp7cCNm3InKShhMagbioeyEYgOiTNDy.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:44.000000Z",
                        "updated_at": "2025-07-09T14:38:44.000000Z",
                        "question_num_long": 640653740320,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "1893db4fa295f1e7b378fff5a9957cc0",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": null,
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "73892a0e-ca40-4b2c-a157-84983b052d4a",
                        "question_image_url": [
                            "/question_images/VmlNwtu9RPhzbqPJP87xp7cCNm3InKShhMagbioeyEYgOiTNDy.png"
                        ],
                        "question_texts": [
                            "  Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86789,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 0,
                    "question_text_1": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                    "question_image_1": "654TJMlH0gsCzUvcUCYgHk3sc9vJyszfBwikHpnrTENbXOKimQ.png",
                    "question_type": "COMPREHENSION",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740325,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 0,
                    "hash": "b4f53b55bd4053fdbc148717005f92dc",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions.",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "e66cf5e8-f9d7-49ca-9a69-d58e3d932675",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/654TJMlH0gsCzUvcUCYgHk3sc9vJyszfBwikHpnrTENbXOKimQ.png"
                    ],
                    "question_texts": [
                        "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                        "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions."
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 86790,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 188,
                    "question_text_1": "Suppose the number of learnable parameters in the source input embedding layer is 1600, how many parameters are there in the positional embedding layer of the source language? Assume the positional embeddings are learnable.",
                    "question_image_1": null,
                    "question_type": "SA",
                    "total_mark": "3.00",
                    "value_start": "512",
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740326,
                    "answer_type": "Equal",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "fbdef4442a06712234387dc3c8a0863e",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86789,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "e6b892aa-1946-4452-aaa4-bb7385e8d9fd",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Suppose the number of learnable parameters in the source input embedding layer is 1600, how many parameters are there in the positional embedding layer of the source language? Assume the positional embeddings are learnable."
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86789,
                        "exam_id": 1,
                        "question_paper_id": 272,
                        "question_number": 0,
                        "question_text_1": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                        "question_image_1": "654TJMlH0gsCzUvcUCYgHk3sc9vJyszfBwikHpnrTENbXOKimQ.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:44.000000Z",
                        "updated_at": "2025-07-09T14:38:44.000000Z",
                        "question_num_long": 640653740325,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "b4f53b55bd4053fdbc148717005f92dc",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions.",
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "e66cf5e8-f9d7-49ca-9a69-d58e3d932675",
                        "question_image_url": [
                            "/question_images/654TJMlH0gsCzUvcUCYgHk3sc9vJyszfBwikHpnrTENbXOKimQ.png"
                        ],
                        "question_texts": [
                            "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                            "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86791,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 189,
                    "question_text_1": "How many parameters are there in the multi-head attention layer of the encoder (exclude the parameters in the WO matrix used for linear transformation)?",
                    "question_image_1": null,
                    "question_type": "SA",
                    "total_mark": "3.00",
                    "value_start": "768",
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740327,
                    "answer_type": "Equal",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "db0aba9af3053fffa483e446c414d77e",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86789,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "885afa21-1c0f-4621-bd4f-484914409639",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "How many parameters are there in the multi-head attention layer of the encoder (exclude the parameters in the WO matrix used for linear transformation)?"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86789,
                        "exam_id": 1,
                        "question_paper_id": 272,
                        "question_number": 0,
                        "question_text_1": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                        "question_image_1": "654TJMlH0gsCzUvcUCYgHk3sc9vJyszfBwikHpnrTENbXOKimQ.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:44.000000Z",
                        "updated_at": "2025-07-09T14:38:44.000000Z",
                        "question_num_long": 640653740325,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "b4f53b55bd4053fdbc148717005f92dc",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions.",
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "e66cf5e8-f9d7-49ca-9a69-d58e3d932675",
                        "question_image_url": [
                            "/question_images/654TJMlH0gsCzUvcUCYgHk3sc9vJyszfBwikHpnrTENbXOKimQ.png"
                        ],
                        "question_texts": [
                            "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer. ",
                            "Consider the following configuration for the Vannila transformer architecture with one encoder layer and one decoder layer.   Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86792,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 190,
                    "question_text_1": null,
                    "question_image_1": "HaJVhFSOyNRWIR6kVoJmzXHPuy5KCcXt1RNkJzKmWlRIBuSxwM.png",
                    "question_type": "MCQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740328,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "4ffbe9c453f1ce9f9404e567e6b8f02d",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "06bb8b7a-5f91-4966-8017-efd41a90e159",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/HaJVhFSOyNRWIR6kVoJmzXHPuy5KCcXt1RNkJzKmWlRIBuSxwM.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 231458,
                            "question_id": 86792,
                            "option_text": "",
                            "option_image": "0qxTN2uBKh4YA7zJFDmu4C5UuI9prduZfudQWIBEsDVwQnHs9S.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475940,
                            "option_image_url": "app/option_images/0qxTN2uBKh4YA7zJFDmu4C5UuI9prduZfudQWIBEsDVwQnHs9S.png"
                        },
                        {
                            "id": 231459,
                            "question_id": 86792,
                            "option_text": "",
                            "option_image": "Oe1CGvwzSzTdet0dXTacIHmu0Cuhy28wLmw9At4Sw5xwCgJUWZ.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475941,
                            "option_image_url": "app/option_images/Oe1CGvwzSzTdet0dXTacIHmu0Cuhy28wLmw9At4Sw5xwCgJUWZ.png"
                        },
                        {
                            "id": 231460,
                            "question_id": 86792,
                            "option_text": "",
                            "option_image": "J23SHc7akroNCTuzkOjcYaAHSv8LsSB60aakfzrt62T5mVL4td.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475942,
                            "option_image_url": "app/option_images/J23SHc7akroNCTuzkOjcYaAHSv8LsSB60aakfzrt62T5mVL4td.png"
                        },
                        {
                            "id": 231461,
                            "question_id": 86792,
                            "option_text": "",
                            "option_image": "Wauy9GE2omVZeXjgh9uOnZxaSAPhzmm8hnq6vV09pVsFGaJaWf.png",
                            "score": "4.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475943,
                            "option_image_url": "app/option_images/Wauy9GE2omVZeXjgh9uOnZxaSAPhzmm8hnq6vV09pVsFGaJaWf.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86793,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 191,
                    "question_text_1": "Suppose we run the pre-trained GPT model in an autoregressive fashion for generating a text sequence. Then, the statement that \u201c We do not need to add the positional embedding to the predicted tokens at every time step\u201d is",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740334,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "d1b6c8713505cdc78c14cfe8a22a6985",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "cacadf3a-65ba-41d6-91a8-cad65cd32d9a",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Suppose we run the pre-trained GPT model in an autoregressive fashion for generating a text sequence. Then, the statement that \u201c We do not need to add the positional embedding to the predicted tokens at every time step\u201d is"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 231462,
                            "question_id": 86793,
                            "option_text": "TRUE",
                            "option_image": "",
                            "score": "4.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475956,
                            "option_image_url": null
                        },
                        {
                            "id": 231463,
                            "question_id": 86793,
                            "option_text": "FALSE",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475957,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86794,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 192,
                    "question_text_1": "Suppose we have a dataset for machine translation tasks with thousands of samples. Suppose a team considers training the transformer model. The model could be trained in two approaches  A\u00a0\u00a0Autoregressive training B\u00a0\u00a0Teacher forcing  Choose the correct statements",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740329,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "9a70fca32196040690ade4acb3ee9db6",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "efa7c7e9-e3f4-491c-aa4c-f94fda661706",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Suppose we have a dataset for machine translation tasks with thousands of samples. Suppose a team considers training the transformer model. The model could be trained in two approaches  A\u00a0\u00a0Autoregressive training B\u00a0\u00a0Teacher forcing  Choose the correct statements"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 231464,
                            "question_id": 86794,
                            "option_text": "Approach A helps the model to converge faster than approach B",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475944,
                            "option_image_url": null
                        },
                        {
                            "id": 231465,
                            "question_id": 86794,
                            "option_text": "Approach B helps the model converge faster than approach A",
                            "option_image": "",
                            "score": "1.500",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475945,
                            "option_image_url": null
                        },
                        {
                            "id": 231466,
                            "question_id": 86794,
                            "option_text": "One can start the training with approach B first and then switch to approach Aafter some training steps",
                            "option_image": "",
                            "score": "1.500",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475946,
                            "option_image_url": null
                        },
                        {
                            "id": 231467,
                            "question_id": 86794,
                            "option_text": "Once the training starts with approach B and then switching to approach Aafter some training steps can not be done",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475947,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86795,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 193,
                    "question_text_1": null,
                    "question_image_1": "xwYVgBT5tCKdullDcaHTuWoSxKnHxNtpnWceG5vSmAtAejACzy.png",
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740330,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "7eb186a2f14a718138ffef35dd23fc7c",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "85dacc4c-ef04-4f14-9aa1-8a13e7d2f7ce",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/xwYVgBT5tCKdullDcaHTuWoSxKnHxNtpnWceG5vSmAtAejACzy.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 231468,
                            "question_id": 86795,
                            "option_text": "512",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475948,
                            "option_image_url": null
                        },
                        {
                            "id": 231469,
                            "question_id": 86795,
                            "option_text": "1024",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475949,
                            "option_image_url": null
                        },
                        {
                            "id": 231470,
                            "question_id": 86795,
                            "option_text": "1536",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475950,
                            "option_image_url": null
                        },
                        {
                            "id": 231471,
                            "question_id": 86795,
                            "option_text": "2048",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475951,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86796,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 194,
                    "question_text_1": "Choose the correct statements",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740335,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "c2054983c06a7d34eafc2632c5f08177",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "60f9013e-2860-4c0a-81c4-2b1d42fe3b7c",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Choose the correct statements"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 231472,
                            "question_id": 86796,
                            "option_text": "All the parameters of the GPT model are randomly initialized during pre-training",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475958,
                            "option_image_url": null
                        },
                        {
                            "id": 231473,
                            "question_id": 86796,
                            "option_text": "The model minimizes the CLM objective during fine-tuning to improve theperformance",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475959,
                            "option_image_url": null
                        },
                        {
                            "id": 231474,
                            "question_id": 86796,
                            "option_text": "All parameters of the GPT model are randomly initialized during fine-tuning",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475960,
                            "option_image_url": null
                        },
                        {
                            "id": 231475,
                            "question_id": 86796,
                            "option_text": "In general, fine-tuning requires a dataset with labels",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475961,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86797,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 195,
                    "question_text_1": null,
                    "question_image_1": "aTtHKyve7yMeZRj4V2kKg3nai8IPqfc62hWGqUt6UapODbSmVS.png",
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740336,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "e2abe7c2610becf8b9b50cb68296f10c",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "3f035aae-cf7b-4dd2-9a1e-e1a2c40b907d",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/aTtHKyve7yMeZRj4V2kKg3nai8IPqfc62hWGqUt6UapODbSmVS.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 231476,
                            "question_id": 86797,
                            "option_text": "Top-k sampling",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475962,
                            "option_image_url": null
                        },
                        {
                            "id": 231477,
                            "question_id": 86797,
                            "option_text": "Beam search with the beam size K = 2",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475963,
                            "option_image_url": null
                        },
                        {
                            "id": 231478,
                            "question_id": 86797,
                            "option_text": "Top-p sampling",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475964,
                            "option_image_url": null
                        },
                        {
                            "id": 231479,
                            "question_id": 86797,
                            "option_text": "Exhaustive search",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475965,
                            "option_image_url": null
                        },
                        {
                            "id": 231480,
                            "question_id": 86797,
                            "option_text": "None of these",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475966,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 86798,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 0,
                    "question_text_1": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                    "question_image_1": "F8zmUTV2QWpIcGHMizqu3cBtCCT9MFEvmQLCXam6BUQl0ds43o.png",
                    "question_type": "COMPREHENSION",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740331,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 0,
                    "hash": "791e0ce80e66c4f270c3568b2953163a",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions.",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "11287925-47e7-490f-8f06-18f58eea6308",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/F8zmUTV2QWpIcGHMizqu3cBtCCT9MFEvmQLCXam6BUQl0ds43o.png"
                    ],
                    "question_texts": [
                        "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                        "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions."
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 86799,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 196,
                    "question_text_1": "The attention score matrix given in Table 1 is appropriate for the causal language modelling task",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740332,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "09548631674a8690543a9919d6557d94",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86798,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "b346756d-69d0-42d1-ba43-84a57355886f",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "The attention score matrix given in Table 1 is appropriate for the causal language modelling task"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 231481,
                            "question_id": 86799,
                            "option_text": "True",
                            "option_image": "",
                            "score": "4.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475952,
                            "option_image_url": null
                        },
                        {
                            "id": 231482,
                            "question_id": 86799,
                            "option_text": "False",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475953,
                            "option_image_url": null
                        },
                        {
                            "id": 231483,
                            "question_id": 86799,
                            "option_text": "Insufficient information",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475954,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": {
                        "id": 86798,
                        "exam_id": 1,
                        "question_paper_id": 272,
                        "question_number": 0,
                        "question_text_1": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                        "question_image_1": "F8zmUTV2QWpIcGHMizqu3cBtCCT9MFEvmQLCXam6BUQl0ds43o.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:44.000000Z",
                        "updated_at": "2025-07-09T14:38:44.000000Z",
                        "question_num_long": 640653740331,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "791e0ce80e66c4f270c3568b2953163a",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions.",
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "11287925-47e7-490f-8f06-18f58eea6308",
                        "question_image_url": [
                            "/question_images/F8zmUTV2QWpIcGHMizqu3cBtCCT9MFEvmQLCXam6BUQl0ds43o.png"
                        ],
                        "question_texts": [
                            "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                            "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86800,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 197,
                    "question_text_1": "Assume the time step starts from t = 0 and ends at t = 6. Suppose the model is at time step t = 4, what is the attention value assigned for the word \u201cis\u201d?",
                    "question_image_1": null,
                    "question_type": "SA",
                    "total_mark": "4.00",
                    "value_start": "0",
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740333,
                    "answer_type": "Equal",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "27ab2ee264aa09f00a8c213026d4fb47",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": 86798,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "3b1d9706-f8ed-4832-8831-604079311d85",
                    "solutions_count": 1,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Assume the time step starts from t = 0 and ends at t = 6. Suppose the model is at time step t = 4, what is the attention value assigned for the word \u201cis\u201d?"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [],
                    "parent_question": {
                        "id": 86798,
                        "exam_id": 1,
                        "question_paper_id": 272,
                        "question_number": 0,
                        "question_text_1": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                        "question_image_1": "F8zmUTV2QWpIcGHMizqu3cBtCCT9MFEvmQLCXam6BUQl0ds43o.png",
                        "question_type": "COMPREHENSION",
                        "total_mark": "0.00",
                        "value_start": null,
                        "value_end": null,
                        "created_at": "2025-07-09T14:38:44.000000Z",
                        "updated_at": "2025-07-09T14:38:44.000000Z",
                        "question_num_long": 640653740331,
                        "answer_type": null,
                        "response_type": null,
                        "have_answers": 0,
                        "hash": "791e0ce80e66c4f270c3568b2953163a",
                        "course_id": 85,
                        "is_markdown": 0,
                        "question_text_2": "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions.",
                        "question_image_2": null,
                        "question_image_3": null,
                        "question_image_4": null,
                        "question_image_5": null,
                        "question_image_7": null,
                        "question_image_8": null,
                        "question_image_9": null,
                        "question_image_10": null,
                        "parent_question_id": null,
                        "question_text_3": null,
                        "question_text_4": null,
                        "question_text_5": null,
                        "uuid": "11287925-47e7-490f-8f06-18f58eea6308",
                        "question_image_url": [
                            "/question_images/F8zmUTV2QWpIcGHMizqu3cBtCCT9MFEvmQLCXam6BUQl0ds43o.png"
                        ],
                        "question_texts": [
                            "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below  ",
                            "Consider a GPT model used for Causal language modelling. We feed the input sentence \u201cThis is a cool idea\u201d to the model by appending special staring [BOS] and ending [EOS] tokens ( that is, \u201c[BOS] This is a cool idea [EOS]\u201d). Assume the context length of the model is 7. The attention matrix computed in one of the attention layers is given below    Based on the above data, answer the given subquestions."
                        ]
                    }
                },
                {
                    "id": 86801,
                    "exam_id": 1,
                    "question_paper_id": 272,
                    "question_number": 198,
                    "question_text_1": "The statement that \u201cthe Next Sentence Prediction (NSP) task requires the BERT model to run autoregressively given the first sentence( or segment)\u201d is",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-07-09T14:38:44.000000Z",
                    "updated_at": "2025-07-09T14:38:44.000000Z",
                    "question_num_long": 640653740337,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "51732f4a56370f3c372695516ef0d224",
                    "course_id": 85,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "5533a477-77a1-4faa-aa0e-564f305286b2",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "The statement that \u201cthe Next Sentence Prediction (NSP) task requires the BERT model to run autoregressively given the first sentence( or segment)\u201d is"
                    ],
                    "course": {
                        "id": 85,
                        "course_name": "LLM",
                        "course_code": "LLM",
                        "created_at": "2024-10-29T12:29:16.000000Z",
                        "updated_at": "2024-10-29T12:29:16.000000Z",
                        "program_id": 1,
                        "uuid": "5dee317d-ef80-4b86-921a-549e0cf9f948"
                    },
                    "options": [
                        {
                            "id": 231484,
                            "question_id": 86801,
                            "option_text": "TRUE",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475967,
                            "option_image_url": null
                        },
                        {
                            "id": 231485,
                            "question_id": 86801,
                            "option_text": "FALSE",
                            "option_image": "",
                            "score": "3.000",
                            "is_correct": 1,
                            "created_at": "2025-07-09T14:38:44.000000Z",
                            "updated_at": "2025-07-09T14:38:44.000000Z",
                            "option_number": 6406532475968,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                }
            ]
        },
        "summary": "### Summary of Core Topics, Concepts, Principles, and Equations\n\n#### Core Topics and Concepts\n\n1. **Large Language Models (LLMs)**\n   - **Transformer Architecture**: Understanding the encoder and decoder layers, attention mechanisms, and positional encoding.\n   - **Embedding Matrices**: How tokens are embedded and the role of embedding matrices.\n   - **Positional Encoding**: Adding positional information to tokens using sine and cosine functions.\n\n2. **Attention Mechanisms**\n   - **Self-Attention**: Computation of attention scores and weights.\n   - **Multi-Head Attention**: Understanding the role of different weight matrices (W_Q, W_K, W_V, W_O).\n   - **Causal Language Modeling**: Attention matrices and their properties in causal language models.\n\n3. **Model Training and Inference**\n   - **Autoregressive Training vs. Teacher Forcing**: Differences and applications in training language models.\n   - **Decoding Strategies**: Top-k sampling, beam search, top-p sampling, and exhaustive search.\n   - **Fine-Tuning**: Initialization of parameters and objectives during fine-tuning.\n\n4. **Model Configurations**\n   - **Parameter Counting**: Calculating the number of parameters in different layers (embedding, positional embedding, multi-head attention).\n   - **Context Length**: Understanding the context window and its impact on text generation.\n\n#### Important Equations\n\n1. **Positional Encoding**:\n   \\[\n   P(pos, 2i) = \\sin\\left(\\frac{pos}{10^{(2i/d_{model})}}\\right)\n   \\]\n   \\[\n   P(pos, 2i + 1) = \\cos\\left(\\frac{pos}{10^{(2i/d_{model})}}\\right)\n   \\]\n\n2. **Attention Scores**:\n   \\[\n   \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n   \\]\n\n3. **Parameter Counting**:\n   - **Embedding Layer**: Vocabulary size \u00d7 embedding dimension.\n   - **Positional Embedding Layer**: Context length \u00d7 embedding dimension.\n   - **Multi-Head Attention Layer**: Number of heads \u00d7 (query, key, value dimensions).\n\n#### Knowledge Graph \n\n```mermaid\ngraph TD;\n    A[Large Language Models] --> B[Transformer Architecture]\n    A --> C[Model Training and Inference]\n    A --> D[Model Configurations]\n    B --> E[Embedding Matrices]\n    B --> F[Positional Encoding]\n    B --> G[Attention Mechanisms]\n    G --> H[Self-Attention]\n    G --> I[Multi-Head Attention]\n    C --> J[Autoregressive Training]\n    C --> K[Teacher Forcing]\n    C --> L[Decoding Strategies]\n    D --> M[Parameter Counting]\n    D --> N[Context Length]\n```\n\n```mermaid\ngraph TD;\n    G[Attention Mechanisms] --> O[Causal Language Modeling]\n    L[Decoding Strategies] --> P[Top-k Sampling]\n    L --> Q[Beam Search]\n    L --> R[Top-p Sampling]\n    L --> S[Exhaustive Search]\n    M[Parameter Counting] --> T[Embedding Layer]\n    M --> U[Positional Embedding Layer]\n    M --> V[Multi-Head Attention Layer]\n```\n\nThis summary and knowledge graph should help you focus on the essential topics and understand the relationships between different concepts for your exam.",
        "total_score": "50.00"
    },
    "url": "/question-paper/practise/85/c9a76912-84e",
    "version": "ee3d5d44299e610bd137ea6200db5ac2"
}