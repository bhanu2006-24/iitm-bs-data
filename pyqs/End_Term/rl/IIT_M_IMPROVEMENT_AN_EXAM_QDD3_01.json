{
    "component": "Quizpractise/PractiseQuestionPaper",
    "props": {
        "errors": {},
        "auth": {
            "user": null
        },
        "flash": {
            "error": []
        },
        "banner": null,
        "file_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "file_do_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "question_paper": {
            "id": 168,
            "group_id": 24,
            "exam_id": 3,
            "total_score": "1.00",
            "duration": 4,
            "created_at": "2024-11-27T22:49:37.000000Z",
            "updated_at": "2024-11-27T22:49:37.000000Z",
            "question_paper_name": "IIT M IMPROVEMENT AN EXAM QDD3 01",
            "question_paper_description": "2024 Sep01: IIT M AN EXAM QDD3",
            "uuid": "efd2c6a3-51a",
            "year": 2024,
            "is_new": 0,
            "exam": {
                "id": 3,
                "exam_name": "End Term Quiz",
                "created_at": "2024-10-16T08:08:51.000000Z",
                "updated_at": "2024-10-16T08:08:51.000000Z",
                "uuid": "7a6ff569-f50c-40e7-a08b-f5c334392600",
                "en_id": "eyJpdiI6Imd4RlBmdlFCL1hQdXNVS01pb3BSWUE9PSIsInZhbHVlIjoidjBrTVArOGNWZ3hjUDlCNFB3YkVHUT09IiwibWFjIjoiMWFjYjc0NjBlN2RlZTM5ZTc0MzA1MDA2ZTgxMmNkNDVkMzRjY2E3YmZkMjZjZTMwZWFiMjU3OWExOGM1YmMzNSIsInRhZyI6IiJ9"
            },
            "questions": [
                {
                    "id": 50650,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 176,
                    "question_text_1": "THIS IS QUESTION PAPER FOR THE SUBJECT \"DEGREE LEVEL : REINFORCEMENT LEARNING (COMPUTER BASED EXAM)\"  ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902886,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "052d530f649c606afb55cca70e1de69a",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "9ee8992e-6fbb-4ba8-876a-0b822a3b7561",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "THIS IS QUESTION PAPER FOR THE SUBJECT \"DEGREE LEVEL : REINFORCEMENT LEARNING (COMPUTER BASED EXAM)\"  ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)"
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135213,
                            "question_id": 50650,
                            "option_text": "YES",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040718,
                            "option_image_url": null
                        },
                        {
                            "id": 135214,
                            "question_id": 50650,
                            "option_text": "NO",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040719,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50651,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 177,
                    "question_text_1": "Note:  For numerical answer type questions, enter your answer correct upto two decimal places without rounding up or off unless stated otherwise.",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902887,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "4ae6000e4f3337c0c61d63ffd094f721",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "8034fd43-87b3-40b7-b0dc-c67cfc97ed25",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Note:  For numerical answer type questions, enter your answer correct upto two decimal places without rounding up or off unless stated otherwise."
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135215,
                            "question_id": 50651,
                            "option_text": "Instructions has been mentioned above.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040720,
                            "option_image_url": null
                        },
                        {
                            "id": 135216,
                            "question_id": 50651,
                            "option_text": "This Instructions is just for a reference & not for an evaluation.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040721,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50652,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 178,
                    "question_text_1": "Consider following assertion reason pair:  Assertion: Reinforcement learning is a type of unsupervised learning algorithm as both don\u2019t have correct labels. Reason: In unsupervised learning, a reward like quantity is not maximized.",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902888,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "daa028abef475ca559fbaefddeefebe7",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "1ce4fa58-4098-484c-a7ba-c4c7ee26d102",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider following assertion reason pair:  Assertion: Reinforcement learning is a type of unsupervised learning algorithm as both don\u2019t have correct labels. Reason: In unsupervised learning, a reward like quantity is not maximized."
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135217,
                            "question_id": 50652,
                            "option_text": "Assertion and Reason are both true and Reason is a correct explanation ofAssertion.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040722,
                            "option_image_url": null
                        },
                        {
                            "id": 135218,
                            "question_id": 50652,
                            "option_text": "Assertion and Reason are both true and Reason is not a correct explanation ofAssertion.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040723,
                            "option_image_url": null
                        },
                        {
                            "id": 135219,
                            "question_id": 50652,
                            "option_text": "Assertion is true but Reason is false.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040724,
                            "option_image_url": null
                        },
                        {
                            "id": 135220,
                            "question_id": 50652,
                            "option_text": "Assertion is false but Reason is true.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040725,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50653,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 179,
                    "question_text_1": "Which of these statements is true regarding the rewards obtained in an MDP?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902889,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "bc94778f861653cb68b4c6308b12f64f",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "5c14759c-d581-46c0-adfb-dca45d6605fb",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of these statements is true regarding the rewards obtained in an MDP?"
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135221,
                            "question_id": 50653,
                            "option_text": "",
                            "option_image": "hgma9VWxFxAOrdO5LZJI246fyAhC4cBjTlsaFGgrqyxJhooN5w.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040726,
                            "option_image_url": "app/option_images/hgma9VWxFxAOrdO5LZJI246fyAhC4cBjTlsaFGgrqyxJhooN5w.png"
                        },
                        {
                            "id": 135222,
                            "question_id": 50653,
                            "option_text": "",
                            "option_image": "Zm6IsSKGRKQLi1Eo7QSwp46E0l0O067fsRN7An4EpjFLiDU2Z1.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040727,
                            "option_image_url": "app/option_images/Zm6IsSKGRKQLi1Eo7QSwp46E0l0O067fsRN7An4EpjFLiDU2Z1.png"
                        },
                        {
                            "id": 135223,
                            "question_id": 50653,
                            "option_text": "",
                            "option_image": "5ecZvyVgRc5GpbuaelJvTInsR0jaXLGxCkNz9iuzD3yn6uxPC6.png",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040728,
                            "option_image_url": "app/option_images/5ecZvyVgRc5GpbuaelJvTInsR0jaXLGxCkNz9iuzD3yn6uxPC6.png"
                        },
                        {
                            "id": 135224,
                            "question_id": 50653,
                            "option_text": "",
                            "option_image": "jvokgacXxgcuNHG6ymyBM39sqMizTs3H1V9JANP8AXqNJOPNF3.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040729,
                            "option_image_url": "app/option_images/jvokgacXxgcuNHG6ymyBM39sqMizTs3H1V9JANP8AXqNJOPNF3.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50654,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 180,
                    "question_text_1": "Consider a reinforcement learning agent trying to balance a pole in a continuous environment. The agent receives a reward of +1 for each time step the pole remains balanced and 0 otherwise. Which of the following statements accurately describes the differences between Monte Carlo and Temporal Difference (TD) learning in this scenario?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902892,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "8b0e77ec6dbe1e0264312f528ec89e92",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "753d6ad5-6459-439d-a6d2-961cb1978be5",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider a reinforcement learning agent trying to balance a pole in a continuous environment. The agent receives a reward of +1 for each time step the pole remains balanced and 0 otherwise. Which of the following statements accurately describes the differences between Monte Carlo and Temporal Difference (TD) learning in this scenario?"
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135225,
                            "question_id": 50654,
                            "option_text": "Monte Carlo methods update the value function based on complete episodes,while TD methods update after each step.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040740,
                            "option_image_url": null
                        },
                        {
                            "id": 135226,
                            "question_id": 50654,
                            "option_text": "TD methods are guaranteed to converge to the optimal policy, while MonteCarlo methods may not converge.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040741,
                            "option_image_url": null
                        },
                        {
                            "id": 135227,
                            "question_id": 50654,
                            "option_text": "Monte Carlo methods are less sensitive to the choice of the discount factorcompared to TD methods.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040742,
                            "option_image_url": null
                        },
                        {
                            "id": 135228,
                            "question_id": 50654,
                            "option_text": "TD methods are more effective in environments with high variance andstochasticity compared to Monte Carlo methods.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040743,
                            "option_image_url": null
                        },
                        {
                            "id": 135229,
                            "question_id": 50654,
                            "option_text": "None of these.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040744,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50655,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 181,
                    "question_text_1": "Consider a reinforcement learning agent learning to control a robotic arm in a simulated environment. The agent receives a reward of +1 for successfully placing an object in target location and 0 otherwise. Which of the following statements accurately describes a difference between SARSA and Q-learning in this scenario?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902893,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "1818e2b74c5adad3d6fad5946ac49ea5",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "7f539db6-5964-47c6-8e82-f8d3c20fa694",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider a reinforcement learning agent learning to control a robotic arm in a simulated environment. The agent receives a reward of +1 for successfully placing an object in target location and 0 otherwise. Which of the following statements accurately describes a difference between SARSA and Q-learning in this scenario?"
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135230,
                            "question_id": 50655,
                            "option_text": "SARSA updates its action-value function using the action taken in the nextstate, while Q-learning updates its action-value function using the maximum action-value across all possible actions in the next state.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040745,
                            "option_image_url": null
                        },
                        {
                            "id": 135231,
                            "question_id": 50655,
                            "option_text": "SARSA is less sensitive to the choice of policy compared to Q-learning, makingit more robust in environments with frequent changes.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040746,
                            "option_image_url": null
                        },
                        {
                            "id": 135232,
                            "question_id": 50655,
                            "option_text": "SARSA converges more quickly than Q-learning in environments with highreward variance.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040747,
                            "option_image_url": null
                        },
                        {
                            "id": 135233,
                            "question_id": 50655,
                            "option_text": "Q-learning is inherently more computationally efficient than SARSA due tofewer updates required per episode.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040748,
                            "option_image_url": null
                        },
                        {
                            "id": 135234,
                            "question_id": 50655,
                            "option_text": "None of these.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040749,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50656,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 182,
                    "question_text_1": "In Q-learning, what is the impact of maximization bias on the algorithm\u2019s performance, especially in environments with noisy or stochastic rewards?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902895,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "5de6de560e5cdf7fef16c1a018c620cf",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "d91ef8d1-3171-4eeb-ac86-185bfbc3046a",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "In Q-learning, what is the impact of maximization bias on the algorithm\u2019s performance, especially in environments with noisy or stochastic rewards?"
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135235,
                            "question_id": 50656,
                            "option_text": "Maximization bias can lead to an overestimation of action values, causing thealgorithm to favor suboptimal actions and potentially delaying convergence to the optimal policy.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040751,
                            "option_image_url": null
                        },
                        {
                            "id": 135236,
                            "question_id": 50656,
                            "option_text": "Maximization bias tends to enhance the algorithm\u2019s performance byconsistently selecting actions with higher estimated values, leading to quicker convergence.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040752,
                            "option_image_url": null
                        },
                        {
                            "id": 135237,
                            "question_id": 50656,
                            "option_text": "Maximization bias may result in excessive exploration, allowing the algorithmto discover better strategies in environments with highly variable rewards.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040753,
                            "option_image_url": null
                        },
                        {
                            "id": 135238,
                            "question_id": 50656,
                            "option_text": "Maximization bias typically has a minor effect on the performance of Q-learning, as the exploration process naturally mitigates its influence over time.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040754,
                            "option_image_url": null
                        },
                        {
                            "id": 135239,
                            "question_id": 50656,
                            "option_text": "None of these.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040755,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50657,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 183,
                    "question_text_1": null,
                    "question_image_1": "7E1VFoCXFLm9tyVtyttr4r0JijVmwwK0flGkqGRxKykVSbBR1D.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902897,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "622105a2f79c45f1dcafbfb174e39ebc",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "bc700904-07e0-4591-a35e-46c15325c21f",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/7E1VFoCXFLm9tyVtyttr4r0JijVmwwK0flGkqGRxKykVSbBR1D.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135240,
                            "question_id": 50657,
                            "option_text": "",
                            "option_image": "EzGa1EfBkyfx34SCs3b0AlXZvLlt77iccnjHdQrZHNag19AQOo.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040757,
                            "option_image_url": "app/option_images/EzGa1EfBkyfx34SCs3b0AlXZvLlt77iccnjHdQrZHNag19AQOo.png"
                        },
                        {
                            "id": 135241,
                            "question_id": 50657,
                            "option_text": "",
                            "option_image": "Ed9Y6FdMkUL9Drt1Iu5plItofJrcQxdUFnyDxZ7Y7RZ5wnPLmK.png",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040758,
                            "option_image_url": "app/option_images/Ed9Y6FdMkUL9Drt1Iu5plItofJrcQxdUFnyDxZ7Y7RZ5wnPLmK.png"
                        },
                        {
                            "id": 135242,
                            "question_id": 50657,
                            "option_text": "",
                            "option_image": "oNX7HdfuTOqWABO91U3OltXQ7JNvgnPTxKcTRVwmIlrVkF3oSz.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040759,
                            "option_image_url": "app/option_images/oNX7HdfuTOqWABO91U3OltXQ7JNvgnPTxKcTRVwmIlrVkF3oSz.png"
                        },
                        {
                            "id": 135243,
                            "question_id": 50657,
                            "option_text": "",
                            "option_image": "h5Vr20A0bgsZwxwLMGmrkc9H2WT45RzLcRtDekP6UrUj6U7I1K.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040760,
                            "option_image_url": "app/option_images/h5Vr20A0bgsZwxwLMGmrkc9H2WT45RzLcRtDekP6UrUj6U7I1K.png"
                        },
                        {
                            "id": 135244,
                            "question_id": 50657,
                            "option_text": "",
                            "option_image": "zFgts4rH07fOMhYA0ZzTkKmxWGli6NJXdqfHdDuQB05CcL4G6s.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040761,
                            "option_image_url": "app/option_images/zFgts4rH07fOMhYA0ZzTkKmxWGli6NJXdqfHdDuQB05CcL4G6s.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50658,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 184,
                    "question_text_1": null,
                    "question_image_1": "PN8mW3gZ5ev8RQn9MicxSgKRsGDnfr1jfoqM5Ka1zYSco2Cq25.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902899,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "e81b2acd818408435465905578359651",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "cdd36ea0-3431-4fe8-b233-afa1185b3931",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/PN8mW3gZ5ev8RQn9MicxSgKRsGDnfr1jfoqM5Ka1zYSco2Cq25.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135245,
                            "question_id": 50658,
                            "option_text": "",
                            "option_image": "0ojb0FT6agJ9VPRVVoZQELV1xohJv4P5uhvwBa9nhb6sgYezhM.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040763,
                            "option_image_url": "app/option_images/0ojb0FT6agJ9VPRVVoZQELV1xohJv4P5uhvwBa9nhb6sgYezhM.png"
                        },
                        {
                            "id": 135246,
                            "question_id": 50658,
                            "option_text": "",
                            "option_image": "DUVimMhm75Ln4k8Y1pSbbxxhRNLGgFWubJSWIsdv6UgspiifLB.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040764,
                            "option_image_url": "app/option_images/DUVimMhm75Ln4k8Y1pSbbxxhRNLGgFWubJSWIsdv6UgspiifLB.png"
                        },
                        {
                            "id": 135247,
                            "question_id": 50658,
                            "option_text": "",
                            "option_image": "0dj058D02Qa2UioQSQkZCkEuqD3EiAe4Z6Q9XULPkCQwklKInK.png",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040765,
                            "option_image_url": "app/option_images/0dj058D02Qa2UioQSQkZCkEuqD3EiAe4Z6Q9XULPkCQwklKInK.png"
                        },
                        {
                            "id": 135248,
                            "question_id": 50658,
                            "option_text": "",
                            "option_image": "6verohQ3BhXAg7F1fjnG4mPIOt3B9YpucjcvWk8XetB22FcPDX.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040766,
                            "option_image_url": "app/option_images/6verohQ3BhXAg7F1fjnG4mPIOt3B9YpucjcvWk8XetB22FcPDX.png"
                        },
                        {
                            "id": 135249,
                            "question_id": 50658,
                            "option_text": "",
                            "option_image": "ppbtP8dO6MSJWhyPTgy4zdDBnDbxzbVlnz4AiSkoDAYJeVT958.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040767,
                            "option_image_url": "app/option_images/ppbtP8dO6MSJWhyPTgy4zdDBnDbxzbVlnz4AiSkoDAYJeVT958.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50659,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 185,
                    "question_text_1": "What key issue in standard DQN does the dueling architecture seek to address?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902900,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "365497104a0cb265dff31affbb6e6289",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "4a6e50b0-37aa-4684-882c-00d35254265e",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What key issue in standard DQN does the dueling architecture seek to address?"
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135250,
                            "question_id": 50659,
                            "option_text": "The problem of ensuring faster convergence in environments with high-dimensional state spaces.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040768,
                            "option_image_url": null
                        },
                        {
                            "id": 135251,
                            "question_id": 50659,
                            "option_text": "The challenge of distinguishing between actions in environments with a largenumber of potential moves.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040769,
                            "option_image_url": null
                        },
                        {
                            "id": 135252,
                            "question_id": 50659,
                            "option_text": "The inefficiency in accurately evaluating actions that do not significantly affectthe overall value of the state.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040770,
                            "option_image_url": null
                        },
                        {
                            "id": 135253,
                            "question_id": 50659,
                            "option_text": "The difficulty in learning effective value functions when rewards are sparseand infrequent.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040771,
                            "option_image_url": null
                        },
                        {
                            "id": 135254,
                            "question_id": 50659,
                            "option_text": "None of these.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040772,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50660,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 186,
                    "question_text_1": "Which of the following best captures the key difference between policy gradient methods and Q-learning in reinforcement learning?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902903,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "959087199160a449b1ead208d01c45bf",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "cc668f07-96b4-4397-8846-43299d49b90e",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of the following best captures the key difference between policy gradient methods and Q-learning in reinforcement learning?"
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135255,
                            "question_id": 50660,
                            "option_text": "Policy gradient methods directly optimize the policy by computing gradients ofthe expected reward, while Q-learning indirectly improves the policy by learning a value function that estimates the expected rewards for actions.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040775,
                            "option_image_url": null
                        },
                        {
                            "id": 135256,
                            "question_id": 50660,
                            "option_text": "Q-learning focuses on optimizing the policy parameters directly, while policygradient methods estimate the action-value function to guide policy improvement.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040776,
                            "option_image_url": null
                        },
                        {
                            "id": 135257,
                            "question_id": 50660,
                            "option_text": "Policy gradient methods require a model of the environment\u2019s dynamics tocompute gradients, while Q-learning does not rely on any such model.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040777,
                            "option_image_url": null
                        },
                        {
                            "id": 135258,
                            "question_id": 50660,
                            "option_text": "Q-learning is used primarily for continuous action spaces, while policygradient methods are better suited for discrete action spaces.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040778,
                            "option_image_url": null
                        },
                        {
                            "id": 135259,
                            "question_id": 50660,
                            "option_text": "None of these.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040779,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50661,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 187,
                    "question_text_1": "Consider following assertion reason pair:  Assertion: A3C can provide better performance compared to A2C, provided the updates are small enough. Reason: In A2C, if a thread runs for a long time, other threads have to wait for it to finish.",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902904,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "e8424f102af9d4d63e6cea0d8ed32bab",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "2797d3cb-97aa-48df-9f90-6b733c2dbfd5",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider following assertion reason pair:  Assertion: A3C can provide better performance compared to A2C, provided the updates are small enough. Reason: In A2C, if a thread runs for a long time, other threads have to wait for it to finish."
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135260,
                            "question_id": 50661,
                            "option_text": "Assertion and Reason are both true and Reason is a correct explanation ofAssertion.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040780,
                            "option_image_url": null
                        },
                        {
                            "id": 135261,
                            "question_id": 50661,
                            "option_text": "Assertion and Reason are both true and Reason is not a correct explanation ofAssertion.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040781,
                            "option_image_url": null
                        },
                        {
                            "id": 135262,
                            "question_id": 50661,
                            "option_text": "Assertion is true but Reason is false.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040782,
                            "option_image_url": null
                        },
                        {
                            "id": 135263,
                            "question_id": 50661,
                            "option_text": "Assertion is false but Reason is true.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040783,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50662,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 188,
                    "question_text_1": "What role do \u201dmeta-policies\u201d play in Hierarchical Reinforcement Learning?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902905,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "59767f66abe3f9bad811e71cdbf2691d",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "730bdfa4-3b17-41b3-aa9d-6dd1088f6e84",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What role do \u201dmeta-policies\u201d play in Hierarchical Reinforcement Learning?"
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135264,
                            "question_id": 50662,
                            "option_text": "They define the specific actions to take in each state.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040784,
                            "option_image_url": null
                        },
                        {
                            "id": 135265,
                            "question_id": 50662,
                            "option_text": "They manage the high-level decisions and selection of subtasks.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040785,
                            "option_image_url": null
                        },
                        {
                            "id": 135266,
                            "question_id": 50662,
                            "option_text": "They directly compute the rewards for each subtask.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040786,
                            "option_image_url": null
                        },
                        {
                            "id": 135267,
                            "question_id": 50662,
                            "option_text": "They optimize the low-level action policies.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040787,
                            "option_image_url": null
                        },
                        {
                            "id": 135268,
                            "question_id": 50662,
                            "option_text": "None of these.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040788,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50663,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 189,
                    "question_text_1": "In HRL, what is a \u201dsubtask\u201d typically used for?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902906,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "df63c5bdecf3be13618c8ecc4f8d5dee",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "d561589d-c710-431b-8049-16e8b0886ebc",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "In HRL, what is a \u201dsubtask\u201d typically used for?"
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135269,
                            "question_id": 50663,
                            "option_text": "To evaluate the performance of the overall policy",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040789,
                            "option_image_url": null
                        },
                        {
                            "id": 135270,
                            "question_id": 50663,
                            "option_text": "To execute the final decision made by the high-level policy",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040790,
                            "option_image_url": null
                        },
                        {
                            "id": 135271,
                            "question_id": 50663,
                            "option_text": "To break down complex tasks into more manageable components",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040791,
                            "option_image_url": null
                        },
                        {
                            "id": 135272,
                            "question_id": 50663,
                            "option_text": "To directly interact with the environment and collect rewards",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040792,
                            "option_image_url": null
                        },
                        {
                            "id": 135273,
                            "question_id": 50663,
                            "option_text": "None of these.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040793,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50664,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 190,
                    "question_text_1": "How does HRL handle long-term dependencies in tasks?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902907,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "c72ff5b6ecee08d7b46e5db4056cd2ce",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "b47b33e5-da3b-4c49-a307-8f2f8b38eb1b",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "How does HRL handle long-term dependencies in tasks?"
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135274,
                            "question_id": 50664,
                            "option_text": "By using recurrent neural networks (RNNs)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040794,
                            "option_image_url": null
                        },
                        {
                            "id": 135275,
                            "question_id": 50664,
                            "option_text": "By focusing on immediate rewards only",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040795,
                            "option_image_url": null
                        },
                        {
                            "id": 135276,
                            "question_id": 50664,
                            "option_text": "By leveraging hierarchical structures to manage dependencies",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040796,
                            "option_image_url": null
                        },
                        {
                            "id": 135277,
                            "question_id": 50664,
                            "option_text": "By reducing the state space through dimensionality reduction",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040797,
                            "option_image_url": null
                        },
                        {
                            "id": 135278,
                            "question_id": 50664,
                            "option_text": "None of these.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040798,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50665,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 191,
                    "question_text_1": "What is a common challenge when implementing HRL in practice?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902908,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "beda318252b1fe43c1b548d75862306c",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "29dcb974-5061-4e98-a1e0-7494c1e42eab",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What is a common challenge when implementing HRL in practice?"
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135279,
                            "question_id": 50665,
                            "option_text": "Finding suitable reward functions for all levels",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040799,
                            "option_image_url": null
                        },
                        {
                            "id": 135280,
                            "question_id": 50665,
                            "option_text": "Scaling the approach to very large state spaces",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040800,
                            "option_image_url": null
                        },
                        {
                            "id": 135281,
                            "question_id": 50665,
                            "option_text": "Ensuring the subtasks are independent of each other",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040801,
                            "option_image_url": null
                        },
                        {
                            "id": 135282,
                            "question_id": 50665,
                            "option_text": "Integrating HRL with existing non-hierarchical methods",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040802,
                            "option_image_url": null
                        },
                        {
                            "id": 135283,
                            "question_id": 50665,
                            "option_text": "None of these.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040803,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50666,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 192,
                    "question_text_1": "Choose the correct formula for UCT:",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902909,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "2c6ff74293ebea285907859abddeedf8",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "c0732e97-a6e3-4672-b7b4-839fe998660b",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Choose the correct formula for UCT:"
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135284,
                            "question_id": 50666,
                            "option_text": "",
                            "option_image": "0rQNejZUuaXho4h8ReYxoG5Pyugri2w4iPuDdFrqv6VImGjUSP.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040804,
                            "option_image_url": "app/option_images/0rQNejZUuaXho4h8ReYxoG5Pyugri2w4iPuDdFrqv6VImGjUSP.png"
                        },
                        {
                            "id": 135285,
                            "question_id": 50666,
                            "option_text": "",
                            "option_image": "ya9efgjirktiUzGIBo530Vx0PJlRlzg1PshNQlv1Mp3vepsgfs.png",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040805,
                            "option_image_url": "app/option_images/ya9efgjirktiUzGIBo530Vx0PJlRlzg1PshNQlv1Mp3vepsgfs.png"
                        },
                        {
                            "id": 135286,
                            "question_id": 50666,
                            "option_text": "",
                            "option_image": "PwBsDXN0R5vABVMuvO7FOVBiAJ7GfoiwME8dQIAU34b1t5dNP6.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040806,
                            "option_image_url": "app/option_images/PwBsDXN0R5vABVMuvO7FOVBiAJ7GfoiwME8dQIAU34b1t5dNP6.png"
                        },
                        {
                            "id": 135287,
                            "question_id": 50666,
                            "option_text": "",
                            "option_image": "3QMH7JuemmFQO5NFIKDPMjmFnQ2DAfDFuIloyUIFCmArdvN02U.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040807,
                            "option_image_url": "app/option_images/3QMH7JuemmFQO5NFIKDPMjmFnQ2DAfDFuIloyUIFCmArdvN02U.png"
                        },
                        {
                            "id": 135288,
                            "question_id": 50666,
                            "option_text": "",
                            "option_image": "cMwqydMfqkxdGCIlaeMsTblmersuyHVWkzFtyk6bgd1avgFVPr.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040808,
                            "option_image_url": "app/option_images/cMwqydMfqkxdGCIlaeMsTblmersuyHVWkzFtyk6bgd1avgFVPr.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50667,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 193,
                    "question_text_1": null,
                    "question_image_1": "pdvKmBfamplsvLaUdG64chJa7nYP3Li3AJj2dyKv3rc8AKGoSh.png",
                    "question_type": "MSQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902890,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "aec1973cba16b10a40076fc88b3bca68",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "b25ad621-ffed-403f-8bdd-254e67cf32c7",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/pdvKmBfamplsvLaUdG64chJa7nYP3Li3AJj2dyKv3rc8AKGoSh.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135289,
                            "question_id": 50667,
                            "option_text": "",
                            "option_image": "9sUpdKQirRzcByl5QXFVrDt8ckZCx8S66orpd3JZnYNm1aXgQN.png",
                            "score": "0.500",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040730,
                            "option_image_url": "app/option_images/9sUpdKQirRzcByl5QXFVrDt8ckZCx8S66orpd3JZnYNm1aXgQN.png"
                        },
                        {
                            "id": 135290,
                            "question_id": 50667,
                            "option_text": "",
                            "option_image": "VbwOnRq88EeSPllwhFrOprYYFgFGxcVSGOxeORIknckHfTHfSt.png",
                            "score": "0.500",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040731,
                            "option_image_url": "app/option_images/VbwOnRq88EeSPllwhFrOprYYFgFGxcVSGOxeORIknckHfTHfSt.png"
                        },
                        {
                            "id": 135291,
                            "question_id": 50667,
                            "option_text": "",
                            "option_image": "p1vovDANmN9gAxnrG6n8rmjKAF2ZaXMUbIALRKhE5nnop2H3GP.png",
                            "score": "0.500",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040732,
                            "option_image_url": "app/option_images/p1vovDANmN9gAxnrG6n8rmjKAF2ZaXMUbIALRKhE5nnop2H3GP.png"
                        },
                        {
                            "id": 135292,
                            "question_id": 50667,
                            "option_text": "",
                            "option_image": "OWdJQyVsxeg90wFaS4PqDglR5PqBIu2uni5PhvJlRJ2hetdH7t.png",
                            "score": "0.500",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040733,
                            "option_image_url": "app/option_images/OWdJQyVsxeg90wFaS4PqDglR5PqBIu2uni5PhvJlRJ2hetdH7t.png"
                        },
                        {
                            "id": 135293,
                            "question_id": 50667,
                            "option_text": "",
                            "option_image": "1RdYF9UtSH7VOHfSeTKf40yJSmRZ1bGmlSG5kdfUfxYMfMIa22.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040734,
                            "option_image_url": "app/option_images/1RdYF9UtSH7VOHfSeTKf40yJSmRZ1bGmlSG5kdfUfxYMfMIa22.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50668,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 194,
                    "question_text_1": "Select the correct statements about Generalized Policy Iteration (GPI).",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902891,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "f6d48f6b23af427a39526ef347df434b",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "4a45b488-2909-443e-80b5-558cdf0fa233",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Select the correct statements about Generalized Policy Iteration (GPI)."
                    ],
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [
                        {
                            "id": 135294,
                            "question_id": 50668,
                            "option_text": "GPI lets policy evaluation and policy improvement interact with each otherregardless of the details of the two processes.",
                            "option_image": "",
                            "score": "0.500",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040735,
                            "option_image_url": null
                        },
                        {
                            "id": 135295,
                            "question_id": 50668,
                            "option_text": "At the end of evaluation, the policy is not greedy with respect to the valuefunction computed",
                            "option_image": "",
                            "score": "0.500",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040736,
                            "option_image_url": null
                        },
                        {
                            "id": 135296,
                            "question_id": 50668,
                            "option_text": "GPI converges only when a policy has been found which is greedy with respectto its own value function.",
                            "option_image": "",
                            "score": "0.500",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040737,
                            "option_image_url": null
                        },
                        {
                            "id": 135297,
                            "question_id": 50668,
                            "option_text": "The policy and value function found by GPI at convergence will both beoptimal.",
                            "option_image": "",
                            "score": "0.500",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040738,
                            "option_image_url": null
                        },
                        {
                            "id": 135298,
                            "question_id": 50668,
                            "option_text": "None of these.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:49:38.000000Z",
                            "updated_at": "2024-11-27T22:49:38.000000Z",
                            "option_number": 6406533040739,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 50669,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 195,
                    "question_text_1": null,
                    "question_image_1": "3Lhq6L4YShMQWuczu7nbjHFW0O0OLzjEg63JYI9L1jReTaFCol.png",
                    "question_type": "SA",
                    "total_mark": "2.00",
                    "value_start": "9.93",
                    "value_end": "10.3",
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902894,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "c1f5975fa4648e5c5df4eb8046dd8dad",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "b60827c4-36bc-41d2-b059-717f4c474b42",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/3Lhq6L4YShMQWuczu7nbjHFW0O0OLzjEg63JYI9L1jReTaFCol.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 50670,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 196,
                    "question_text_1": null,
                    "question_image_1": "sDawXi7BBWf0tyGNh3cRPz9BbTi88i1bFCknhUzcO3UaMi1XM0.png",
                    "question_type": "SA",
                    "total_mark": "2.00",
                    "value_start": "1.35",
                    "value_end": "1.45",
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902902,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "b5d806d93a487af304c71cbfc40f8ef8",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "8e799563-7270-4cff-b149-cfe5d4784e35",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/sDawXi7BBWf0tyGNh3cRPz9BbTi88i1bFCknhUzcO3UaMi1XM0.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 50671,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 197,
                    "question_text_1": null,
                    "question_image_1": "ciQsoTc7qFdvj6C9MlMqBftl5B4sO6CbHJ3mtvbIX8YySjHLci.png",
                    "question_type": "SA",
                    "total_mark": "4.00",
                    "value_start": "2.85",
                    "value_end": "2.95",
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902896,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "1abf9f1ff8bcc84b0f100cf2c7ede52e",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "3bbb50dc-c615-4712-9fbb-36a6e32e4883",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/ciQsoTc7qFdvj6C9MlMqBftl5B4sO6CbHJ3mtvbIX8YySjHLci.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 50672,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 198,
                    "question_text_1": null,
                    "question_image_1": "B2Il7EGUnVB9UEFNeGFnOOdOEYRrLtUw6djLqGyFZ5J8zGBvNJ.png",
                    "question_type": "SA",
                    "total_mark": "4.00",
                    "value_start": "4.3",
                    "value_end": "4.4",
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902898,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "3d72c766cd93a9cc5813301597eb42ef",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "ccfad72d-f812-4900-b5ed-75c632bff6b4",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/B2Il7EGUnVB9UEFNeGFnOOdOEYRrLtUw6djLqGyFZ5J8zGBvNJ.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [],
                    "parent_question": null
                },
                {
                    "id": 50673,
                    "exam_id": 3,
                    "question_paper_id": 168,
                    "question_number": 199,
                    "question_text_1": null,
                    "question_image_1": "Z8fiObulGJuHDK6TT76708bzVEIfrMlrO3cO2Ddm8nSz8J99oa.png",
                    "question_type": "SA",
                    "total_mark": "4.00",
                    "value_start": "7.95",
                    "value_end": "8.05",
                    "created_at": "2024-11-27T22:49:38.000000Z",
                    "updated_at": "2024-11-27T22:49:38.000000Z",
                    "question_num_long": 640653902901,
                    "answer_type": "Range",
                    "response_type": "Numeric",
                    "have_answers": 0,
                    "hash": "2103f1e91f9ca4fc3fbd74e6bd8affc0",
                    "course_id": 43,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "3c021d5b-9e00-4be0-bfec-461868eba898",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/Z8fiObulGJuHDK6TT76708bzVEIfrMlrO3cO2Ddm8nSz8J99oa.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 43,
                        "course_name": "RL",
                        "course_code": "RL",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "ab3e4efa-6f4b-4ac6-a32f-ad4f63a17064"
                    },
                    "options": [],
                    "parent_question": null
                }
            ]
        },
        "summary": "Here\u2019s a structured **summary of core topics, concepts, principles, and equations** for the **Reinforcement Learning (RL) exam**, followed by **Mermaid knowledge graphs** to visualize relationships.\n\n---\n\n## **1. Core Topics & Key Concepts**\n### **A. Fundamentals of Reinforcement Learning**\n1. **RL vs. Supervised/Unsupervised Learning**:\n   - RL learns from **rewards** (not labels).\n   - **MDP (Markov Decision Process)**:\n     - States (\\(s\\)), Actions (\\(a\\)), Rewards (\\(r\\)), Transition Probabilities (\\(p(s',r|s,a)\\)), Discount Factor (\\(\\gamma\\)).\n   - **Reward Function**:\n     - \\( r_{t+1} \\) depends on \\( s_t, a_t, s_{t+1} \\) (not just \\(a_t\\) or \\(s_t\\)).\n     - **Bellman Equation** (for \\(Q(s,a)\\)):\n       \\[\n       Q(s,a) = \\mathbb{E}_{s'} \\left[ r + \\gamma \\max_{a'} Q(s', a') \\right]\n       \\]\n\n2. **Exploration vs. Exploitation**:\n   - **\u03b5-greedy policy**: Choose random action with probability \u03b5, else greedy.\n   - **Optimistic Initialization**: Start with high Q-values to encourage exploration.\n\n3. **Value Functions**:\n   - **State-Value Function (\\(V(s)\\))**: Expected return from state \\(s\\).\n   - **Action-Value Function (\\(Q(s,a)\\))**: Expected return from taking \\(a\\) in \\(s\\).\n   - **Optimal Value Functions**:\n     \\[\n     V^*(s) = \\max_a Q^*(s,a), \\quad Q^*(s,a) = \\mathbb{E} \\left[ r + \\gamma \\max_{a'} Q^*(s', a') \\right]\n     \\]\n\n---\n\n### **B. Temporal Difference (TD) Learning**\n1. **TD(0) Update Rule**:\n   \\[\n   V(s) \\leftarrow V(s) + \\alpha \\left[ r + \\gamma V(s') - V(s) \\right]\n   \\]\n2. **SARSA (On-Policy TD Control)**:\n   - Updates \\(Q(s,a)\\) using the **actual next action** \\(a'\\).\n   - Update Rule:\n     \\[\n     Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma Q(s',a') - Q(s,a) \\right]\n     \\]\n3. **Q-Learning (Off-Policy TD Control)**:\n   - Updates \\(Q(s,a)\\) using the **max next Q-value**.\n   - Update Rule:\n     \\[\n     Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\right]\n     \\]\n4. **Maximization Bias**:\n   - Q-learning **overestimates** Q-values due to \\(\\max\\) operator.\n   - **Double Q-Learning** mitigates this by using two Q-functions:\n     \\[\n     Q_A(s,a) \\leftarrow Q_A(s,a) + \\alpha \\left[ r + \\gamma Q_A(s', \\arg\\max_{a'} Q_B(s',a')) - Q_A(s,a) \\right]\n     \\]\n\n---\n\n### **C. Monte Carlo (MC) Methods**\n1. **MC vs. TD**:\n   - MC learns from **complete episodes** (no bootstrapping).\n   - TD learns **online** (updates after each step).\n2. **First-Visit vs. Every-Visit MC**:\n   - First-visit: Average returns only for the **first occurrence** of \\((s,a)\\).\n   - Every-visit: Average returns for **all occurrences**.\n\n---\n\n### **D. Function Approximation**\n1. **Linear Function Approximation**:\n   - \\( \\hat{V}(s; w) = w^T x(s) \\)\n   - **TD(0) Update for Weights**:\n     \\[\n     w \\leftarrow w + \\alpha \\left[ r + \\gamma w^T x(s') - w^T x(s) \\right] x(s)\n     \\]\n2. **Deep Q-Networks (DQN)**:\n   - Uses **neural networks** to approximate \\(Q(s,a)\\).\n   - **Experience Replay**: Stores transitions \\((s,a,r,s')\\) to break correlation.\n   - **Target Network**: Periodically updates a separate network for stability.\n\n3. **Dueling DQN**:\n   - Separates **value (\\(V(s)\\))** and **advantage (\\(A(s,a)\\))** streams.\n   - Combines them to compute \\(Q(s,a)\\):\n     \\[\n     Q(s,a) = V(s) + \\left( A(s,a) - \\frac{1}{|A|} \\sum_{a'} A(s,a') \\right)\n     \\]\n   - **Purpose**: Better action selection in states where actions don\u2019t affect value much.\n\n---\n\n### **E. Policy Gradient Methods**\n1. **REINFORCE Algorithm**:\n   - Directly optimizes policy \\(\\pi_\\theta\\) via **gradient ascent**.\n   - **Policy Gradient Theorem**:\n     \\[\n     \\nabla_\\theta J(\\theta) = \\mathbb{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) Q(s,a) \\right]\n     \\]\n2. **Actor-Critic Methods**:\n   - **Actor**: Policy \\(\\pi_\\theta(a|s)\\).\n   - **Critic**: Estimates \\(Q(s,a)\\) or \\(V(s)\\).\n   - **Advantage Actor-Critic (A2C/A3C)**:\n     - A3C uses **asynchronous parallel actors** for faster training.\n     - **Advantage**: \\(A(s,a) = Q(s,a) - V(s)\\).\n\n3. **Bandit Problems**:\n   - **Binary Bandit**: Actions \\(a \\in \\{0,1\\}\\), rewards \\(R(a)\\).\n   - **Policy Gradient for Bandits**:\n     \\[\n     \\nabla_\\theta J(\\theta) = \\mathbb{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a) R \\right]\n     \\]\n   - Example: For Bernoulli policy \\(\\pi_\\theta(a=1) = p\\),\n     \\[\n     \\nabla_\\theta J(\\theta) = (R(a=1) - R(a=0)) \\cdot \\frac{\\partial p}{\\partial \\theta}\n     \\]\n\n---\n\n### **F. Hierarchical Reinforcement Learning (HRL)**\n1. **Meta-Policies**:\n   - High-level policies that **select subtasks**.\n2. **Subtasks**:\n   - Break complex tasks into **simpler components**.\n3. **Options Framework**:\n   - **Option**: \\((I, \\pi, \\beta)\\) where:\n     - \\(I\\): Initiation set (states where option can start).\n     - \\(\\pi\\): Policy for the option.\n     - \\(\\beta\\): Termination condition.\n4. **Challenges**:\n   - **Credit Assignment**: Assigning rewards to subtasks.\n   - **Reward Shaping**: Designing rewards for hierarchical levels.\n\n---\n\n### **G. Advanced Topics**\n1. **Upper Confidence Bound (UCT)**:\n   - Used in **Monte Carlo Tree Search (MCTS)**.\n   - Balances exploration/exploitation:\n     \\[\n     \\arg\\max_a \\left[ Q(s,a) + 2c \\sqrt{\\frac{2 \\ln N(s)}{N(s,a)}} \\right]\n     \\]\n2. **Generalized Policy Iteration (GPI)**:\n   - Alternates between **policy evaluation** and **policy improvement**.\n   - Converges to **optimal policy** \\(\\pi^*\\) and **optimal value function** \\(V^*\\).\n\n---\n\n## **2. Key Equations Summary**\n| **Concept**               | **Equation**                                                                 |\n|---------------------------|-----------------------------------------------------------------------------|\n| **Bellman Equation (Q)**  | \\( Q(s,a) = \\mathbb{E} \\left[ r + \\gamma \\max_{a'} Q(s', a') \\right] \\)    |\n| **TD(0) Update**          | \\( V(s) \\leftarrow V(s) + \\alpha [r + \\gamma V(s') - V(s)] \\)               |\n| **SARSA Update**          | \\( Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma Q(s',a') - Q(s,a)] \\)      |\n| **Q-Learning Update**     | \\( Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)] \\) |\n| **Double Q-Learning**     | \\( Q_A(s,a) \\leftarrow Q_A(s,a) + \\alpha [r + \\gamma Q_A(s', \\arg\\max_{a'} Q_B(s',a')) - Q_A(s,a)] \\) |\n| **Dueling DQN**           | \\( Q(s,a) = V(s) + \\left( A(s,a) - \\frac{1}{|A|} \\sum_{a'} A(s,a') \\right) \\) |\n| **Policy Gradient**       | \\( \\nabla_\\theta J(\\theta) = \\mathbb{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) Q(s,a) \\right] \\) |\n| **UCT (MCTS)**            | \\( \\arg\\max_a \\left[ Q(s,a) + 2c \\sqrt{\\frac{2 \\ln N(s)}{N(s,a)}} \\right] \\) |\n| **Linear FA TD(0) Update**| \\( w \\leftarrow w + \\alpha [r + \\gamma w^T x(s') - w^T x(s)] x(s) \\)       |\n\n---\n\n## **3. Mermaid Knowledge Graphs**\n### **Graph 1: Core RL Concepts**\n```mermaid\ngraph TD\n    A[Reinforcement Learning] --> B[Markov Decision Process]\n    A --> C[Value Functions]\n    A --> D[Policy Methods]\n    A --> E[Model-Free vs Model-Based]\n\n    B --> B1[States S]\n    B --> B2[Actions A]\n    B --> B3[Rewards R]\n    B --> B4[Transition P]\n\n    C --> C1[State-Value V(s)]\n    C --> C2[Action-Value Q(s,a)]\n    C --> C3[Bellman Equations]\n\n    D --> D1[Policy Gradient]\n    D --> D2[Q-Learning]\n    D --> D3[Actor-Critic]\n\n    E --> E1[Monte Carlo]\n    E --> E2[Temporal Difference]\n    E --> E3[DQN]\n```\n\n### **Graph 2: TD Learning & Q-Learning**\n```mermaid\ngraph TD\n    A[Temporal Difference Learning] --> B[TD(0)]\n    A --> C[SARSA]\n    A --> D[Q-Learning]\n\n    B --> B1[\"Update: V(s) \u2190 V(s) + \u03b1[r + \u03b3V(s') - V(s)]\"]\n    C --> C1[\"On-Policy: Uses actual next action a'\"]\n    C --> C2[\"Update: Q(s,a) \u2190 Q(s,a) + \u03b1[r + \u03b3Q(s',a') - Q(s,a)]\"]\n\n    D --> D1[\"Off-Policy: Uses max Q(s',a')\"]\n    D --> D2[\"Update: Q(s,a) \u2190 Q(s,a) + \u03b1[r + \u03b3max Q(s',a') - Q(s,a)]\"]\n    D --> D3[Maximization Bias]\n    D3 --> D4[Double Q-Learning]\n```\n\n### **Graph 3: Deep RL & Function Approximation**\n```mermaid\ngraph TD\n    A[Function Approximation] --> B[Linear Methods]\n    A --> C[Deep Learning]\n\n    B --> B1[\"V(s) \u2248 w^T x(s)\"]\n    B --> B2[\"TD Update for Weights\"]\n\n    C --> C1[DQN]\n    C --> C2[Dueling DQN]\n    C --> C3[Policy Gradient]\n\n    C1 --> C1a[Experience Replay]\n    C1 --> C1b[Target Network]\n\n    C2 --> C2a[\"Q(s,a) = V(s) + A(s,a)\"]\n    C2 --> C2b[\"Solves Maximization Bias\"]\n\n    C3 --> C3a[REINFORCE]\n    C3 --> C3b[Actor-Critic]\n```\n\n### **Graph 4: Hierarchical RL**\n```mermaid\ngraph TD\n    A[Hierarchical RL] --> B[Meta-Policies]\n    A --> C[Subtasks]\n    A --> D[Options Framework]\n\n    B --> B1[\"High-Level Decisions\"]\n    B --> B2[\"Selects Subtasks\"]\n\n    C --> C1[\"Breaks Complex Tasks\"]\n    C --> C2[\"Modular Policies\"]\n\n    D --> D1[\"Option = (I, \u03c0, \u03b2)\"]\n    D --> D2[\"Initiation Set I\"]\n    D --> D3[\"Policy \u03c0\"]\n    D --> D4[\"Termination \u03b2\"]\n```\n\n### **Graph 5: Exploration vs Exploitation**\n```mermaid\ngraph TD\n    A[Exploration vs Exploitation] --> B[\u03b5-Greedy]\n    A --> C[Optimistic Initialization]\n    A --> D[UCT]\n\n    B --> B1[\"Random Action w/ Prob \u03b5\"]\n    B --> B2[\"Greedy Action w/ Prob 1-\u03b5\"]\n\n    D --> D1[\"Balances Exploration/Exploitation\"]\n    D --> D2[\"Used in MCTS\"]\n```\n\n---\n## **4. Exam Tips**\n1. **Understand the Bellman Equation**: Critical for DP, TD, and Q-learning.\n2. **Know the Difference**:\n   - **SARSA (on-policy)** vs. **Q-learning (off-policy)**.\n   - **MC (episode-based)** vs. **TD (online updates)**.\n3. **Double Q-Learning**: Mitigates overestimation bias.\n4. **Dueling DQN**: Separates value and advantage streams.\n5. **Policy Gradient**: Directly optimizes policy using gradients.\n6. **HRL**: Meta-policies manage subtasks; options provide modularity.\n7. **UCT**: Used in MCTS for game-playing agents (e.g., AlphaGo).\n\n---\nThis summary covers **all key topics** from the question paper. Focus on **equations, algorithms (SARSA, Q-learning, DQN), and conceptual differences** (e.g., on-policy vs. off-policy). The Mermaid graphs help visualize relationships between concepts. Good luck! \ud83d\ude80",
        "total_score": "50.00"
    },
    "url": "/question-paper/practise/43/efd2c6a3-51a",
    "version": "ee3d5d44299e610bd137ea6200db5ac2"
}