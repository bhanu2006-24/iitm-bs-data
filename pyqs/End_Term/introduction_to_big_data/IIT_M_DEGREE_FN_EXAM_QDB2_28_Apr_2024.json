{
    "component": "Quizpractise/PractiseQuestionPaper",
    "props": {
        "errors": {},
        "auth": {
            "user": null
        },
        "flash": {
            "error": []
        },
        "banner": null,
        "file_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "file_do_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "question_paper": {
            "id": 153,
            "group_id": 19,
            "exam_id": 3,
            "total_score": "1.00",
            "duration": 4,
            "created_at": "2024-11-27T22:26:55.000000Z",
            "updated_at": "2024-11-27T22:26:55.000000Z",
            "question_paper_name": "IIT M DEGREE FN EXAM QDB2 28 Apr 2024",
            "question_paper_description": "2024 Apr28: IIT M FN EXAM QDB2",
            "uuid": "a257da7c-11d",
            "year": 2024,
            "is_new": 0,
            "exam": {
                "id": 3,
                "exam_name": "End Term Quiz",
                "created_at": "2024-10-16T08:08:51.000000Z",
                "updated_at": "2024-10-16T08:08:51.000000Z",
                "uuid": "7a6ff569-f50c-40e7-a08b-f5c334392600",
                "en_id": "eyJpdiI6IkxQSUNTYWFUK21YMTNiSmNqVVU3cXc9PSIsInZhbHVlIjoiN1dEV3BhVTNEQjc2VGt3UUpnbTRhUT09IiwibWFjIjoiMDAwMmFlZTQ2NDMxNTQxMjE3ZGIxZTIzNzA5NzE3ZWY0OWU4YWM3Zjc2NzE2NDlkMTg1MzViMjRlODU2ODE1MSIsInRhZyI6IiJ9"
            },
            "questions": [
                {
                    "id": 45662,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 165,
                    "question_text_1": "THIS IS QUESTION PAPER FOR THE SUBJECT \"DEGREE LEVEL : INTRODUCTION TO BIG DATA (COMPUTER BASED EXAM)\"  ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821211,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "77eba7aa2b7466364bc5a60ebd482c0c",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "4a5c4713-dcc3-4038-8edd-4c1a0c306e8e",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "THIS IS QUESTION PAPER FOR THE SUBJECT \"DEGREE LEVEL : INTRODUCTION TO BIG DATA (COMPUTER BASED EXAM)\"  ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121772,
                            "question_id": 45662,
                            "option_text": "YES",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756188,
                            "option_image_url": null
                        },
                        {
                            "id": 121773,
                            "question_id": 45662,
                            "option_text": "NO",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756189,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45663,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 166,
                    "question_text_1": "What best describes \"big data\"?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821212,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "1ae49850f8bda6fcc066e383228b9e16",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "1563b538-c911-48ce-952b-ae7a821ac9f8",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What best describes \"big data\"?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121774,
                            "question_id": 45663,
                            "option_text": "Big data is used to refer to the set of technologies built around Hadoop",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756190,
                            "option_image_url": null
                        },
                        {
                            "id": 121775,
                            "question_id": 45663,
                            "option_text": "Big data refers to the cloud-native design principle",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756191,
                            "option_image_url": null
                        },
                        {
                            "id": 121776,
                            "question_id": 45663,
                            "option_text": "Big data is about All data, Any Time, Any Method",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756192,
                            "option_image_url": null
                        },
                        {
                            "id": 121777,
                            "question_id": 45663,
                            "option_text": "Big data is about volume, velocity, variety",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756193,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45664,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 167,
                    "question_text_1": "Which of these are implementations of the divide-and-conquer paradigm?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821213,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "eae73b2768f7e1833c0119c78d4fbf21",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "dbe95152-a9f6-40c3-aed9-b85107dc3d8f",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of these are implementations of the divide-and-conquer paradigm?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121778,
                            "question_id": 45664,
                            "option_text": "Spark and Map",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756194,
                            "option_image_url": null
                        },
                        {
                            "id": 121779,
                            "question_id": 45664,
                            "option_text": "Serverless and Message Broker",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756195,
                            "option_image_url": null
                        },
                        {
                            "id": 121780,
                            "question_id": 45664,
                            "option_text": "Hadoop and Spark",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756196,
                            "option_image_url": null
                        },
                        {
                            "id": 121781,
                            "question_id": 45664,
                            "option_text": "MapReduce and Google Cloud Functions",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756197,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45665,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 168,
                    "question_text_1": "A website sees 1 Billion hits every month. The website owner wants to count average hits per customer in the latest month, where a customer is denoted by the IP address of the device from which the customer is accessing the website. The owner has at his disposal a Hadoop cluster of 5 workers and 2 masters each with 1GB of RAM. Which of the following methods is the most likely to finish fastest?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821215,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "f55b117bca4214c3eabafb554fe4e02f",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "a908a5e1-fc8c-48b8-8e88-3b88f520e77b",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "A website sees 1 Billion hits every month. The website owner wants to count average hits per customer in the latest month, where a customer is denoted by the IP address of the device from which the customer is accessing the website. The owner has at his disposal a Hadoop cluster of 5 workers and 2 masters each with 1GB of RAM. Which of the following methods is the most likely to finish fastest?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121782,
                            "question_id": 45665,
                            "option_text": "Write a MapReduce program where the Map does nothing useful, Combinecomputes the aggregated hits per customer, Shuffle combined data based on IP address acrossworkers, and in the Reduce, build hash table on each machine with hash key = IP address andhash value = counter, followed by another Reduce that finally computes the avg on top of all hashvalues.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756203,
                            "option_image_url": null
                        },
                        {
                            "id": 121783,
                            "question_id": 45665,
                            "option_text": "Write a Spark program that forms a Dataframe as grouping by IP address withcount as aggregate, followed by a take into a list in the Spark driver which further computes theaverage of all the individual counts in the list",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756204,
                            "option_image_url": null
                        },
                        {
                            "id": 121784,
                            "question_id": 45665,
                            "option_text": "Write a Spark program that forms a Dataframe as grouping by IP address withcount as aggregate, followed by another stage that computes the avg on top of the Dataframe ofthe first stage",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756205,
                            "option_image_url": null
                        },
                        {
                            "id": 121785,
                            "question_id": 45665,
                            "option_text": "Write a MapReduce program where 2 pairs of Map Reduce are chainedtogether: 1st pair is where the Map does nothing useful, Shuffle data based on IP address acrossworkers, and in the Reduce, build hash table on each machine with hash key = IP address andhash value = counter, while the 2nd pair is another Map that does nothing useful followed by aReduce that finally computes avg on top of all hash values.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756206,
                            "option_image_url": null
                        },
                        {
                            "id": 121786,
                            "question_id": 45665,
                            "option_text": "All will finish in approximately the same time.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756207,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45666,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 169,
                    "question_text_1": "An enterprise software designer wants to leverage the best of Google cloud to minimize the number of administrative overheads associated with her big data pipeline while also getting on-demand scalability without sacrificing flexibility. What option should she choose to best serve these needs?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821216,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "9d7fe3d2feda5e3dc1460477d05d0c72",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "3e3b9d4f-7412-403e-a4f1-43f4c06e6e1e",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "An enterprise software designer wants to leverage the best of Google cloud to minimize the number of administrative overheads associated with her big data pipeline while also getting on-demand scalability without sacrificing flexibility. What option should she choose to best serve these needs?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121787,
                            "question_id": 45666,
                            "option_text": "Build the data pipeline using VMs \u2013 one for Python & one for storing files",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756208,
                            "option_image_url": null
                        },
                        {
                            "id": 121788,
                            "question_id": 45666,
                            "option_text": "Build the data pipeline using Python running on Google Cloud Functionswhere the data is stored on GCS",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756209,
                            "option_image_url": null
                        },
                        {
                            "id": 121789,
                            "question_id": 45666,
                            "option_text": "Build the data pipeline using MapReduce on, data storage on HDFS, anddeploy both on Dataproc",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756210,
                            "option_image_url": null
                        },
                        {
                            "id": 121790,
                            "question_id": 45666,
                            "option_text": "Build the data pipeline using Dataflow on top of data stored on GCS",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756211,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45667,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 170,
                    "question_text_1": "Consider an application that can scale from handling 1000 users to handling 100 million users by simply making copies of itself, logs information about its workings on a central logger backed by Kafka, but has no automation to detect task failures and retry. Which of the following statements is true?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821217,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "a293f8d52d5e99acf738c06d664bba36",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "15ca15e6-c4ea-4295-bd09-5886cf77462d",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider an application that can scale from handling 1000 users to handling 100 million users by simply making copies of itself, logs information about its workings on a central logger backed by Kafka, but has no automation to detect task failures and retry. Which of the following statements is true?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121791,
                            "question_id": 45667,
                            "option_text": "This application has adopted cloud-native design",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756212,
                            "option_image_url": null
                        },
                        {
                            "id": 121792,
                            "question_id": 45667,
                            "option_text": "This application cannot be called as cloud-native since it is not observable at alltimes",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756213,
                            "option_image_url": null
                        },
                        {
                            "id": 121793,
                            "question_id": 45667,
                            "option_text": "This application cannot be called as cloud-native since it is not manageableeasily",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756214,
                            "option_image_url": null
                        },
                        {
                            "id": 121794,
                            "question_id": 45667,
                            "option_text": "This application cannot be called as cloud-native since it is not resilient",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756215,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45668,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 171,
                    "question_text_1": "Consider a file \u201cdata.bin\u201d which is formatted as follows: every data record has 10 key-value pairs of the format \u201ckey:value\u201d, with each pair separated by a comma. Every data record occurs in its own line. You are asked to write a data processing script using Python that scales with big data. Which of the following represents your approach?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821219,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "880ba6e981007dded97f78b724f25958",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "f8a41516-1223-4563-830c-bc44474c1051",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider a file \u201cdata.bin\u201d which is formatted as follows: every data record has 10 key-value pairs of the format \u201ckey:value\u201d, with each pair separated by a comma. Every data record occurs in its own line. You are asked to write a data processing script using Python that scales with big data. Which of the following represents your approach?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121795,
                            "question_id": 45668,
                            "option_text": "Since data.bin is compliant with the RFC 4180, use PySpark\u2019s read_csv() to readthe data as is.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756220,
                            "option_image_url": null
                        },
                        {
                            "id": 121796,
                            "question_id": 45668,
                            "option_text": "Rename the file data.bin to data.csv to make it compliant with RFC 4180 andthen use PySpark\u2019s read_csv() to read the data",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756221,
                            "option_image_url": null
                        },
                        {
                            "id": 121797,
                            "question_id": 45668,
                            "option_text": "The problem cannot be solved since the file cannot be converted to a validformat for reading consistently without additional information",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756222,
                            "option_image_url": null
                        },
                        {
                            "id": 121798,
                            "question_id": 45668,
                            "option_text": "Write PySpark code to read all lines in data.bin, use string split on \u201c,\u201d asdelimiter, and then collect all column names and corresponding values into a RDD for furtherprocessing",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756223,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45669,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 172,
                    "question_text_1": "Consider the program outline as below running on a Spark cluster of 1 driver and 4 worker nodes with 2 executors per worker node:  ",
                    "question_image_1": "wCvJeUGWWi0UPd3yEmWSY3V7Ofsok56RjTbus9fc18nlTghDxD.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821220,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "a862171936994e6192c9b16b376539da",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": "Consider the program outline as below running on a Spark cluster of 1 driver and 4 worker nodes with 2 executors per worker node:    Consider the below mutually-exclusive characterisations about the program: i. Program will not run since RDD transformation operation map() runs on Executors that is referencing a variable train_master which is declared in the Spark driver ii. Program will run successfully and produce the count of bookings that match for trains present in the train_master  For either of the 2 characterizations, consider the following (one or more) actions you could carry out in order to improve on the characterizations: 1. Program will need to be changed to bring the train_master into the Closure for executors to pick it up 2. Program will perform more poorly as the number of executors per worker node increases. Broadcast of train_master will need to be used to make it perform better. 3. Program will perform more poorly as the number of worker nodes increases (keeping number of executors per worker node constant). Broadcast of train_master will need to be used to make it perform better.  Which of the following combinations of statements are correct?",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "a24296e9-ba74-4e99-b83b-59c3d92661f4",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/wCvJeUGWWi0UPd3yEmWSY3V7Ofsok56RjTbus9fc18nlTghDxD.png"
                    ],
                    "question_texts": [
                        "Consider the program outline as below running on a Spark cluster of 1 driver and 4 worker nodes with 2 executors per worker node:  ",
                        "Consider the program outline as below running on a Spark cluster of 1 driver and 4 worker nodes with 2 executors per worker node:    Consider the below mutually-exclusive characterisations about the program: i. Program will not run since RDD transformation operation map() runs on Executors that is referencing a variable train_master which is declared in the Spark driver ii. Program will run successfully and produce the count of bookings that match for trains present in the train_master  For either of the 2 characterizations, consider the following (one or more) actions you could carry out in order to improve on the characterizations: 1. Program will need to be changed to bring the train_master into the Closure for executors to pick it up 2. Program will perform more poorly as the number of executors per worker node increases. Broadcast of train_master will need to be used to make it perform better. 3. Program will perform more poorly as the number of worker nodes increases (keeping number of executors per worker node constant). Broadcast of train_master will need to be used to make it perform better.  Which of the following combinations of statements are correct?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121799,
                            "question_id": 45669,
                            "option_text": "i and 1",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756224,
                            "option_image_url": null
                        },
                        {
                            "id": 121800,
                            "question_id": 45669,
                            "option_text": "i and 1 followed by 2",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756225,
                            "option_image_url": null
                        },
                        {
                            "id": 121801,
                            "question_id": 45669,
                            "option_text": "i and 1 followed by 2 & 3",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756226,
                            "option_image_url": null
                        },
                        {
                            "id": 121802,
                            "question_id": 45669,
                            "option_text": "ii and both 2 & 3",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756227,
                            "option_image_url": null
                        },
                        {
                            "id": 121803,
                            "question_id": 45669,
                            "option_text": "ii and 2",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756228,
                            "option_image_url": null
                        },
                        {
                            "id": 121804,
                            "question_id": 45669,
                            "option_text": "ii and 3",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756229,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45670,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 173,
                    "question_text_1": "What happens when a Spark Structured Streaming pipeline operating with Kafka as source is subject to a failure of a machine in either of the Kafka cluster or the Spark cluster?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821225,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "2efef7e781126019a71357d87382deea",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "d9a950c1-59be-4f11-9128-85fb77f59f0d",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What happens when a Spark Structured Streaming pipeline operating with Kafka as source is subject to a failure of a machine in either of the Kafka cluster or the Spark cluster?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121805,
                            "question_id": 45670,
                            "option_text": "Failure of a machine in the Kafka cluster will result in an Exception in the Sparkpipeline which will then fail and halt.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756252,
                            "option_image_url": null
                        },
                        {
                            "id": 121806,
                            "question_id": 45670,
                            "option_text": "The Spark pipeline will not be able to start again from previously committedoffset by restarting itself, resulting in at least-once processing semantics",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756253,
                            "option_image_url": null
                        },
                        {
                            "id": 121807,
                            "question_id": 45670,
                            "option_text": "Irrespective of whatever machine fails, Spark will throw an error and halt.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756254,
                            "option_image_url": null
                        },
                        {
                            "id": 121808,
                            "question_id": 45670,
                            "option_text": "Data that is being processed will not be processed again, resulting in atmost-once semantics.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756255,
                            "option_image_url": null
                        },
                        {
                            "id": 121809,
                            "question_id": 45670,
                            "option_text": "The pipeline will be restarted automatically by Spark which is able to pick upthe exact data from Kafka which was being processed at the time of error, resulting in exactly-once semantics.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756256,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45671,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 174,
                    "question_text_1": "A big data streaming application that uses Kafka as source is observed to be really lagging behind currently live data. The Kafka cluster has 2 broker nodes and this application is reading from 1 topic that has 10 partitions. On closer investigation, it was found that Kafka is not scaling to the velocity of input data coming in. What can you first try to do to scale Kafka further while incurring minimal overall costs?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821226,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "ba33bf13811f1dc7792f65d1734bdc4f",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "2a291eee-2fd3-4727-9d24-4391aea7a133",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "A big data streaming application that uses Kafka as source is observed to be really lagging behind currently live data. The Kafka cluster has 2 broker nodes and this application is reading from 1 topic that has 10 partitions. On closer investigation, it was found that Kafka is not scaling to the velocity of input data coming in. What can you first try to do to scale Kafka further while incurring minimal overall costs?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121810,
                            "question_id": 45671,
                            "option_text": "Add disks to each broker in the cluster, and disks are the cheapest computercomponent",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756257,
                            "option_image_url": null
                        },
                        {
                            "id": 121811,
                            "question_id": 45671,
                            "option_text": "Increase memory in each of the brokers in the cluster. While cost of memory ismore than cost of disks, it is still cheaper than adding brokers and helps to scale.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756258,
                            "option_image_url": null
                        },
                        {
                            "id": 121812,
                            "question_id": 45671,
                            "option_text": "Add new brokers to the cluster, even though this is more expensive than theother options this is the only foolproof way to scale.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756259,
                            "option_image_url": null
                        },
                        {
                            "id": 121813,
                            "question_id": 45671,
                            "option_text": "Create more topics and change input application to reroute data to all topics tobe able to spread input data better. This is nearly the least expensive since only developer effort isrequired to change application.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756260,
                            "option_image_url": null
                        },
                        {
                            "id": 121814,
                            "question_id": 45671,
                            "option_text": "Double the number of partitions for this single topic to be able to spread inputdata better. This is the least expensive since only administrator effort is required without changingapplication.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756261,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45672,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 175,
                    "question_text_1": "A company with headquarters (HQ) in the Middle East operates on a Sunday-Thursday weekday schedule with Friday & Saturday as weekend days. It computes end of week revenue numbers using an ETL pipeline by first computing sales for each day at 1AM local time of the next day, and then summing up the weekly sales every Sunday early morning at 3AM local time. This number gets reported to leadership every Sunday morning 9AM local time, so the ETL pipeline is scheduled to run every Sunday morning at 8AM local time. As a result of management change, it has decided to relocate its HQ to India. Which of the following changes will need to be done to its ETL pipeline to ensure the correct output continues to be produced?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821228,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "f593588de96aa657da3d8762895682f4",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "6472fb71-5664-4370-b3d0-00e82a421e9e",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "A company with headquarters (HQ) in the Middle East operates on a Sunday-Thursday weekday schedule with Friday & Saturday as weekend days. It computes end of week revenue numbers using an ETL pipeline by first computing sales for each day at 1AM local time of the next day, and then summing up the weekly sales every Sunday early morning at 3AM local time. This number gets reported to leadership every Sunday morning 9AM local time, so the ETL pipeline is scheduled to run every Sunday morning at 8AM local time. As a result of management change, it has decided to relocate its HQ to India. Which of the following changes will need to be done to its ETL pipeline to ensure the correct output continues to be produced?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121815,
                            "question_id": 45672,
                            "option_text": "The business time for the final weekly sum operation needs to be changed tothat of Monday 3AM India time instead of Sunday 3AM Middle East time.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756267,
                            "option_image_url": null
                        },
                        {
                            "id": 121816,
                            "question_id": 45672,
                            "option_text": "There is zero change needed since neither event time nor business time ischanging whereas only the operational time is changing.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756268,
                            "option_image_url": null
                        },
                        {
                            "id": 121817,
                            "question_id": 45672,
                            "option_text": "Since time zone has changed as well as week definition too, the definition ofbusiness time has changed. So, the ETL has to be rewritten entirely.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756269,
                            "option_image_url": null
                        },
                        {
                            "id": 121818,
                            "question_id": 45672,
                            "option_text": "Nothing needs to change since daily sales is available at 1AM Middle East timewhich is anyway behind India time and so the numbers will be available before leadership comesin at 9AM.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756270,
                            "option_image_url": null
                        },
                        {
                            "id": 121819,
                            "question_id": 45672,
                            "option_text": "Event time has changed since the event of week ending has changed indefinition, and so the ETL needs to be changed to consider the new event in the data.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756271,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45673,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 176,
                    "question_text_1": "You are appointed as a Data Engineer in a company that has a legacy reporting application written in Java which suffers from performance problems. The reporting application plots dashboards with near real-time refresh (once every minute) of key business indicators to help management take live decisions. The application reads data directly from the source database of MongoDB, aggregates using simple counts and shows them visually in a UI. The performance problem of this application comes because the source database is at times overloaded and therefore the dashboard is not able to refresh fast enough. Choose the best option that gives the best performance with minimal maintenance effort:",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821229,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "8792b65473fe7daa608185fdb9919099",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "d08869be-e389-4485-b0a0-4e77709ee5e1",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are appointed as a Data Engineer in a company that has a legacy reporting application written in Java which suffers from performance problems. The reporting application plots dashboards with near real-time refresh (once every minute) of key business indicators to help management take live decisions. The application reads data directly from the source database of MongoDB, aggregates using simple counts and shows them visually in a UI. The performance problem of this application comes because the source database is at times overloaded and therefore the dashboard is not able to refresh fast enough. Choose the best option that gives the best performance with minimal maintenance effort:"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121820,
                            "question_id": 45673,
                            "option_text": "Since MongoDB is OLTP, it is not able to support business reporting. So,replace it with Hadoop which supports OLAP better",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756272,
                            "option_image_url": null
                        },
                        {
                            "id": 121821,
                            "question_id": 45673,
                            "option_text": "Convert the application from using plain Java to using Spark Streaming in Java",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756273,
                            "option_image_url": null
                        },
                        {
                            "id": 121822,
                            "question_id": 45673,
                            "option_text": "Extract raw data from MongoDB using Change Data Capture (CDC) once everyminute into Kafka, and then use Spark Streaming to compute the KPIs and then populate into aNoSQL DB like Redis for the UI to consume.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756274,
                            "option_image_url": null
                        },
                        {
                            "id": 121823,
                            "question_id": 45673,
                            "option_text": "Query MongoDB every 1 minute for new data using a check on documentinserted timestamp, use Spark Streaming to compute the KPIs with the queried data, and thenpopulate into a NoSQL DB like Redis for the UI to consume.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756275,
                            "option_image_url": null
                        },
                        {
                            "id": 121824,
                            "question_id": 45673,
                            "option_text": "Convert application to using Python along with a NoSQL database for storingand retrieving the aggregated counts.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756276,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45674,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 177,
                    "question_text_1": "Dhoni is on the crease with a bat in hand that has sensors embedded throughout. The sensors talk to the spider cam every second. The spider cam is itself a powerful ARM based computer which has connectivity to the cloud through the wire on which it hangs. Using this connectivity, it can send as much or as little data as required and also receive instructions from the cloud. There is a machine learning model which suggests to the batsman to loosen the grip on his bat or tighten it based on the shots played using the sensor measurements. The way the suggestion happens is using dynamic vibration intensity communicated to the sensors embedded in the bat handle. Your task is to design the data pipeline that enables such feedback to Dhoni ideally before every ball with as much accuracy as possible throughout the match. Which of the following options best satisfies the requirements?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821230,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "6cad752ebe07e7620d97316f2f7d7e63",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "44eec1ec-0bc1-4a53-9bcb-30e96fa370a8",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Dhoni is on the crease with a bat in hand that has sensors embedded throughout. The sensors talk to the spider cam every second. The spider cam is itself a powerful ARM based computer which has connectivity to the cloud through the wire on which it hangs. Using this connectivity, it can send as much or as little data as required and also receive instructions from the cloud. There is a machine learning model which suggests to the batsman to loosen the grip on his bat or tighten it based on the shots played using the sensor measurements. The way the suggestion happens is using dynamic vibration intensity communicated to the sensors embedded in the bat handle. Your task is to design the data pipeline that enables such feedback to Dhoni ideally before every ball with as much accuracy as possible throughout the match. Which of the following options best satisfies the requirements?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121825,
                            "question_id": 45674,
                            "option_text": "Ingest all data into Pub/Sub, process using Google Cloud Dataflow, invoke theML model, and then write back output from Cloud to the spider cam to relay to the bat.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756277,
                            "option_image_url": null
                        },
                        {
                            "id": 121826,
                            "question_id": 45674,
                            "option_text": "Compress the ML model to fit into the spider cam\u2019s available resource, andwrite pipelines to execute the model in the spider cam itself",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756278,
                            "option_image_url": null
                        },
                        {
                            "id": 121827,
                            "question_id": 45674,
                            "option_text": "Compress the ML model to fit into the spider cam\u2019s available resource, andwrite pipelines to execute in the spider cam itself, with periodically data being sent to the cloud,retrain the model using Google Cloud ML and then redeploy the model to the spider cam.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756279,
                            "option_image_url": null
                        },
                        {
                            "id": 121828,
                            "question_id": 45674,
                            "option_text": "Compress the data in the spider cam every 5 seconds, write to Pub/Sub thecompressed data, invoke the ML model and then write back output from Cloud to the spider camto relay to the bat.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756280,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45675,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 178,
                    "question_text_1": "In the class, we saw the UDF for mobilenet_v2. By definition, UDFs are scalar. In Spark, there is another class of user defined routines called UDAFs, which stand for User Defined Aggregator Functions. UDAFs are meant to provide a means to write a custom aggregation function which aggregates over a grouping of values to arrive at a single value. UDAF structure differs saliently from UDFs in that it provides a buffer to keep track of intermediate state before finalizing aggregate output. Why is a buffer required for UDAF and not for a UDF?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821231,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "c476b969f30843924fb00573615cb1da",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "a72697a2-def5-4904-ae72-7589889c1281",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "In the class, we saw the UDF for mobilenet_v2. By definition, UDFs are scalar. In Spark, there is another class of user defined routines called UDAFs, which stand for User Defined Aggregator Functions. UDAFs are meant to provide a means to write a custom aggregation function which aggregates over a grouping of values to arrive at a single value. UDAF structure differs saliently from UDFs in that it provides a buffer to keep track of intermediate state before finalizing aggregate output. Why is a buffer required for UDAF and not for a UDF?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121829,
                            "question_id": 45675,
                            "option_text": "A UDF is a scalar operation executing on 1 row at a time and producing outputimmediately, and therefore there is no intermediate output necessary. Whereas, a UDAF operateson multiple rows which will require multiple passes for the final output thereby requiring a buffer.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756281,
                            "option_image_url": null
                        },
                        {
                            "id": 121830,
                            "question_id": 45675,
                            "option_text": "A UDF is a scalar operation executing on 1 row at a time and producing outputimmediately, and therefore there is no intermediate output necessary. Whereas, a UDAF operateson multiple rows of unbounded size requiring a divide-and-conquer approach for computingaggregates, which uses the intermediate buffer to store partial values before finalizing the resultaggregate.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756282,
                            "option_image_url": null
                        },
                        {
                            "id": 121831,
                            "question_id": 45675,
                            "option_text": "A UDF is a scalar operation executing on many rows at a time, grouped by akey and producing a single output, and therefore there is no intermediate output necessary.Whereas, a UDAF operates on multiple rows which will require multiple passes for the final outputthereby requiring a buffer.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756283,
                            "option_image_url": null
                        },
                        {
                            "id": 121832,
                            "question_id": 45675,
                            "option_text": "A UDF is a scalar operation executing on many rows at a time, grouped by akey and producing a single output, and therefore there is no intermediate output necessary.Whereas, a UDAF operates on multiple rows of unbounded size requiring a divide-and-conquerapproach for computing aggregates, which uses the intermediate buffer to store partial valuesbefore finalizing the result aggregate.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756284,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45676,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 179,
                    "question_text_1": "In the class, we saw the UDF for mobilenet_v2. Specifically, the predict() function contained the below lines of code  ",
                    "question_image_1": "BwL454JKtBfNXa4LT28IWG6H0XNjMW5WouxBOlWXHsg0DhcLzD.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821232,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "ce06d3b24cf089e62804e4e9ef021d05",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": "In the class, we saw the UDF for mobilenet_v2. Specifically, the predict() function contained the below lines of code    Suppose there\u2019s a new transformer model from Facebook that is orders of magnitude bigger than mobilenet_v2 but that is able to provide better F1 score. Will these lines of code still be required in the pipeline?",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "940a428c-8d98-4f94-857b-6dba2dd10085",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/BwL454JKtBfNXa4LT28IWG6H0XNjMW5WouxBOlWXHsg0DhcLzD.png"
                    ],
                    "question_texts": [
                        "In the class, we saw the UDF for mobilenet_v2. Specifically, the predict() function contained the below lines of code  ",
                        "In the class, we saw the UDF for mobilenet_v2. Specifically, the predict() function contained the below lines of code    Suppose there\u2019s a new transformer model from Facebook that is orders of magnitude bigger than mobilenet_v2 but that is able to provide better F1 score. Will these lines of code still be required in the pipeline?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121833,
                            "question_id": 45676,
                            "option_text": "Yes, since they check the syntax of the model function so that there are noerrors",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756285,
                            "option_image_url": null
                        },
                        {
                            "id": 121834,
                            "question_id": 45676,
                            "option_text": "Yes, they invoke PyTorch libraries that have already been setup for model scoring on GCP usingAPIs embedded within the function",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756286,
                            "option_image_url": null
                        },
                        {
                            "id": 121835,
                            "question_id": 45676,
                            "option_text": "No, since the primary function of these lines of code is to eliminate repeatedDL model loads as DL models are large in size",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756287,
                            "option_image_url": null
                        },
                        {
                            "id": 121836,
                            "question_id": 45676,
                            "option_text": "Yes, they are Map-style UDFs that make it an embarrassingly parallelcomputation thus making the execution parallelized and fast.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756288,
                            "option_image_url": null
                        },
                        {
                            "id": 121837,
                            "question_id": 45676,
                            "option_text": "Yes, since the new model will potentially have large load times.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756289,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45677,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 180,
                    "question_text_1": "You are given a Spark Streaming pipeline that invokes a pre-trained DL model for every image it receives as input and produces the classification result in quick time. The model with the best recall rate from the PyTorch library already runs in under 3 seconds on an average, when executing on a single GPU Spark worker machine. However, your management has instructed you to reduce the cost of AI projects significantly. What is the best option to explore to meet the expectations without compromising on false negatives while also being within 10-20% of the average execution time?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821233,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "55599cabd9287a8f6ccbd49845026579",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "144e425c-f0d8-4b13-96a6-2d0a73e0e6c5",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are given a Spark Streaming pipeline that invokes a pre-trained DL model for every image it receives as input and produces the classification result in quick time. The model with the best recall rate from the PyTorch library already runs in under 3 seconds on an average, when executing on a single GPU Spark worker machine. However, your management has instructed you to reduce the cost of AI projects significantly. What is the best option to explore to meet the expectations without compromising on false negatives while also being within 10-20% of the average execution time?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121838,
                            "question_id": 45677,
                            "option_text": "Build a custom model that compresses the highest recall rate model justenough to be able to execute within the stipulated time, and measure recall.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756290,
                            "option_image_url": null
                        },
                        {
                            "id": 121839,
                            "question_id": 45677,
                            "option_text": "Use a different DL model from PyTorch that is already compressed to half thesize.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756291,
                            "option_image_url": null
                        },
                        {
                            "id": 121840,
                            "question_id": 45677,
                            "option_text": "Remove complexity associated with Spark Streaming and convert the modelexecution pipeline into a single threaded Python application running on the same GPU machine.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756292,
                            "option_image_url": null
                        },
                        {
                            "id": 121841,
                            "question_id": 45677,
                            "option_text": "Change Spark machine to use CPUs and train a fresh pipeline to achieveobjectives.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756293,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45678,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 181,
                    "question_text_1": "Consider a Structured Streaming application running on Google Dataproc firing up every 10 seconds, consuming any number of records from Kafka available since last read, and emitting some computed answers to another Kafka topic. Consider also that apart from the functional logic, the same application is also emitting into a file the start time and end time of every batch invocation for audit purposes.  Assume there is a failure in one of the Dataproc machines that results in a failure of a specific run. For the external world (i.e. anybody consuming the outputs of this application), will they see any change in output as a result of the failure at all, or will the only visible impact of failure be of slower performance for the failed-and-retried run?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821235,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "ed74220a73aeaa779337242771fb5cb3",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "1d434db4-bbaa-473c-9e97-c7d1d663de26",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider a Structured Streaming application running on Google Dataproc firing up every 10 seconds, consuming any number of records from Kafka available since last read, and emitting some computed answers to another Kafka topic. Consider also that apart from the functional logic, the same application is also emitting into a file the start time and end time of every batch invocation for audit purposes.  Assume there is a failure in one of the Dataproc machines that results in a failure of a specific run. For the external world (i.e. anybody consuming the outputs of this application), will they see any change in output as a result of the failure at all, or will the only visible impact of failure be of slower performance for the failed-and-retried run?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121842,
                            "question_id": 45678,
                            "option_text": "No, the failure is not visible. The only visible effect for the external world wouldbe in the form of a slowdown in runtime for completion of that mini-batch as StructuredStreaming retries the mini-batch that failed thus taking twice as much time as normal.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756298,
                            "option_image_url": null
                        },
                        {
                            "id": 121843,
                            "question_id": 45678,
                            "option_text": "No, the failure is not visible since Structured Streaming uses transactions andidempotence to achieve exactly-once processing.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756299,
                            "option_image_url": null
                        },
                        {
                            "id": 121844,
                            "question_id": 45678,
                            "option_text": "No, the failure is not visible since Structured Streaming can process the samedata in a retry resulting in the same outputs again.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756300,
                            "option_image_url": null
                        },
                        {
                            "id": 121845,
                            "question_id": 45678,
                            "option_text": "Yes, the failure is visible because the side effect of emitting timestamps in abatch will be visible as 2 consecutive Start timestamps without any end timestamp as StructuredStreaming retries the failed batch.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756301,
                            "option_image_url": null
                        },
                        {
                            "id": 121846,
                            "question_id": 45678,
                            "option_text": "Yes, the failure is visible since the logs in the backend of Spark StructuredStreaming are also logging the state of the machine.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756302,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45679,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 182,
                    "question_text_1": "Let us say we are using structured streaming for continuously reading data from Kafka and storing the results back into a Kafka topic using window function aggregates. Now, instead, we decide that we need to just perform a one-time batch operation using the same logic, where there is a need to read specific data from Kafka (i.e. using pre-determined offsets). How will we need to modify the code to make it work?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821236,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "c4443955772298e10ec733f77c09ef2d",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "80b725f3-562c-4337-9532-213c2f3e5923",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Let us say we are using structured streaming for continuously reading data from Kafka and storing the results back into a Kafka topic using window function aggregates. Now, instead, we decide that we need to just perform a one-time batch operation using the same logic, where there is a need to read specific data from Kafka (i.e. using pre-determined offsets). How will we need to modify the code to make it work?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121847,
                            "question_id": 45679,
                            "option_text": "The read and write commands will remain the same, but the remaining codewill need to be modified, as operations on streaming dataframes are not supported on staticdataframes.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756303,
                            "option_image_url": null
                        },
                        {
                            "id": 121848,
                            "question_id": 45679,
                            "option_text": "The entire code will need to be modified as the APIs for stream and batchprocessing are completely different.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756304,
                            "option_image_url": null
                        },
                        {
                            "id": 121849,
                            "question_id": 45679,
                            "option_text": "The read and write commands need to be modified to specify that it\u2019s a batchoperation. Further, the specific logic of window functions will also need to be modified since thereare no time windows anymore in batch processing.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756305,
                            "option_image_url": null
                        },
                        {
                            "id": 121850,
                            "question_id": 45679,
                            "option_text": "Only the read and write commands need to be modified to specify that it's abatch operation.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756306,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45680,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 183,
                    "question_text_1": "Consider a Kafka system that has two brokers with the exact same resources. Let\u2019s consider a topic A with a single partition, whose two copies are being maintained in sync by Kafka. Consider the following cases:  Case 1: Each broker has one copy of the partition Case 2: Both the copies reside in the same broker  Two of the key promises of Kafka are:  (i) Availability (ii) Throughput  Regarding which of the above promises, case 1 is at an advantage as compared to case 2?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821237,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "808958b3ac687e2b27a4b733243ff946",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "83f91a81-f30b-4add-acbb-d131b75b9cc7",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider a Kafka system that has two brokers with the exact same resources. Let\u2019s consider a topic A with a single partition, whose two copies are being maintained in sync by Kafka. Consider the following cases:  Case 1: Each broker has one copy of the partition Case 2: Both the copies reside in the same broker  Two of the key promises of Kafka are:  (i) Availability (ii) Throughput  Regarding which of the above promises, case 1 is at an advantage as compared to case 2?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121851,
                            "question_id": 45680,
                            "option_text": "Only (ii)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756307,
                            "option_image_url": null
                        },
                        {
                            "id": 121852,
                            "question_id": 45680,
                            "option_text": "Only (i)",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756308,
                            "option_image_url": null
                        },
                        {
                            "id": 121853,
                            "question_id": 45680,
                            "option_text": "Both (i) and (ii)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756309,
                            "option_image_url": null
                        },
                        {
                            "id": 121854,
                            "question_id": 45680,
                            "option_text": "Neither (i) nor (ii)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756310,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45681,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 184,
                    "question_text_1": "Kubernetes is an open-source system for automating deployment, scaling and management of containerized applications. Google Datastore and HBase are both highly-scalable NoSQL database systems for interactive, real-time applications. Consider the following pipeline choices for effecting the same outcome:  (i) Shell producer on VM on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 HBase on same Hadoop VMs on GCP (ii) Shell producer in Google Cloud Function \u2192 Kafka VM on GCP \u2192 Dataflow \u2192 Datastore (iii) Shell producer in Kubernetes on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Google Dataproc \u2192 Datastore (iv) Shell producer on VM on GCP\u2192 Pub/Sub \u2192 Dataflow\u2192 Datastore (v) Shell producer in Kubernetes on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 HBase on same Hadoop VMs on GCP  Which option below represents the correct order of pipeline options that has the \u201cmost IaaS\u201d entry to the left and the \u201cmost PaaS\u201d entry to the right?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821238,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "031b27c68a8a3c0dac03b1eda3b28730",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "0741fc13-1648-4a0e-a3c5-13b588cb3bf0",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Kubernetes is an open-source system for automating deployment, scaling and management of containerized applications. Google Datastore and HBase are both highly-scalable NoSQL database systems for interactive, real-time applications. Consider the following pipeline choices for effecting the same outcome:  (i) Shell producer on VM on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 HBase on same Hadoop VMs on GCP (ii) Shell producer in Google Cloud Function \u2192 Kafka VM on GCP \u2192 Dataflow \u2192 Datastore (iii) Shell producer in Kubernetes on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Google Dataproc \u2192 Datastore (iv) Shell producer on VM on GCP\u2192 Pub/Sub \u2192 Dataflow\u2192 Datastore (v) Shell producer in Kubernetes on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 HBase on same Hadoop VMs on GCP  Which option below represents the correct order of pipeline options that has the \u201cmost IaaS\u201d entry to the left and the \u201cmost PaaS\u201d entry to the right?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121855,
                            "question_id": 45681,
                            "option_text": "(i), (v), (iii), (iv), (ii)",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756311,
                            "option_image_url": null
                        },
                        {
                            "id": 121856,
                            "question_id": 45681,
                            "option_text": "(i), (ii), (iii), (iv), (v)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756312,
                            "option_image_url": null
                        },
                        {
                            "id": 121857,
                            "question_id": 45681,
                            "option_text": "(ii), (iii), (i), (iv), (v)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756313,
                            "option_image_url": null
                        },
                        {
                            "id": 121858,
                            "question_id": 45681,
                            "option_text": "(v), (iii), (i), (iv), (ii)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756314,
                            "option_image_url": null
                        },
                        {
                            "id": 121859,
                            "question_id": 45681,
                            "option_text": "All are equally PaaS / IaaS",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756315,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45682,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 185,
                    "question_text_1": "A Spark Streaming application is configured to execute once per minute. However, each run takes 10+ minutes consistently resulting in a never-ending backlog of work. The code in a nutshell looks as follows:  ",
                    "question_image_1": "REr23awXkYljBHMLQsehuxm0Yiw93zoAVPbPAdhBY8Wm6DgBgA.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821239,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "334d0c663e0d218773fdb403cd064c69",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": "A Spark Streaming application is configured to execute once per minute. However, each run takes 10+ minutes consistently resulting in a never-ending backlog of work. The code in a nutshell looks as follows:    Your goal is to optimize the code to bring down execution of each iteration within 1 minute. Which of the following represent options that will help in this mission?",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "73ae11b0-55f9-4688-b63c-c7b0ad59263e",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/REr23awXkYljBHMLQsehuxm0Yiw93zoAVPbPAdhBY8Wm6DgBgA.png"
                    ],
                    "question_texts": [
                        "A Spark Streaming application is configured to execute once per minute. However, each run takes 10+ minutes consistently resulting in a never-ending backlog of work. The code in a nutshell looks as follows:  ",
                        "A Spark Streaming application is configured to execute once per minute. However, each run takes 10+ minutes consistently resulting in a never-ending backlog of work. The code in a nutshell looks as follows:    Your goal is to optimize the code to bring down execution of each iteration within 1 minute. Which of the following represent options that will help in this mission?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121860,
                            "question_id": 45682,
                            "option_text": "",
                            "option_image": "qrxMxzoJF6ErHAhP87MYju0szCZhFw2eGKUHmrwrHi4CeGRXWG.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756316,
                            "option_image_url": "app/option_images/qrxMxzoJF6ErHAhP87MYju0szCZhFw2eGKUHmrwrHi4CeGRXWG.png"
                        },
                        {
                            "id": 121861,
                            "question_id": 45682,
                            "option_text": "",
                            "option_image": "2psjErmwFGOOqXN4FeXLQQUa49ZIQLypBgalH2rQ3lBtsPvzvw.png",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756317,
                            "option_image_url": "app/option_images/2psjErmwFGOOqXN4FeXLQQUa49ZIQLypBgalH2rQ3lBtsPvzvw.png"
                        },
                        {
                            "id": 121862,
                            "question_id": 45682,
                            "option_text": "",
                            "option_image": "Ql2PUDGqb0pEcw1fQlT6pSAhCllEqg5FGdpGy2osQyh37ufCdv.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756318,
                            "option_image_url": "app/option_images/Ql2PUDGqb0pEcw1fQlT6pSAhCllEqg5FGdpGy2osQyh37ufCdv.png"
                        },
                        {
                            "id": 121863,
                            "question_id": 45682,
                            "option_text": "",
                            "option_image": "JV24RsC0TXPke90dnP67rEhMnDbZsLLuvRqToSCiNiF9kYMwgF.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756319,
                            "option_image_url": "app/option_images/JV24RsC0TXPke90dnP67rEhMnDbZsLLuvRqToSCiNiF9kYMwgF.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45683,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 186,
                    "question_text_1": "You are given a Spark program that runs on a Google Dataproc cluster on a daily schedule from 8PM-10PM to produce as output the total amount of purchases made by every customer the previous day. The data is coming into GCS every minute from a variety of sources as standalone files. Therefore, the business leader now feels that having to wait till 10PM the next day is no longer acceptable and instead wants approximate purchase information for each customer at least every 5 minutes. What\u2019s more, she wants to be able to change this time window later as she pleases without involving you. Which amongst the below represents the best option to achieve the above?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821240,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "b4909eb218d7c845c6b0b2767dc9be26",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "040c3097-406d-4e7a-a86f-5d56831e6727",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are given a Spark program that runs on a Google Dataproc cluster on a daily schedule from 8PM-10PM to produce as output the total amount of purchases made by every customer the previous day. The data is coming into GCS every minute from a variety of sources as standalone files. Therefore, the business leader now feels that having to wait till 10PM the next day is no longer acceptable and instead wants approximate purchase information for each customer at least every 5 minutes. What\u2019s more, she wants to be able to change this time window later as she pleases without involving you. Which amongst the below represents the best option to achieve the above?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121864,
                            "question_id": 45683,
                            "option_text": "Change the code to run every 5 minutes, no other change required",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756320,
                            "option_image_url": null
                        },
                        {
                            "id": 121865,
                            "question_id": 45683,
                            "option_text": "Change the code to leverage Spark Streaming with streaming window as \u201c5minutes\u201d, & let her manage the execution of the code on Dataproc",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756321,
                            "option_image_url": null
                        },
                        {
                            "id": 121866,
                            "question_id": 45683,
                            "option_text": "Change the code to leverage Spark Streaming with streaming window as \u201c5minutes\u201d, convert from Dataproc to Dataflow, & let her manage the execution of the code onDataflow",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756322,
                            "option_image_url": null
                        },
                        {
                            "id": 121867,
                            "question_id": 45683,
                            "option_text": "Write a Cloud Function to move all incoming per-minute standalone files fromGCS to Pub/Sub, change the code to leverage Spark Streaming with streaming window as \u201c5 mins\u201d,convert from Dataproc to Dataflow, point source to Pub/Sub, & let her manage the execution ofthe code on Dataflow",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756323,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45684,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 187,
                    "question_text_1": "Since it is the onset of summer, there is a surge in railway ticket bookings. The business head at IRCTC is interested in a real-time view of all of her stations irrespective of whether there were bookings or not. She wants to see this be presented in a monitor mounted in her office wall that refreshes with the latest info on an India map every 1 minute. Along with the info, there also needs to be the current time so that she gets a visual confirmation that this is the latest data. This dashboard allows her to plan for new summer-special trains as required. What solution option below best solves for the need?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821241,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "d5f029d564a7a4952f90913d2cde7bbb",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "e1d91a62-2fb0-43ce-acf7-4eb2e7b003d6",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Since it is the onset of summer, there is a surge in railway ticket bookings. The business head at IRCTC is interested in a real-time view of all of her stations irrespective of whether there were bookings or not. She wants to see this be presented in a monitor mounted in her office wall that refreshes with the latest info on an India map every 1 minute. Along with the info, there also needs to be the current time so that she gets a visual confirmation that this is the latest data. This dashboard allows her to plan for new summer-special trains as required. What solution option below best solves for the need?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121868,
                            "question_id": 45684,
                            "option_text": "Route a copy of the ticket purchase to a Kafka topic, use Spark StructuredStreaming to continuously read from this topic and update the aggregates by destinations, andemit using output mode \u201cUpdate\u201d.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756324,
                            "option_image_url": null
                        },
                        {
                            "id": 121869,
                            "question_id": 45684,
                            "option_text": "Route a copy of the ticket purchase to a Kafka topic, use Spark StructuredStreaming to periodically read from this topic every 1 minute and update the aggregates bydestinations, and emit all aggregates using the output mode \u201cComplete\u201d.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756325,
                            "option_image_url": null
                        },
                        {
                            "id": 121870,
                            "question_id": 45684,
                            "option_text": "Route a copy of the ticket purchase to a Kafka topic, use Spark StructuredStreaming to periodically read from this topic every 1 minute and count the destinations in thatbatch, and emit only all aggregates in that batch using the output mode \u201cAppend\u201d.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756326,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45685,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 188,
                    "question_text_1": "What option(s) best describe the differences between MapReduce and Spark?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821214,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "a5722f0e3e01cf9b23d79e8237e780c4",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "3edfd68f-2df7-4679-bc19-e19ae840a660",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What option(s) best describe the differences between MapReduce and Spark?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121871,
                            "question_id": 45685,
                            "option_text": "MapReduce leverages memory heavily, while Spark optimizes for disk-based computations",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756198,
                            "option_image_url": null
                        },
                        {
                            "id": 121872,
                            "question_id": 45685,
                            "option_text": "MapReduce forces barrier synchronization after every step, while Spark usesdirected acyclic graphs to execute as many steps as possible in parallel",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756199,
                            "option_image_url": null
                        },
                        {
                            "id": 121873,
                            "question_id": 45685,
                            "option_text": "MapReduce leverages disk heavily, while Spark optimizes for memory-basedcomputations",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756200,
                            "option_image_url": null
                        },
                        {
                            "id": 121874,
                            "question_id": 45685,
                            "option_text": "MapReduce enables massively parallel computation, while Spark's driverprogram sequentially executes each worker",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756201,
                            "option_image_url": null
                        },
                        {
                            "id": 121875,
                            "question_id": 45685,
                            "option_text": "MapReduce is restricted in flexibility since only Map and Reduce are possiblesteps, while Spark has a variety of Actions possible making it highly flexible",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756202,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45686,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 189,
                    "question_text_1": "You are provided with a Spark program that picks out a list of suspicious transactions. It\u2019s logic is based on both the financial value of the transaction and the geographic location of the transaction. If the financial value is higher than a threshold and the geographic location is from a set of suspected locations (provided as a 1MB file), then the program deems the transaction as suspicious. The version of the Spark program given to you is written in such a way that it pulls all the transactions from the Workers to the Driver and then applies the logic. Which 2 changes from the list below will get you the most benefit in performance?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821218,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "18aee0b44d79852829e2382a62ec0190",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "2ea5167c-21f8-4d30-9344-8712878b81d4",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are provided with a Spark program that picks out a list of suspicious transactions. It\u2019s logic is based on both the financial value of the transaction and the geographic location of the transaction. If the financial value is higher than a threshold and the geographic location is from a set of suspected locations (provided as a 1MB file), then the program deems the transaction as suspicious. The version of the Spark program given to you is written in such a way that it pulls all the transactions from the Workers to the Driver and then applies the logic. Which 2 changes from the list below will get you the most benefit in performance?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121876,
                            "question_id": 45686,
                            "option_text": "Use broadcast variables for the 1MB file",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756216,
                            "option_image_url": null
                        },
                        {
                            "id": 121877,
                            "question_id": 45686,
                            "option_text": "Hardcode threshold value as a filter condition in the Driver program",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756217,
                            "option_image_url": null
                        },
                        {
                            "id": 121878,
                            "question_id": 45686,
                            "option_text": "Reorder operations on the Driver such that geographic location is checked firstbefore filtering high value transactions",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756218,
                            "option_image_url": null
                        },
                        {
                            "id": 121879,
                            "question_id": 45686,
                            "option_text": "Hardcode threshold value as a filter condition in the Workers itself",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756219,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45687,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 190,
                    "question_text_1": "Which of the following types of data sources can you read successfully without missing data using a program that extracts once every day? If no data is missed, then all data in source will match data extracted successfully.",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821221,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "2cf02878531fee984ac177972c22ef23",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "c3dae293-69cd-44f0-addd-407b6166185a",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of the following types of data sources can you read successfully without missing data using a program that extracts once every day? If no data is missed, then all data in source will match data extracted successfully."
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121880,
                            "question_id": 45687,
                            "option_text": "A MySQL table that is modelled as a Type II SCD where all Creates are newrows and all Updates are appended only, and these can happen anytime during the day.",
                            "option_image": "",
                            "score": "0.667",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756230,
                            "option_image_url": null
                        },
                        {
                            "id": 121881,
                            "question_id": 45687,
                            "option_text": "A Linux machine\u2019s system.log file into which all processes append their events",
                            "option_image": "",
                            "score": "0.667",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756231,
                            "option_image_url": null
                        },
                        {
                            "id": 121882,
                            "question_id": 45687,
                            "option_text": "Share transactions stored as facts & share prices updated in-place in aPostgreSQL database",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756232,
                            "option_image_url": null
                        },
                        {
                            "id": 121883,
                            "question_id": 45687,
                            "option_text": "A weather API that provides the temperature & rainfall readings for all itsweather stations in India for the specific time instant when being queried",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756233,
                            "option_image_url": null
                        },
                        {
                            "id": 121884,
                            "question_id": 45687,
                            "option_text": "Google drive account that stores all historical census data for India",
                            "option_image": "",
                            "score": "0.667",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756234,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45688,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 191,
                    "question_text_1": "Which of the following is true?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821222,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "82ca6bf11a929bcd50b89262ae5c9f94",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "9c25be48-2f49-4134-a882-8aa0e100a020",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of the following is true?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121885,
                            "question_id": 45688,
                            "option_text": "Snapshots of source systems can be created using an event capture tool likeCDC and then replaying the events in sequence for that time period.",
                            "option_image": "",
                            "score": "0.500",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756235,
                            "option_image_url": null
                        },
                        {
                            "id": 121886,
                            "question_id": 45688,
                            "option_text": "A data lake is a collection of data to be provided as input for data sciencealgorithms.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756236,
                            "option_image_url": null
                        },
                        {
                            "id": 121887,
                            "question_id": 45688,
                            "option_text": "Zookeeper is a library for ensuring that critical services used in big data areable to stay in sync with each other as to the health of those services.",
                            "option_image": "",
                            "score": "0.500",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756237,
                            "option_image_url": null
                        },
                        {
                            "id": 121888,
                            "question_id": 45688,
                            "option_text": "A single Spark cluster can have multiple \u201cleader\u201d master nodes.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756238,
                            "option_image_url": null
                        },
                        {
                            "id": 121889,
                            "question_id": 45688,
                            "option_text": "Spark is optimized for in-memory computation.",
                            "option_image": "",
                            "score": "0.500",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756239,
                            "option_image_url": null
                        },
                        {
                            "id": 121890,
                            "question_id": 45688,
                            "option_text": "Given the RDD underlying a Dataframe, you can recreate the same Dataframeprovided you know the schema",
                            "option_image": "",
                            "score": "0.500",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756240,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45689,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 192,
                    "question_text_1": "What are some capabilities common to both \u201cstreaming processing\u201d & \u201cbatch processing\u201d when using Spark for big data?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821223,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "6443c40811c147a6413b3036fc49f04c",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "1ff2ef13-9bb1-4f60-be71-499d208bcc38",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What are some capabilities common to both \u201cstreaming processing\u201d & \u201cbatch processing\u201d when using Spark for big data?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121891,
                            "question_id": 45689,
                            "option_text": "Batch operates on a set of data elements taken together while streaming canalso operate on a set of data elements as determined by the window",
                            "option_image": "",
                            "score": "0.667",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756241,
                            "option_image_url": null
                        },
                        {
                            "id": 121892,
                            "question_id": 45689,
                            "option_text": "Batch operates on data that is static while streaming operates on data that isdynamically changing",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756242,
                            "option_image_url": null
                        },
                        {
                            "id": 121893,
                            "question_id": 45689,
                            "option_text": "Batch processing can assume data as fully specified and complete whilestreaming cannot make that assumption",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756243,
                            "option_image_url": null
                        },
                        {
                            "id": 121894,
                            "question_id": 45689,
                            "option_text": "Batch processing is typically high latency while streaming processing isnecessary for real-time latencies",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756244,
                            "option_image_url": null
                        },
                        {
                            "id": 121895,
                            "question_id": 45689,
                            "option_text": "Both Streaming & Batch processing can operate on massively large data sets",
                            "option_image": "",
                            "score": "0.667",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756245,
                            "option_image_url": null
                        },
                        {
                            "id": 121896,
                            "question_id": 45689,
                            "option_text": "Both Streaming and Batch processing in Spark can use the same syntax forprogramming the main functional logic of the application",
                            "option_image": "",
                            "score": "0.667",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756246,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45690,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 193,
                    "question_text_1": "Which of the following are best practices associated with Streaming applications?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821224,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "38e36646213114061527ebf42489ebd9",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "6cd875bb-054d-473a-ad57-56eab906fb27",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of the following are best practices associated with Streaming applications?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121897,
                            "question_id": 45690,
                            "option_text": "Use a message store that supports message replay so that no data is lost inprocessing.",
                            "option_image": "",
                            "score": "0.667",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756247,
                            "option_image_url": null
                        },
                        {
                            "id": 121898,
                            "question_id": 45690,
                            "option_text": "\u201cHot potato\u201d principle is when the streaming application operates on theminimum amount of processing to produce valid output, which then becomes input for the nextminimal processor, and so on, thus ensuring that no single process is doing too many things inone go.",
                            "option_image": "",
                            "score": "0.667",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756248,
                            "option_image_url": null
                        },
                        {
                            "id": 121899,
                            "question_id": 45690,
                            "option_text": "Hadoop is best suited for executing Streaming applications.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756249,
                            "option_image_url": null
                        },
                        {
                            "id": 121900,
                            "question_id": 45690,
                            "option_text": "Use checkpointing when faced with mission-critical workloads that require100% accuracy.",
                            "option_image": "",
                            "score": "0.667",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756250,
                            "option_image_url": null
                        },
                        {
                            "id": 121901,
                            "question_id": 45690,
                            "option_text": "Handle state pollution by restarting the persistent store software periodically.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756251,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45691,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 194,
                    "question_text_1": "You are given the task of improving the performance of a Spark SQL program that is doing a simple count after a series of transformations. You suspect that the culprit is the main transformation job in the program. When you run EXPLAIN on that SQL, you see that Spark wrongly estimates that there are only 10 values for the key on which the main transformation job is hinged upon, whereas in reality the underlying data has a million values for that key. What actions would you perform from the below to improve performance of the program?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821227,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "50fdb3995572df8bcdcafd7c17caaee6",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "1aa478de-8dd3-4bab-b0fa-687a7b1e8cdc",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are given the task of improving the performance of a Spark SQL program that is doing a simple count after a series of transformations. You suspect that the culprit is the main transformation job in the program. When you run EXPLAIN on that SQL, you see that Spark wrongly estimates that there are only 10 values for the key on which the main transformation job is hinged upon, whereas in reality the underlying data has a million values for that key. What actions would you perform from the below to improve performance of the program?"
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121902,
                            "question_id": 45691,
                            "option_text": "Create all tables as external tables.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756262,
                            "option_image_url": null
                        },
                        {
                            "id": 121903,
                            "question_id": 45691,
                            "option_text": "Ensure cost based optimizer (CBO) is ON.",
                            "option_image": "",
                            "score": "0.667",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756263,
                            "option_image_url": null
                        },
                        {
                            "id": 121904,
                            "question_id": 45691,
                            "option_text": "Partition all tables on the same key on which the aggregate is happening.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756264,
                            "option_image_url": null
                        },
                        {
                            "id": 121905,
                            "question_id": 45691,
                            "option_text": "Run ANALYZE on all tables to ensure the right estimates are available to theoptimizer.",
                            "option_image": "",
                            "score": "0.667",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756265,
                            "option_image_url": null
                        },
                        {
                            "id": 121906,
                            "question_id": 45691,
                            "option_text": "Cache the table in a step with actions ahead of the SQL statement that is theculprit.",
                            "option_image": "",
                            "score": "0.667",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756266,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 45692,
                    "exam_id": 3,
                    "question_paper_id": 153,
                    "question_number": 195,
                    "question_text_1": "Observe the below image and select the options that are true.  ",
                    "question_image_1": "kOuXKmUbYhaw1sIa7J5B3jGQwGj3mHwmPDerwgemIht0Pd3UbT.png",
                    "question_type": "MSQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-11-27T22:26:56.000000Z",
                    "updated_at": "2024-11-27T22:26:56.000000Z",
                    "question_num_long": 640653821234,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "6f03a40a7483686ff10f9af6cf31ab41",
                    "course_id": 95,
                    "is_markdown": 0,
                    "question_text_2": "Observe the below image and select the options that are true.  ",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "e88fb9f1-7ac1-47ff-aaed-aef2e0a6d6f0",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/kOuXKmUbYhaw1sIa7J5B3jGQwGj3mHwmPDerwgemIht0Pd3UbT.png"
                    ],
                    "question_texts": [
                        "Observe the below image and select the options that are true.  ",
                        "Observe the below image and select the options that are true.  "
                    ],
                    "course": {
                        "id": 95,
                        "course_name": "Introduction to Big Data",
                        "course_code": "Introduction to Big Data",
                        "created_at": "2024-11-27T22:26:37.000000Z",
                        "updated_at": "2024-11-27T22:26:37.000000Z",
                        "program_id": 1,
                        "uuid": "a02fba4b-ebff-436d-b146-c8e861d7b126"
                    },
                    "options": [
                        {
                            "id": 121907,
                            "question_id": 45692,
                            "option_text": "Each subscriber gets one third of all messages published into Topic 1.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756294,
                            "option_image_url": null
                        },
                        {
                            "id": 121908,
                            "question_id": 45692,
                            "option_text": "Publisher 1 can safely send data only for Subscriber 1\u2019s consumption whilerestricting access to Subscribers 2 & 3.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756295,
                            "option_image_url": null
                        },
                        {
                            "id": 121909,
                            "question_id": 45692,
                            "option_text": "Subscriber 2 gets all messages published into Topic 1 by Publisher 1 but onlyhalf of Publisher 2\u2019s messages.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756296,
                            "option_image_url": null
                        },
                        {
                            "id": 121910,
                            "question_id": 45692,
                            "option_text": "Each subscriber gets all messages published into Topic 1.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-11-27T22:26:56.000000Z",
                            "updated_at": "2024-11-27T22:26:56.000000Z",
                            "option_number": 6406532756297,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                }
            ]
        },
        "summary": "Here\u2019s a structured **summary of core topics, concepts, principles, and equations** for the **\"Introduction to Big Data\"** exam, followed by **Mermaid knowledge graphs** to visualize relationships.\n\n---\n\n---\n\n### **Core Topics & Concepts**\n#### **1. Big Data Fundamentals**\n- **Definition**:\n  - *\"All data, Any Time, Any Method\"* (Answer: C).\n  - **5Vs**: Volume, Velocity, Variety, Veracity, Value.\n  - **Technologies**: Hadoop, Spark, MapReduce, Kafka, Cloud-native systems (GCP, AWS).\n- **Divide-and-Conquer Paradigm**:\n  - Implemented by **Hadoop (MapReduce)** and **Spark** (Answer: C).\n  - **MapReduce**:\n    - **Map**: Emits `<key, value>` pairs.\n    - **Reduce**: Aggregates values by key.\n    - **Barrier synchronization** after each step (slower than Spark).\n  - **Spark**:\n    - Uses **Directed Acyclic Graphs (DAGs)** for parallel execution.\n    - Optimized for **in-memory computation** (vs. MapReduce\u2019s disk-heavy approach).\n\n#### **2. Hadoop vs. Spark**\n| Feature               | Hadoop (MapReduce)                     | Spark                                  |\n|-----------------------|----------------------------------------|----------------------------------------|\n| **Computation**       | Disk-based (slower)                    | In-memory (faster)                     |\n| **Execution Model**   | Barrier synchronization after each step | DAG-based (parallel steps)             |\n| **Flexibility**       | Limited (Map/Reduce only)              | Rich APIs (RDDs, DataFrames, SQL)      |\n| **Fault Tolerance**   | Re-executes failed tasks                | Lineage-based recovery (RDDs)          |\n| **Use Case**          | Batch processing                        | Batch + Streaming + ML                 |\n\n#### **3. Spark Core Concepts**\n- **RDD (Resilient Distributed Dataset)**:\n  - Immutable, partitioned collections.\n  - **Transformations** (lazy): `map`, `filter`, `groupBy`.\n  - **Actions** (eager): `count`, `collect`, `reduce`.\n- **DataFrames/Datasets**:\n  - Optimized with **Catalyst optimizer** and **Tungsten engine**.\n  - Supports SQL queries.\n- **Broadcast Variables**:\n  - Cache small datasets (e.g., 1MB file) on all workers to avoid shuffling.\n  - Example: `sc.broadcast(small_dataset)`.\n- **Accumulators**:\n  - Shared variables for aggregation (e.g., counters).\n- **Closures**:\n  - Variables referenced in Spark operations must be **serializable** or broadcasted.\n  - **Error**: Referencing driver variables directly in executors (Answer: E in Q8).\n\n#### **4. Spark Streaming**\n- **Micro-Batch Processing**:\n  - Divides stream into small batches (e.g., 1-second intervals).\n- **Structured Streaming**:\n  - **Exactly-once processing** (Answer: E in Q9).\n  - **Output Modes**:\n    - `Append`: Only new results.\n    - `Complete`: Full aggregated results (Answer: B in Q23).\n    - `Update`: Changed results.\n  - **Watermarking**: Handles late data.\n- **Stateful Processing**:\n  - Uses **checkpointing** for fault tolerance (Answer: D in Q29).\n- **Performance Optimization**:\n  - **Broadcast joins** for small datasets (Answer: B in Q21).\n  - Avoid **shuffles** (e.g., `groupBy` \u2192 `reduceByKey`).\n\n#### **5. Kafka**\n- **Publish-Subscribe Model** (Answer: D in Q31):\n  - **Topics**: Categories for messages.\n  - **Partitions**: Ordered, immutable sequences (scaled by increasing partitions).\n  - **Brokers**: Servers storing partitions.\n  - **Replication**: Ensures availability (Answer: B in Q19).\n  - **Consumer Groups**: Parallel consumption.\n- **Scaling**:\n  - **Add brokers** (not just disks/memory) for throughput (Answer: B in Q10).\n  - **Increase partitions** to parallelize consumption.\n- **Failure Handling**:\n  - **Exactly-once semantics** with idempotent producers/transactions.\n\n#### **6. Cloud-Native Design**\n- **Principles**:\n  - **Resilience**: Auto-recovery from failures (Answer: D in Q6).\n  - **Observability**: Logging, metrics, tracing.\n  - **Scalability**: Horizontal scaling (e.g., Kubernetes).\n  - **Managed Services**: Reduce administrative overhead (Answer: D in Q5).\n- **Serverless**:\n  - **Google Cloud Functions**, **Dataflow**, **Dataproc**.\n  - **Trade-off**: Less control vs. reduced ops.\n\n#### **7. Data Pipelines**\n- **ETL/ELT**:\n  - **Extract**: CDC (Change Data Capture), Kafka, Pub/Sub.\n  - **Transform**: Spark, Dataflow, Flink.\n  - **Load**: Data Warehouses (BigQuery), NoSQL (Redis, Datastore).\n- **Real-Time vs. Batch**:\n  - **Streaming**: Low latency (e.g., fraud detection).\n  - **Batch**: High throughput (e.g., daily reports).\n- **Example Pipeline** (Answer: C in Q12):\n  ```\n  MongoDB (CDC) \u2192 Kafka \u2192 Spark Streaming \u2192 Redis \u2192 Dashboard\n  ```\n\n#### **8. Performance Optimization**\n- **Spark**:\n  - **Partitioning**: Avoid skew (e.g., `repartition`).\n  - **Caching**: `df.cache()` for iterative algorithms.\n  - **Broadcast Joins**: For small tables (Answer: A in Q25).\n  - **Cost-Based Optimizer (CBO)**: Use `ANALYZE TABLE` for statistics (Answer: D in Q30).\n- **Kafka**:\n  - **Increase partitions** (not brokers) for parallelism.\n  - **Compression**: Reduce network overhead.\n- **General**:\n  - **Hot Potato Principle**: Minimal processing per stage (Answer: B in Q29).\n  - **Avoid Driver Bottlenecks**: Push logic to workers.\n\n#### **9. Fault Tolerance & Recovery**\n- **Spark**:\n  - **RDD Lineage**: Recomputes lost partitions.\n  - **Checkpointing**: Saves state to storage (e.g., HDFS).\n- **Kafka**:\n  - **Replication Factor**: `RF=3` for high availability.\n  - **Consumer Offsets**: Track progress.\n- **Structured Streaming**:\n  - **End-to-End Exactly-Once**: Idempotent sinks + replayable sources.\n\n#### **10. Data Storage & Formats**\n- **Data Lakes**:\n  - Raw data storage (e.g., GCS, S3).\n  - **Not just for ML** (Answer: B in Q27 is false).\n- **File Formats**:\n  - **Parquet/ORC**: Columnar (good for analytics).\n  - **Avro**: Row-based (good for streaming).\n- **Databases**:\n  - **OLTP**: MongoDB, PostgreSQL (transactions).\n  - **OLAP**: BigQuery, Redshift (analytics).\n\n#### **11. UDFs vs. UDAFs**\n| Feature       | UDF (User-Defined Function)       | UDAF (User-Defined Aggregate Function) |\n|---------------|-----------------------------------|-----------------------------------------|\n| **Purpose**   | Scalar operations (row-wise)      | Aggregations (group-wise)               |\n| **State**     | Stateless                         | **Buffer** for intermediate state       |\n| **Example**   | `sqrt(x)`                         | `SUM(x) GROUP BY key`                   |\n| **Parallelism**| Embarrassingly parallel           | Divide-and-conquer (Answer: B in Q14)   |\n\n#### **12. Deep Learning in Spark**\n- **Challenges**:\n  - Large models (e.g., Facebook\u2019s transformer) have **high load times** (Answer: E in Q15).\n  - **Solutions**:\n    - **Model Compression**: Quantization, pruning.\n    - **Edge Deployment**: Run on device (e.g., spider cam in Q13).\n    - **Broadcast Models**: Avoid repeated loading.\n- **Trade-offs**:\n  - **Accuracy vs. Latency**: Compress models to meet SLA (Answer: A in Q16).\n\n#### **13. Time Handling**\n- **Event Time vs. Processing Time**:\n  - **Event Time**: When data was generated (e.g., transaction timestamp).\n  - **Processing Time**: When data was processed.\n- **Time Zones**:\n  - **Business Time**: Align with operational hours (Answer: B in Q11).\n  - **Watermarks**: Handle late data in streaming.\n\n#### **14. Cost Optimization**\n- **Cloud**:\n  - **Serverless > VMs**: Reduce ops (Answer: D in Q5).\n  - **Spot Instances**: For fault-tolerant workloads.\n- **Spark**:\n  - **Dynamic Allocation**: Scale executors up/down.\n  - **Avoid Over-Partitioning**: Too many small tasks \u2192 overhead.\n\n#### **15. Best Practices**\n- **Streaming**:\n  - **Message Replay**: Use Kafka/PubSub for recovery (Answer: A in Q29).\n  - **Checkpointing**: For stateful ops.\n  - **Monitoring**: Track lag, throughput.\n- **Batch**:\n  - **Partition Pruning**: Filter data early.\n  - **Predicate Pushdown**: Offload filtering to storage.\n\n---\n\n### **Key Equations (Math/Performance)**\n1. **Spark Parallelism**:\n   - **Optimal Partitions** = `Total Cores \u00d7 2\u20133` (avoid too many small tasks).\n   - **Shuffle Overhead**: `O(N log N)` for `groupBy` vs. `O(N)` for `reduceByKey`.\n\n2. **Kafka Throughput**:\n   - **Max Throughput** = `Min(Producer Throughput, Consumer Throughput)`.\n   - **Partitions per Consumer** \u2264 `Number of Consumer Threads`.\n\n3. **Latency vs. Throughput**:\n   - **Streaming**: Low latency, moderate throughput.\n   - **Batch**: High throughput, high latency.\n\n4. **Data Skew Mitigation**:\n   - **Salting**: Add random prefix to keys to distribute load.\n     ```python\n     df = df.withColumn(\"salted_key\", concat(col(\"key\"), lit(\"_\"), (rand() * 10).cast(\"int\")))\n     ```\n\n5. **Broadcast Join Threshold**:\n   - Broadcast if `dataset_size < 10MB` (configurable via `spark.sql.autoBroadcastJoinThreshold`).\n\n---\n\n### **Mermaid Knowledge Graphs**\n#### **1. Big Data Ecosystem**\n```mermaid\ngraph TD\n    A[Big Data] --> B[Batch Processing]\n    A --> C[Stream Processing]\n    A --> D[Machine Learning]\n\n    B --> E[Hadoop MapReduce]\n    B --> F[Spark Batch]\n    C --> G[Spark Streaming]\n    C --> H[Flink]\n    C --> I[Kafka Streams]\n    D --> J[Spark MLlib]\n    D --> K[TensorFlow on Spark]\n\n    E --> L[HDFS]\n    F --> L\n    G --> M[Kafka]\n    H --> M\n    I --> M\n    J --> N[Data Lakes]\n    K --> N\n```\n\n#### **2. Spark Components**\n```mermaid\ngraph TD\n    A[Spark] --> B[Spark Core]\n    A --> C[Spark SQL]\n    A --> D[Spark Streaming]\n    A --> E[MLlib]\n    A --> F[GraphX]\n\n    B --> G[RDDs]\n    B --> H[Transformations/Actions]\n    C --> I[DataFrames]\n    C --> J[Catalyst Optimizer]\n    D --> K[Structured Streaming]\n    D --> L[DStream API]\n    E --> M[Pipelines]\n    E --> N[Feature Transformers]\n```\n\n#### **3. Kafka Architecture**\n```mermaid\ngraph TD\n    A[Kafka Cluster] --> B[Broker 1]\n    A --> C[Broker 2]\n    A --> D[Broker 3]\n\n    B --> E[Topic A (Partition 0)]\n    B --> F[Topic A (Partition 1)]\n    C --> G[Topic A (Partition 2)]\n    D --> H[Topic B (Partition 0)]\n\n    E --> I[Replica 1]\n    E --> J[Replica 2]\n    F --> K[Producer]\n    G --> L[Consumer Group 1]\n    H --> M[Consumer Group 2]\n\n    K --> N[Message]\n    L --> O[Offset Commit]\n```\n\n#### **4. Data Pipeline Example (Q12)**\n```mermaid\ngraph LR\n    A[MongoDB] -->|CDC| B[Kafka]\n    B --> C[Spark Streaming]\n    C --> D[Redis]\n    D --> E[Dashboard]\n```\n\n#### **5. Cloud-Native Design Principles**\n```mermaid\ngraph TD\n    A[Cloud-Native] --> B[Resilience]\n    A --> C[Scalability]\n    A --> D[Observability]\n    A --> E[Managed Services]\n\n    B --> F[Auto-Retry]\n    B --> G[Circuit Breakers]\n    C --> H[Horizontal Scaling]\n    C --> I[Serverless]\n    D --> J[Logging]\n    D --> K[Metrics]\n    E --> L[Google Dataflow]\n    E --> M[AWS Lambda]\n```\n\n#### **6. Streaming vs. Batch Processing**\n```mermaid\ngraph LR\n    A[Data Source] --> B[Stream Processing]\n    A --> C[Batch Processing]\n\n    B --> D[Low Latency]\n    B --> E[Stateful Operations]\n    B --> F[Kafka/Spark Streaming]\n\n    C --> G[High Throughput]\n    C --> H[ETL Jobs]\n    C --> I[Hadoop/Spark Batch]\n```\n\n#### **7. Spark Performance Optimization**\n```mermaid\ngraph TD\n    A[Slow Spark Job] --> B[Check Shuffles]\n    A --> C[Broadcast Small Data]\n    A --> D[Partitioning]\n    A --> E[Caching]\n\n    B --> F[Use reduceByKey]\n    C --> G[Broadcast Join]\n    D --> H[Repartition]\n    E --> I[persist()]\n```",
        "total_score": "60.00"
    },
    "url": "/question-paper/practise/95/a257da7c-11d",
    "version": "ee3d5d44299e610bd137ea6200db5ac2"
}