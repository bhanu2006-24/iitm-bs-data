{
    "component": "Quizpractise/PractiseQuestionPaper",
    "props": {
        "errors": {},
        "auth": {
            "user": null
        },
        "flash": {
            "error": []
        },
        "banner": null,
        "file_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "file_do_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "question_paper": {
            "id": 58,
            "group_id": 10,
            "exam_id": 3,
            "total_score": "1.00",
            "duration": 4,
            "created_at": "2024-10-16T08:14:59.000000Z",
            "updated_at": "2024-10-18T09:22:06.000000Z",
            "question_paper_name": "2023 Dec24: IIT M AN EXAM ADB3",
            "question_paper_description": "2023 Dec24: IIT M AN EXAM ADB3",
            "uuid": "af17573b-2c2d-4168-8c90-a94b29eb7fe0",
            "year": 2023,
            "is_new": 0,
            "exam": {
                "id": 3,
                "exam_name": "End Term Quiz",
                "created_at": "2024-10-16T08:08:51.000000Z",
                "updated_at": "2024-10-16T08:08:51.000000Z",
                "uuid": "7a6ff569-f50c-40e7-a08b-f5c334392600",
                "en_id": "eyJpdiI6IlZJZUFoNmk2TWpBWjUxZUVZVHo0eFE9PSIsInZhbHVlIjoiR0RBNWxiVFZUbTAxd3FJS3EraHNldz09IiwibWFjIjoiYTllMGM5NzhkNWFkYTIyNjEwOWZjZjUzMTA0ZTdjMzhhMjg5OGVkYmFiMDU1ODJiZmJmMjkyZTdjY2ZmM2RhMiIsInRhZyI6IiJ9"
            },
            "questions": [
                {
                    "id": 11570,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 180,
                    "question_text_1": "THIS IS QUESTION PAPER FOR THE SUBJECT \"DEGREE LEVEL : INTRODUCTION TO BIG DATA (COMPUTER BASED EXAM)\"  ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699387,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "52b338960058bce4c1ce772665e3259a",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "12bb3f96-6d4f-4f26-9705-d328135b77d0",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "THIS IS QUESTION PAPER FOR THE SUBJECT \"DEGREE LEVEL : INTRODUCTION TO BIG DATA (COMPUTER BASED EXAM)\"  ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31216,
                            "question_id": 11570,
                            "option_text": "YES",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31217,
                            "question_id": 11570,
                            "option_text": "NO",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11571,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 181,
                    "question_text_1": "What happens when a Spark Structured Streaming pipeline operating with Kafka as source is subject to a failure of a machine in either of the Kafka cluster or the Spark cluster?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699389,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "1e258348aa6c99a89c606a3637bd34e0",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "0713bf51-0191-4d91-8bdd-44c84db7657b",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What happens when a Spark Structured Streaming pipeline operating with Kafka as source is subject to a failure of a machine in either of the Kafka cluster or the Spark cluster?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31218,
                            "question_id": 11571,
                            "option_text": "Failure of a machine in the Kafka cluster will result in an Exception in the Sparkpipeline which will then fail and halt.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31219,
                            "question_id": 11571,
                            "option_text": "The Spark pipeline will not be able to start again from previously committedoffset by restarting itself, resulting in at least-once processing semantics",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31220,
                            "question_id": 11571,
                            "option_text": "Irrespective of whatever machine fails, Spark will throw an error and halt.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31221,
                            "question_id": 11571,
                            "option_text": "The pipeline will be restarted automatically by Spark which is able to pick upthe exact data from Kafka which was being processed at the time of error, resulting in exactly-once semantics.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31222,
                            "question_id": 11571,
                            "option_text": "Data that is being processed will not be processed again, resulting in atmost-once semantics.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11572,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 182,
                    "question_text_1": "A big data streaming application that uses Kafka as source is observed to be really slow. The Kafka cluster has 2 broker nodes and this application is reading from 1 topic that has 10 partitions. On closer investigation, it was found that Kafka is not scaling to the velocity of input data coming in. What can you first try to do to scale Kafka further while incurring minimal costs?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699390,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "41a0c0d50baff5b44d216979308d02b3",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "7307325d-2d97-4174-aac4-e718698f85ff",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "A big data streaming application that uses Kafka as source is observed to be really slow. The Kafka cluster has 2 broker nodes and this application is reading from 1 topic that has 10 partitions. On closer investigation, it was found that Kafka is not scaling to the velocity of input data coming in. What can you first try to do to scale Kafka further while incurring minimal costs?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31223,
                            "question_id": 11572,
                            "option_text": "Add disks to each broker in the cluster, and disks are the cheapest computercomponent",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31224,
                            "question_id": 11572,
                            "option_text": "Add new brokers to the cluster, even though this is more expensive than theother options this is the only foolproof way to scale.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31225,
                            "question_id": 11572,
                            "option_text": "Increase memory in each of the brokers in the cluster. While cost of memory ismore than cost of disks, it is still cheaper than adding brokers and helps to scale.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31226,
                            "question_id": 11572,
                            "option_text": "Create more topics and change input application to reroute data to all topics tobe able to spread input data better. This is nearly the least expensive since only developer effort isrequired to change application.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31227,
                            "question_id": 11572,
                            "option_text": "Double the number of partitions for this single topic to be able to spread inputdata better. This is the least expensive since only administrator effort is required without changingapplication.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11573,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 183,
                    "question_text_1": "A company with headquarters (HQ) in the Middle East operates on a Sunday-Thursday weekday schedule with Friday & Saturday as weekend days. It computes end of week revenue numbers using an ETL pipeline by first computing sales for each day at 1AM local time of the next day, and then summing up the weekly sales every Sunday early morning at 3AM local time. This number gets reported to leadership every Sunday morning 9AM local time, so the ETL pipeline is scheduled to run every Sunday morning at 8AM local time. As a result of management change, it has decided to relocate its HQ to India. Which of the following changes will need to be done to its ETL pipeline to ensure the correct output continues to be produced?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699391,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "7715edf06c5abb8fe6409fb33ae3db46",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "e401de4f-3d5f-492a-ad04-55f94c7522a2",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "A company with headquarters (HQ) in the Middle East operates on a Sunday-Thursday weekday schedule with Friday & Saturday as weekend days. It computes end of week revenue numbers using an ETL pipeline by first computing sales for each day at 1AM local time of the next day, and then summing up the weekly sales every Sunday early morning at 3AM local time. This number gets reported to leadership every Sunday morning 9AM local time, so the ETL pipeline is scheduled to run every Sunday morning at 8AM local time. As a result of management change, it has decided to relocate its HQ to India. Which of the following changes will need to be done to its ETL pipeline to ensure the correct output continues to be produced?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31228,
                            "question_id": 11573,
                            "option_text": "The business time for the final weekly sum operation needs to be changed tothat of Monday 3AM India time instead of Sunday 3AM Middle East time.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31229,
                            "question_id": 11573,
                            "option_text": "Since time zone has changed as well as week definition too, the definition ofbusiness time has changed. So, the ETL has to be rewritten entirely.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31230,
                            "question_id": 11573,
                            "option_text": "Nothing needs to change since daily sales is available at 1AM Middle East timewhich is anyway behind India time and so the numbers will be available before leadership comesin at 9AM.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31231,
                            "question_id": 11573,
                            "option_text": "Event time has changed since the event of week ending has changed indefinition, and so the ETL needs to be changed to consider the new event in the data.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31232,
                            "question_id": 11573,
                            "option_text": "No change required since neither event time nor business time is changingwhereas only the operational time is changing.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11574,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 184,
                    "question_text_1": "You are appointed as a Data Engineer in a company that has a legacy reporting application written in Java which suffers from performance problems. The reporting application plots dashboards with near real-time refresh (once every minute) of key business indicators to help management take live decisions. The application reads data directly from the source database of MongoDB, aggregates using simple counts and shows them visually in a UI. The performance problem of this application comes because the source database is at times overloaded and therefore the dashboard is not able to refresh fast enough. Choose from the options below the option that best addresses performance with minimal maintenance effort:",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699392,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "c5160b6719ccbf37587f8c0bc4ad5abe",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "6dc9a441-281d-4ffb-a301-3c3743560401",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are appointed as a Data Engineer in a company that has a legacy reporting application written in Java which suffers from performance problems. The reporting application plots dashboards with near real-time refresh (once every minute) of key business indicators to help management take live decisions. The application reads data directly from the source database of MongoDB, aggregates using simple counts and shows them visually in a UI. The performance problem of this application comes because the source database is at times overloaded and therefore the dashboard is not able to refresh fast enough. Choose from the options below the option that best addresses performance with minimal maintenance effort:"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31233,
                            "question_id": 11574,
                            "option_text": "Since MongoDB is OLTP, it is not able to support business reporting. So,replace it with Hadoop which supports OLAP better",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31234,
                            "question_id": 11574,
                            "option_text": "Convert the application from using plain Java to using Spark Streaming in Java",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31235,
                            "question_id": 11574,
                            "option_text": "Query MongoDB every 1 minute for new data using a check on documentinserted timestamp, use Spark Streaming to compute the KPIs with the queried data, and thenpopulate into a NoSQL DB like Redis for the UI to consume.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31236,
                            "question_id": 11574,
                            "option_text": "Extract raw data from MongoDB using Change Data Capture (CDC) once everyminute into Kafka, and then use Spark Streaming to compute the KPIs and then populate into aNoSQL DB like Redis for the UI to consume.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31237,
                            "question_id": 11574,
                            "option_text": "Convert application to using Python along with a NoSQL database for storingand retrieving the aggregated counts.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11575,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 185,
                    "question_text_1": "Dhoni is on the crease with a bat in hand that has sensors embedded throughout. The sensors talk to the spider cam every second. The spider cam is itself a powerful ARM-based computer which has connectivity to the cloud through the wire on which it hangs. Using this connectivity, it can send as much or as little data as required and also receive instructions from the cloud. There is a machine learning model which suggests to the batsman to loosen the grip on his bat or tighten it based on the shots played using the sensor measurements. The way the suggestion happens is using dynamic vibration intensity communicated to the sensors embedded in the bat handle. Your task is to design the data pipeline that enables such feedback to Dhoni ideally before every ball with as much accuracy as possible throughout the match. Which of the following options best satisfies the requirements?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699393,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "032ca2e2d341815d74c20f29896453f7",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "cc273ec7-082f-4412-b039-dcc9c9de4abd",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Dhoni is on the crease with a bat in hand that has sensors embedded throughout. The sensors talk to the spider cam every second. The spider cam is itself a powerful ARM-based computer which has connectivity to the cloud through the wire on which it hangs. Using this connectivity, it can send as much or as little data as required and also receive instructions from the cloud. There is a machine learning model which suggests to the batsman to loosen the grip on his bat or tighten it based on the shots played using the sensor measurements. The way the suggestion happens is using dynamic vibration intensity communicated to the sensors embedded in the bat handle. Your task is to design the data pipeline that enables such feedback to Dhoni ideally before every ball with as much accuracy as possible throughout the match. Which of the following options best satisfies the requirements?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31238,
                            "question_id": 11575,
                            "option_text": "Ingest all data into Pub/Sub, process using Google Cloud Dataflow, invoke theML model, and then write back output from Cloud to the spider cam to relay to the bat.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31239,
                            "question_id": 11575,
                            "option_text": "Compress the ML model to fit into the spider cam\u2019s available resource, andwrite pipelines to execute the model in the spider cam itself",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31240,
                            "question_id": 11575,
                            "option_text": "Compress the data in the spider cam every 5 seconds, write to Pub/Sub thecompressed data, invoke the ML model and then write back output from Cloud to the spider camto relay to the bat.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31241,
                            "question_id": 11575,
                            "option_text": "Compress the ML model to fit into the spider cam\u2019s available resource, andwrite pipelines to execute in the spider cam itself, with periodically data being sent to the cloud,retrain the model using Google Cloud ML and then redeploy the model to the spider cam.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11576,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 186,
                    "question_text_1": "In the class, we saw the UDF for mobilenet_v2. By definition, UDFs are scalar. In Spark, there is another class of user defined routines called UDAFs, which stand for User Defined Aggregator Functions. UDAFs are meant to provide a means to write a custom aggregation function which aggregates over a grouping of values to arrive at a single value. UDAF structure differs saliently from UDFs in that it exposes the notion of a buffer as a way of maintaining intermediate state before finalizing aggregate output. Why is a buffer required for UDAF and not for a UDF?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699394,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "c1587e21468d869744aa0376e41cfc48",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "c25f745a-4c40-4e6a-b6dc-1c63924a0110",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "In the class, we saw the UDF for mobilenet_v2. By definition, UDFs are scalar. In Spark, there is another class of user defined routines called UDAFs, which stand for User Defined Aggregator Functions. UDAFs are meant to provide a means to write a custom aggregation function which aggregates over a grouping of values to arrive at a single value. UDAF structure differs saliently from UDFs in that it exposes the notion of a buffer as a way of maintaining intermediate state before finalizing aggregate output. Why is a buffer required for UDAF and not for a UDF?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31242,
                            "question_id": 11576,
                            "option_text": "A UDF is a scalar operation executing on 1 row at a time and producing outputimmediately, and therefore there is no intermediate output necessary. Whereas, a UDAF operateson multiple rows which will require multiple passes for the final output thereby requiring a buffer.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31243,
                            "question_id": 11576,
                            "option_text": "A UDF is a scalar operation executing on many rows at a time, grouped by akey and producing a single output, and therefore there is no intermediate output necessary.Whereas, a UDAF operates on multiple rows which will require multiple passes for the final outputthereby requiring a buffer.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31244,
                            "question_id": 11576,
                            "option_text": "A UDF is a scalar operation executing on 1 row at a time and producing outputimmediately, and therefore there is no intermediate output necessary. Whereas, a UDAF operateson multiple rows of unbounded size requiring a divide-and-conquer approach for computingaggregates, which uses the intermediate buffer to store partial values before finalizing the resultaggregate.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31245,
                            "question_id": 11576,
                            "option_text": "A UDF is a scalar operation executing on many rows at a time, grouped by akey and producing a single output, and therefore there is no intermediate output necessary.Whereas, a UDAF operates on multiple rows of unbounded size requiring a divide-and-conquerapproach for computing aggregates, which uses the intermediate buffer to store partial valuesbefore finalizing the result aggregate.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11577,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 187,
                    "question_text_1": null,
                    "question_image_1": "VftDBKQGcf0T7g4dQWNoenJAaA1pv10enPnf1uZGS4EjwJUs93.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699395,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "627f1d885c51baeb40bcc736503824e2",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "d2a9321d-5f2a-4d88-8e77-af43c1a00dce",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/VftDBKQGcf0T7g4dQWNoenJAaA1pv10enPnf1uZGS4EjwJUs93.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31246,
                            "question_id": 11577,
                            "option_text": "Yes, since they check the syntax of the model function so that there are noerrors",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31247,
                            "question_id": 11577,
                            "option_text": "They invoke PyTorch libraries that have already been setup for model scoringon GCP using APIs embedded within the function",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31248,
                            "question_id": 11577,
                            "option_text": "No, since the primary function of these lines of code is to eliminate repeatedDL model loads as DL models are large in size but with XGboost this isn\u2019t required since modelload time is negligible",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31249,
                            "question_id": 11577,
                            "option_text": "They are Map-style UDFs that make it an embarrassingly parallel computationthus making the execution parallelized and fast.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11578,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 188,
                    "question_text_1": "You are given a Spark Streaming pipeline that invokes a pre-trained DL model for every image it receives as input and produces the classification result in quick time. The model with the best recall rate from the PyTorch library already runs in under 3 seconds on an average executing on a single GPU Spark worker machine. However, your management has instructed you to reduce the cost of AI projects significantly. What is the best option to explore to meet the expectations without compromising on false negatives while also being within 10-20% of the average execution time?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699396,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "e864f155d7f6648c2cde4ce3a41b8a36",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "389d5ab8-646e-47f0-bb95-650f76f94b38",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are given a Spark Streaming pipeline that invokes a pre-trained DL model for every image it receives as input and produces the classification result in quick time. The model with the best recall rate from the PyTorch library already runs in under 3 seconds on an average executing on a single GPU Spark worker machine. However, your management has instructed you to reduce the cost of AI projects significantly. What is the best option to explore to meet the expectations without compromising on false negatives while also being within 10-20% of the average execution time?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31250,
                            "question_id": 11578,
                            "option_text": "Use a different DL model from PyTorch that is already compressed to half thesize.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31251,
                            "question_id": 11578,
                            "option_text": "Build a custom model that compresses the highest recall rate model justenough to be able to execute within the stipulated time, and measure recall.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31252,
                            "question_id": 11578,
                            "option_text": "Remove complexity associated with Spark Streaming and convert the modelexecution pipeline into a single threaded Python application running on the same GPU machine.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31253,
                            "question_id": 11578,
                            "option_text": "Change Spark machine to use CPUs and train a fresh pipeline to achieveobjectives.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11579,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 189,
                    "question_text_1": "Which of the following code snippets will give a runtime error? (Note: df is a spark dataframe. It has a column called content which has images represented as byte array)",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699398,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "eef92bf30db6bffc48e9e57b178e4e9d",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "79cc7c56-0a1d-438b-ba83-b97d76375303",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of the following code snippets will give a runtime error? (Note: df is a spark dataframe. It has a column called content which has images represented as byte array)"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31254,
                            "question_id": 11579,
                            "option_text": "",
                            "option_image": "bgB4Pi6xFwgphnmJjW906NFJLYQaeQmToVWz3yLLrzOU8WYuGl.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": "app/option_images/bgB4Pi6xFwgphnmJjW906NFJLYQaeQmToVWz3yLLrzOU8WYuGl.png"
                        },
                        {
                            "id": 31255,
                            "question_id": 11579,
                            "option_text": "",
                            "option_image": "PfmmDGlIRPQSGtfTdKN8Pr1MUu9PgGwvrRQ6rLnIEJj7YYiUrl.png",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": "app/option_images/PfmmDGlIRPQSGtfTdKN8Pr1MUu9PgGwvrRQ6rLnIEJj7YYiUrl.png"
                        },
                        {
                            "id": 31256,
                            "question_id": 11579,
                            "option_text": "",
                            "option_image": "wkAopKdkh4jkX3x4LBKN7JsyhR7RbZCbawXTbr1QB7uTK7whGQ.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": "app/option_images/wkAopKdkh4jkX3x4LBKN7JsyhR7RbZCbawXTbr1QB7uTK7whGQ.png"
                        },
                        {
                            "id": 31257,
                            "question_id": 11579,
                            "option_text": "",
                            "option_image": "aur64zm1DR2xLt3U7N1xHPb4QwczcGwxyv2qjam8jopkJH8u6k.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": "app/option_images/aur64zm1DR2xLt3U7N1xHPb4QwczcGwxyv2qjam8jopkJH8u6k.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11580,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 190,
                    "question_text_1": "Consider a Structured Streaming application running on Google Dataproc firing up every 10 seconds, consuming any number of records from Kafka available since last read, and emitting some computed answers to another Kafka topic. Consider also that apart from the functional logic, the same application is also emitting into a file the start time and end time of every batch invocation for audit purposes.  Assume there is a failure in one of the Dataproc machines that results in a failure of a specific run. For the external world (i.e. anybody consuming the outputs of this application), will they see any change in output as a result of the failure at all, or will the only impact of failure be one of slower performance for the failed-and-retried run? Pick the ones from the list below that answer this question.",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699399,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "31aa75fd19edd300506e9b6b96f6c401",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "fd753673-769b-463e-9a7c-607f630e56d8",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider a Structured Streaming application running on Google Dataproc firing up every 10 seconds, consuming any number of records from Kafka available since last read, and emitting some computed answers to another Kafka topic. Consider also that apart from the functional logic, the same application is also emitting into a file the start time and end time of every batch invocation for audit purposes.  Assume there is a failure in one of the Dataproc machines that results in a failure of a specific run. For the external world (i.e. anybody consuming the outputs of this application), will they see any change in output as a result of the failure at all, or will the only impact of failure be one of slower performance for the failed-and-retried run? Pick the ones from the list below that answer this question."
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31258,
                            "question_id": 11580,
                            "option_text": "No, the failure is not visible. The only visible effect for the external world wouldbe in the form of a slowdown in runtime for completion of that batch as Structured Streamingretries the batch that failed thus taking twice as much time as normal.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31259,
                            "question_id": 11580,
                            "option_text": "No, no failure is visible since Structured Streaming uses transactions andidempotence to achieve exactly-once processing.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31260,
                            "question_id": 11580,
                            "option_text": "Yes, the failure is visible because the side effect of emitting timestamps in abatch will be visible as 2 consecutive Start timestamps without any end timestamp as StructuredStreaming retries the failed batch.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31261,
                            "question_id": 11580,
                            "option_text": "No, no failure is visible since Structured Streaming can process the same datain a retry resulting in the same outputs again.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11581,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 191,
                    "question_text_1": "Let us say we are using structured streaming for continuously reading data from Kafka and storing the results back into a Kafka topic using window function aggregates. Now, instead, we decide that we need to just perform a one-time batch operation using the same logic, where there is a need to read specific data from Kafka (i.e. using pre-determined offsets). How will we need to modify the code to make it work?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699400,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "8bf5a527f12c0162918b46e3fa09f95d",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "729306cf-2fe3-4d55-be29-871ed20609fe",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Let us say we are using structured streaming for continuously reading data from Kafka and storing the results back into a Kafka topic using window function aggregates. Now, instead, we decide that we need to just perform a one-time batch operation using the same logic, where there is a need to read specific data from Kafka (i.e. using pre-determined offsets). How will we need to modify the code to make it work?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31262,
                            "question_id": 11581,
                            "option_text": "The read and write commands will remain the same, but the remaining codewill need to be modified, as operations on streaming dataframes are not supported on staticdataframes.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31263,
                            "question_id": 11581,
                            "option_text": "Only the read and write commands need to be modified to specify that it's abatch operation.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31264,
                            "question_id": 11581,
                            "option_text": "The entire code will need to be modified as the APIs for stream and batchprocessing are completely different.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31265,
                            "question_id": 11581,
                            "option_text": "The read and write commands need to be modified to specify that it\u2019s a batchoperation. Further, the specific logic of window functions will also need to be modified since thereare no time windows anymore in batch processing.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11582,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 192,
                    "question_text_1": "Kubernetes is an open-source system for automating deployment, scaling and management of containerized applications. Google Datastore and HBase are both highly-scalable NoSQL database systems for interactive, real-time applications. Consider the following pipeline choices for effecting the same outcome:  (i) Shell producer on VM on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 HBase on same Hadoop VMs on GCP (ii) Shell producer in Google Cloud Function \u2192 Kafka VM on GCP \u2192 Dataflow \u2192 Datastore (iii) Shell producer in Kubernetes on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Google Dataproc \u2192 Datastore (iv) Shell producer on VM on GCP \u2192 Pub/Sub \u2192 Dataflow \u2192 Datastore (v) Shell producer in Kubernetes on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 HBase on same Hadoop VMs on GCP  Which option below represents the correct order of pipeline options that has the \u201cmost IaaS\u201d entry to the left and the \u201cmost PaaS\u201d entry to the right?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699401,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "a206b8d4e52bf513b235159c867ce6fb",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "1eda0e0a-264a-43c8-b7b5-e856387d87c0",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Kubernetes is an open-source system for automating deployment, scaling and management of containerized applications. Google Datastore and HBase are both highly-scalable NoSQL database systems for interactive, real-time applications. Consider the following pipeline choices for effecting the same outcome:  (i) Shell producer on VM on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 HBase on same Hadoop VMs on GCP (ii) Shell producer in Google Cloud Function \u2192 Kafka VM on GCP \u2192 Dataflow \u2192 Datastore (iii) Shell producer in Kubernetes on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Google Dataproc \u2192 Datastore (iv) Shell producer on VM on GCP \u2192 Pub/Sub \u2192 Dataflow \u2192 Datastore (v) Shell producer in Kubernetes on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 HBase on same Hadoop VMs on GCP  Which option below represents the correct order of pipeline options that has the \u201cmost IaaS\u201d entry to the left and the \u201cmost PaaS\u201d entry to the right?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31266,
                            "question_id": 11582,
                            "option_text": "(i), (v), (iii), (iv), (ii)",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31267,
                            "question_id": 11582,
                            "option_text": "(i), (ii), (iii), (iv), (v)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31268,
                            "question_id": 11582,
                            "option_text": "(ii), (iii), (i), (iv), (v)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31269,
                            "question_id": 11582,
                            "option_text": "(v), (iii), (i), (iv), (ii)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31270,
                            "question_id": 11582,
                            "option_text": "All are equally PaaS / IaaS",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11583,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 193,
                    "question_text_1": "Since it is the onset of summer, there is a surge in railway ticket bookings. The business head at IRCTC is interested in a real-time view of all of her stations irrespective of whether there were bookings or not. She wants to see this be presented in a monitor mounted in her office wall that refreshes with the latest info on an India map every 1 minute along with the time of update so that she gets the confirmation that this is the latest data, so that she can plan for new summer-special trains as required. What solution option below best solves for the need?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699404,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "ebd9619765d335e9c338e5dbce8beaf8",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "a318085e-17b5-47d6-91ca-734c895cc65e",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Since it is the onset of summer, there is a surge in railway ticket bookings. The business head at IRCTC is interested in a real-time view of all of her stations irrespective of whether there were bookings or not. She wants to see this be presented in a monitor mounted in her office wall that refreshes with the latest info on an India map every 1 minute along with the time of update so that she gets the confirmation that this is the latest data, so that she can plan for new summer-special trains as required. What solution option below best solves for the need?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31271,
                            "question_id": 11583,
                            "option_text": "Route a copy of the ticket purchase to a Kafka topic, use Spark StructuredStreaming to continuously read from this topic and update the aggregates by destinations, andemit using output mode \u201cUpdate\u201d.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31272,
                            "question_id": 11583,
                            "option_text": "Route a copy of the ticket purchase to a Kafka topic, use Spark StructuredStreaming to periodically read from this topic every 1 minute and update the aggregates bydestinations, and emit all aggregates using the output mode \u201cComplete\u201d.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31273,
                            "question_id": 11583,
                            "option_text": "Route a copy of the ticket purchase to a Kafka topic, use Spark StructuredStreaming to periodically read from this topic every 1 minute and count the destinations in thatbatch, and emit only all aggregates in that batch using the output mode \u201cAppend\u201d.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11584,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 194,
                    "question_text_1": "In a streaming application, what is the purpose of dynamic scaling, and how does it contribute to resource utilization?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699405,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "790a5e1c38f2d4f70ac67e5a9045d064",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "0d43f56b-5a23-4379-92cf-0e6147772a97",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "In a streaming application, what is the purpose of dynamic scaling, and how does it contribute to resource utilization?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31274,
                            "question_id": 11584,
                            "option_text": "Dynamic scaling adjusts the number of partitions dynamically based on datavolume, optimizing resource usage.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31275,
                            "question_id": 11584,
                            "option_text": "It helps in controlling the frequency of data persistence in the streamingapplication.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31276,
                            "question_id": 11584,
                            "option_text": "Dynamic scaling is necessary for managing backpressure in the system.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31277,
                            "question_id": 11584,
                            "option_text": "It optimizes the execution plan of Spark SQL queries for better performance.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11585,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 195,
                    "question_text_1": "When designing a Spark Structured Streaming application with windowed aggregations, what is the significance of the watermark?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699406,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "f8887dfa38fbaee1bb0f5142c329bfcb",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "43c8d938-ab7b-4bcd-a23e-9aa2e02617f0",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "When designing a Spark Structured Streaming application with windowed aggregations, what is the significance of the watermark?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31278,
                            "question_id": 11585,
                            "option_text": "The watermark defines the maximum allowed lateness for events.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31279,
                            "question_id": 11585,
                            "option_text": "It controls the frequency of checkpointing in the streaming application.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31280,
                            "question_id": 11585,
                            "option_text": "The watermark is a timestamp indicating the current processing time.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31281,
                            "question_id": 11585,
                            "option_text": "It specifies the size of the time window for aggregations.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11586,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 196,
                    "question_text_1": "  Which of the following callbacks is used to handle successful message sends?",
                    "question_image_1": "WYWvRGbxrVwVAFVFyUn1kOSbu6z3cKulU3NIY6RqCgwspUGEeZ.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699407,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "e51561acd5434464db79091354f3c4ff",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "8b26e574-db9c-459b-82a0-1cb544e9392d",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/WYWvRGbxrVwVAFVFyUn1kOSbu6z3cKulU3NIY6RqCgwspUGEeZ.png"
                    ],
                    "question_texts": [
                        "  Which of the following callbacks is used to handle successful message sends?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31282,
                            "question_id": 11586,
                            "option_text": "on_completion",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31283,
                            "question_id": 11586,
                            "option_text": "on_failure",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31284,
                            "question_id": 11586,
                            "option_text": "on_acknowledge",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31285,
                            "question_id": 11586,
                            "option_text": "on_success",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11587,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 197,
                    "question_text_1": "  What is the purpose of the group_id parameter in the KafkaConsumer constructor?",
                    "question_image_1": "xucHvad6X60g6166vkXkaeOsnh9Z4yR6YrXYsStUZt5jUaVYaz.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699408,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "cdc6dd06ca1b916b52c933f45f9d9a7d",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "587ddd77-7b58-44f6-a053-d00f1b945b85",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/xucHvad6X60g6166vkXkaeOsnh9Z4yR6YrXYsStUZt5jUaVYaz.png"
                    ],
                    "question_texts": [
                        "  What is the purpose of the group_id parameter in the KafkaConsumer constructor?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31286,
                            "question_id": 11587,
                            "option_text": "It specifies the Kafka topic to consume messages from.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31287,
                            "question_id": 11587,
                            "option_text": "It uniquely identifies the consumer group to which the consumer belongs.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31288,
                            "question_id": 11587,
                            "option_text": "It defines the deserialization format for consumer records.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31289,
                            "question_id": 11587,
                            "option_text": "It sets the maximum number of records to be polled in each request.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11588,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 198,
                    "question_text_1": "  What does the provided partitioner lambda function do?",
                    "question_image_1": "axHcZZ02jj6HNZhlfZyLfoTTBip06uqexujcQiDhQHVaJX1IHJ.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699409,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "d0e0c08ed684dddee46cb9d1118b6c82",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "27fc925b-f641-458e-b430-24b3591f8347",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/axHcZZ02jj6HNZhlfZyLfoTTBip06uqexujcQiDhQHVaJX1IHJ.png"
                    ],
                    "question_texts": [
                        "  What does the provided partitioner lambda function do?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31290,
                            "question_id": 11588,
                            "option_text": "It uses the default partitioning strategy.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31291,
                            "question_id": 11588,
                            "option_text": "It forces all records to be sent to partition 0.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31292,
                            "question_id": 11588,
                            "option_text": "It dynamically selects the partition based on the key value.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31293,
                            "question_id": 11588,
                            "option_text": "It partitions records based on a round-robin distribution.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11589,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 199,
                    "question_text_1": "You are responsible for a Spark Streaming application that processes data from Kafka and provides real-time analytics. Currently, the application runs on a cluster managed by your team. The business stakeholders have expressed a need for more flexibility in adjusting the processing interval and want to explore options for real-time or near-real-time processing. Which of the following options is the best choice to meet the business requirements?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699410,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "1eab427ff9b868d52ac5b77918437663",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "e235d7e2-c2d1-4687-a341-d23c2a734899",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are responsible for a Spark Streaming application that processes data from Kafka and provides real-time analytics. Currently, the application runs on a cluster managed by your team. The business stakeholders have expressed a need for more flexibility in adjusting the processing interval and want to explore options for real-time or near-real-time processing. Which of the following options is the best choice to meet the business requirements?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31294,
                            "question_id": 11589,
                            "option_text": "Modify the existing Spark Streaming code to run every 5 minutes and deploy iton the existing cluster.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31295,
                            "question_id": 11589,
                            "option_text": "Rewrite the application using Spark Structured Streaming and allow thebusiness stakeholders to configure the processing interval.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31296,
                            "question_id": 11589,
                            "option_text": "Develop a Cloud Function to trigger the Spark Streaming job every 5 minutesand deploy it on a cloud platform.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31297,
                            "question_id": 11589,
                            "option_text": "Convert the Spark Streaming application to use Apache Flink for better real-time processing capabilities.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11590,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 200,
                    "question_text_1": null,
                    "question_image_1": "hlbH9MbXHtYfumtuHLPdoXFQPJAF3G57e9IQB7iGDsB9wWauIJ.png",
                    "question_type": "MCQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699397,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "cbe061e8b4aade85db7838f80dce5acd",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "b8f56bdf-4bb6-4f63-bd3d-1ee556d8ef38",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/hlbH9MbXHtYfumtuHLPdoXFQPJAF3G57e9IQB7iGDsB9wWauIJ.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31298,
                            "question_id": 11590,
                            "option_text": "Each subscriber gets one third of all messages published into Topic 1.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31299,
                            "question_id": 11590,
                            "option_text": "Publisher 1 can safely send data only for Subscriber 1\u2019s consumption whilerestricting access to Subscribers 2 & 3.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31300,
                            "question_id": 11590,
                            "option_text": "Subscriber 2 gets all messages published into Topic 1 by Publisher 1 but onlyhalf of Publisher 2\u2019s messages.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31301,
                            "question_id": 11590,
                            "option_text": "Each subscriber gets all messages published into Topic 1.",
                            "option_image": "",
                            "score": "3.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11591,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 201,
                    "question_text_1": "A Spark Streaming application is configured to execute once per minute. However, each run takes 10+ minutes consistently resulting in a never-ending backlog of work. The code in a nutshell looks as follows:  ",
                    "question_image_1": "nUohlwXUfbAlaHapGvQFWG9qCZ09CrJdAkqXaDrzvlSm3I2p70.png",
                    "question_type": "MCQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699402,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "f70f0db1d0ae825c0058db56dbbb082d",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": "A Spark Streaming application is configured to execute once per minute. However, each run takes 10+ minutes consistently resulting in a never-ending backlog of work. The code in a nutshell looks as follows:    Your goal is to optimize the code to bring down execution of each iteration within 1 minute. Which of the following represent options that will help in this mission?",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "8edbd1e1-c008-44e1-a83f-b084661775f4",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/nUohlwXUfbAlaHapGvQFWG9qCZ09CrJdAkqXaDrzvlSm3I2p70.png"
                    ],
                    "question_texts": [
                        "A Spark Streaming application is configured to execute once per minute. However, each run takes 10+ minutes consistently resulting in a never-ending backlog of work. The code in a nutshell looks as follows:  ",
                        "A Spark Streaming application is configured to execute once per minute. However, each run takes 10+ minutes consistently resulting in a never-ending backlog of work. The code in a nutshell looks as follows:    Your goal is to optimize the code to bring down execution of each iteration within 1 minute. Which of the following represent options that will help in this mission?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31302,
                            "question_id": 11591,
                            "option_text": "",
                            "option_image": "JHee1nDvfrq4A2IBa15pa7I6yXICYURPRUha6Zh66O2vKWhlUD.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": "app/option_images/JHee1nDvfrq4A2IBa15pa7I6yXICYURPRUha6Zh66O2vKWhlUD.png"
                        },
                        {
                            "id": 31303,
                            "question_id": 11591,
                            "option_text": "",
                            "option_image": "Diyoeq6rgTzlvWlfPJBEJbkW7XVyy7LmY423S5kHNPCUYb2IFv.png",
                            "score": "3.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": "app/option_images/Diyoeq6rgTzlvWlfPJBEJbkW7XVyy7LmY423S5kHNPCUYb2IFv.png"
                        },
                        {
                            "id": 31304,
                            "question_id": 11591,
                            "option_text": "",
                            "option_image": "doZ7lSIY43B7MiHZU2PScCUKYX74Cg4oy0B1FkQDoiWG8QECaB.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": "app/option_images/doZ7lSIY43B7MiHZU2PScCUKYX74Cg4oy0B1FkQDoiWG8QECaB.png"
                        },
                        {
                            "id": 31305,
                            "question_id": 11591,
                            "option_text": "",
                            "option_image": "tLf1VPrmNjIYkdRHE1HtWHklccf2rsFiUtxpj984k7mwkBLWON.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": "app/option_images/tLf1VPrmNjIYkdRHE1HtWHklccf2rsFiUtxpj984k7mwkBLWON.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11592,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 202,
                    "question_text_1": "You are given a Spark program that runs on a Google Dataproc cluster on a daily schedule from 1AM-12PM to produce as output the total amount of purchases made by every customer the previous day. The data is coming into GCS every minute from a variety of sources as standalone files. Therefore, the business leader now feels that having to wait till 12PM the next day is no longer acceptable and instead wants approximate purchase information for each customer at least every 5 minutes. What\u2019s more, she wants to be able to change this time window later as she pleases without involving you. Which amongst the below represents the best option to achieve the above?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699403,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "36d809e45a9df864a80779f10282e289",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "16937f31-0a63-4b97-a82f-ae2c6f41ffed",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are given a Spark program that runs on a Google Dataproc cluster on a daily schedule from 1AM-12PM to produce as output the total amount of purchases made by every customer the previous day. The data is coming into GCS every minute from a variety of sources as standalone files. Therefore, the business leader now feels that having to wait till 12PM the next day is no longer acceptable and instead wants approximate purchase information for each customer at least every 5 minutes. What\u2019s more, she wants to be able to change this time window later as she pleases without involving you. Which amongst the below represents the best option to achieve the above?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31306,
                            "question_id": 11592,
                            "option_text": "Change the code to run every 5 minutes, no other change required",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31307,
                            "question_id": 11592,
                            "option_text": "Change the code to leverage Spark Streaming with streaming window as \u201c5minutes\u201d, & let her manage the execution of the code on Dataproc",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31308,
                            "question_id": 11592,
                            "option_text": "Write a Cloud Function to move all incoming per-minute standalone files fromGCS to Pub/Sub, change the code to leverage Spark Streaming with streaming window as \u201c5 mins\u201d,convert from Dataproc to Dataflow, point source to Pub/Sub, & let her manage the execution ofthe code on Dataflow",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31309,
                            "question_id": 11592,
                            "option_text": "Change the code to leverage Spark Streaming with streaming window as \u201c5minutes\u201d, convert from Dataproc to Dataflow, & let her manage the execution of the code onDataflow",
                            "option_image": "",
                            "score": "3.000",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 11593,
                    "exam_id": 3,
                    "question_paper_id": 58,
                    "question_number": 203,
                    "question_text_1": "Which of the following are best practices associated with Streaming applications?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-10-16T08:15:00.000000Z",
                    "updated_at": "2024-10-16T08:15:00.000000Z",
                    "question_num_long": 640653699388,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "38a6603c23e94db769690d58b1474b48",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "f5fd7b95-eb00-4115-b7f1-6a6e14ecfc92",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of the following are best practices associated with Streaming applications?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 31310,
                            "question_id": 11593,
                            "option_text": "Use a message store that supports message replay so that no data is lost inprocessing.",
                            "option_image": "",
                            "score": "1.500",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31311,
                            "question_id": 11593,
                            "option_text": "\u201cHot potato\u201d principle is when the streaming application operates on data fromcache (i.e. the hot area of memory) and therefore is able to produce very high throughput",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31312,
                            "question_id": 11593,
                            "option_text": "Hadoop is best suited for executing Streaming applications",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31313,
                            "question_id": 11593,
                            "option_text": "Use checkpointing when faced with mission-critical workloads that require100% accuracy.",
                            "option_image": "",
                            "score": "1.500",
                            "is_correct": 1,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        },
                        {
                            "id": 31314,
                            "question_id": 11593,
                            "option_text": "Handle state pollution by restarting the persistent store software periodically.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-10-16T08:15:00.000000Z",
                            "updated_at": "2024-10-16T08:15:00.000000Z",
                            "option_number": null,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                }
            ]
        },
        "summary": "",
        "total_score": "50.00"
    },
    "url": "/question-paper/practise/44/af17573b-2c2d-4168-8c90-a94b29eb7fe0",
    "version": "ee3d5d44299e610bd137ea6200db5ac2"
}