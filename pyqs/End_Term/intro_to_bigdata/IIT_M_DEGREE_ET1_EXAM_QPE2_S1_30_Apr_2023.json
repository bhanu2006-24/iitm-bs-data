{
    "component": "Quizpractise/PractiseQuestionPaper",
    "props": {
        "errors": {},
        "auth": {
            "user": null
        },
        "flash": {
            "error": []
        },
        "banner": null,
        "file_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "file_do_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "question_paper": {
            "id": 204,
            "group_id": 22,
            "exam_id": 3,
            "total_score": "1.00",
            "duration": 4,
            "created_at": "2024-12-19T21:08:55.000000Z",
            "updated_at": "2024-12-19T21:08:55.000000Z",
            "question_paper_name": "IIT M DEGREE ET1 EXAM QPE2 S1 30 Apr 2023",
            "question_paper_description": "2023 Apr30: IIT M DEGREE ET1 EXAM QPE2",
            "uuid": "04e274ff-ef4",
            "year": 2023,
            "is_new": 0,
            "exam": {
                "id": 3,
                "exam_name": "End Term Quiz",
                "created_at": "2024-10-16T08:08:51.000000Z",
                "updated_at": "2024-10-16T08:08:51.000000Z",
                "uuid": "7a6ff569-f50c-40e7-a08b-f5c334392600",
                "en_id": "eyJpdiI6InNVdjR1b1B4b2hxNC9kU1NsY1l5R3c9PSIsInZhbHVlIjoiYUtFVkt6M2pIbWZLRVRaQ0Z3WHdFZz09IiwibWFjIjoiYTJhMWQzZTRiMWRjNTI5NTA3OGMzYzg3Y2E2NTYzZTJhYTZhODE4NjJjZTBmYjkyYTMzNTljZTRjYTBmMjYwZSIsInRhZyI6IiJ9"
            },
            "questions": [
                {
                    "id": 63546,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 246,
                    "question_text_1": "THIS IS QUESTION PAPER FOR THE SUBJECT <b>\"DEGREE LEVEL : INTRODUCTION TO BIG DATA </b>(COMPUTER BASED EXAM)\"<b> </b> ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565209,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "8da66f9cb22db136bac2f1bf149ebcc3",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "71f47c8e-e000-403e-9505-604d5b56eba7",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "THIS IS QUESTION PAPER FOR THE SUBJECT <b>\"DEGREE LEVEL : INTRODUCTION TO BIG DATA </b>(COMPUTER BASED EXAM)\"<b> </b> ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN.  (IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS REGISTERED BY YOU)"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 170982,
                            "question_id": 63546,
                            "option_text": "YES",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889365,
                            "option_image_url": null
                        },
                        {
                            "id": 170983,
                            "question_id": 63546,
                            "option_text": "NO",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889366,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63547,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 247,
                    "question_text_1": "What differentiates \u201cstreaming processing\u201d from \u201cbatch processing\u201d in the context of big data?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565210,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "8064d837e1fb64151fc4174188e974bb",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "1aefb545-92d7-4c41-8e8f-a1c363d867b6",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What differentiates \u201cstreaming processing\u201d from \u201cbatch processing\u201d in the context of big data?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 170984,
                            "question_id": 63547,
                            "option_text": "Batch operates on a set of data elements taken together while streamingoperates on data individually as it streams in",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889367,
                            "option_image_url": null
                        },
                        {
                            "id": 170985,
                            "question_id": 63547,
                            "option_text": "Batch operates on data that is static while streaming operates on data that isdynamically changing",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889368,
                            "option_image_url": null
                        },
                        {
                            "id": 170986,
                            "question_id": 63547,
                            "option_text": "Batch processing can assume data as fully specified and complete whilestreaming cannot make that assumption",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889369,
                            "option_image_url": null
                        },
                        {
                            "id": 170987,
                            "question_id": 63547,
                            "option_text": "Batch processing is typically high latency while streaming processing isnecessary for real-time latencies",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889370,
                            "option_image_url": null
                        },
                        {
                            "id": 170988,
                            "question_id": 63547,
                            "option_text": "Batch processing can operate on massively larger data sets than streamingcan",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889371,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63548,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 248,
                    "question_text_1": "What are the core components for a Big Data Streaming application?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565211,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "84bc2ad186000def0bd50787dcca4a2a",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "dce2c59c-a882-4c6d-807d-1d00c18fbe4c",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What are the core components for a Big Data Streaming application?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 170989,
                            "question_id": 63548,
                            "option_text": "Data needs to be retrievable from a persistent store that supports messagereplay. Replay refers to being able to fetch a specific set of data from that persistent store on-demand., where the data is chosen based on filters usually defined on sequence numbers ortimestamps",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889372,
                            "option_image_url": null
                        },
                        {
                            "id": 170990,
                            "question_id": 63548,
                            "option_text": "Data processing needs to be splittable across machines using divide-and-conquer, and composable into steps that execute very quickly",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889373,
                            "option_image_url": null
                        },
                        {
                            "id": 170991,
                            "question_id": 63548,
                            "option_text": "Hadoop needs to be setup for its big data capabilities",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889374,
                            "option_image_url": null
                        },
                        {
                            "id": 170992,
                            "question_id": 63548,
                            "option_text": "The application needs to have the cloud-native properties of being resilient,manageable and observable",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889375,
                            "option_image_url": null
                        },
                        {
                            "id": 170993,
                            "question_id": 63548,
                            "option_text": "Application needs to be deployable on public cloud natively using PaaScomponents",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889376,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63549,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 249,
                    "question_text_1": "A big data streaming application that reads from using Kafka as source is observed to be really slow. The Kafka cluster has 2 broker nodes and this application is reading from 1 topic that has 10 partitions. On closer investigation, it was found that Kafka is not scaling to the velocity of input data coming in. How will you scale Kafka further?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565213,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "93c9d522875cffc22432a375eaf39a7f",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "1171b9ec-31ee-473e-a1b9-8cfefc8b6619",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "A big data streaming application that reads from using Kafka as source is observed to be really slow. The Kafka cluster has 2 broker nodes and this application is reading from 1 topic that has 10 partitions. On closer investigation, it was found that Kafka is not scaling to the velocity of input data coming in. How will you scale Kafka further?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 170994,
                            "question_id": 63549,
                            "option_text": "Increase memory in each of the brokers in the cluster",
                            "option_image": "",
                            "score": "1.500",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889382,
                            "option_image_url": null
                        },
                        {
                            "id": 170995,
                            "question_id": 63549,
                            "option_text": "Add disks to each broker in the cluster",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889383,
                            "option_image_url": null
                        },
                        {
                            "id": 170996,
                            "question_id": 63549,
                            "option_text": "Add new brokers to the cluster",
                            "option_image": "",
                            "score": "1.500",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889384,
                            "option_image_url": null
                        },
                        {
                            "id": 170997,
                            "question_id": 63549,
                            "option_text": "Create more topics and change input application to reroute data to all topics tobe able to spread input data better",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889385,
                            "option_image_url": null
                        },
                        {
                            "id": 170998,
                            "question_id": 63549,
                            "option_text": "Double the number of partitions for this single topic to be able to spread inputdata better",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889386,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63550,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 250,
                    "question_text_1": "You are given the task of improving the performance of a Spark SQL program. You suspect that the culprit is the main transformation job in the program. When you run EXPLAIN on that SQL, you see that Spark wrongly estimates that there are only 10 values for the key being aggregated, whereas in reality the underlying data has a million values for that key. What actions would you perform from the below to ensure that the right estimates are used?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565214,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "42e31f33884a03e1cb9e1fcbee25fa07",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "93c1650f-1153-4b41-96ba-bf6031131875",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are given the task of improving the performance of a Spark SQL program. You suspect that the culprit is the main transformation job in the program. When you run EXPLAIN on that SQL, you see that Spark wrongly estimates that there are only 10 values for the key being aggregated, whereas in reality the underlying data has a million values for that key. What actions would you perform from the below to ensure that the right estimates are used?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 170999,
                            "question_id": 63550,
                            "option_text": "Create all tables as native Spark SQL tables (i.e. available as CatalogTables).",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889387,
                            "option_image_url": null
                        },
                        {
                            "id": 171000,
                            "question_id": 63550,
                            "option_text": "Partition all tables on the same key on which the aggregate is happening.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889388,
                            "option_image_url": null
                        },
                        {
                            "id": 171001,
                            "question_id": 63550,
                            "option_text": "Ensure cost based optimizer (CBO) is ON.",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889389,
                            "option_image_url": null
                        },
                        {
                            "id": 171002,
                            "question_id": 63550,
                            "option_text": "Run ANALYZE on all tables.",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889390,
                            "option_image_url": null
                        },
                        {
                            "id": 171003,
                            "question_id": 63550,
                            "option_text": "Cache the table in a step with actions ahead of the SQL statement that is theculprit.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889391,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63551,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 251,
                    "question_text_1": "Which of the following statements are true?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565221,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "999391f7e7b0cb7546e6ffeca419729c",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "72ff4efb-60d9-46e3-9a37-de25da365df0",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of the following statements are true?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171004,
                            "question_id": 63551,
                            "option_text": "In pull delivery, your subscriber application initiates requests to the Pub/Subserver to retrieve messages.",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889417,
                            "option_image_url": null
                        },
                        {
                            "id": 171005,
                            "question_id": 63551,
                            "option_text": "In push delivery, Pub/Sub initiates requests to your subscriber application todeliver messages.",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889418,
                            "option_image_url": null
                        },
                        {
                            "id": 171006,
                            "question_id": 63551,
                            "option_text": "Pull subscription is preferred when efficiency and throughput of messageprocessing is critical.",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889419,
                            "option_image_url": null
                        },
                        {
                            "id": 171007,
                            "question_id": 63551,
                            "option_text": "The rate of delivery needs to be controlled by the subscriber client in pushsubscription.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889420,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63552,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 252,
                    "question_text_1": "What happens when a Spark Structured Streaming pipeline operating with Kafka as source is subject to a machine failure?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565212,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "aab08e72e5262716054995d8c238ed8c",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "e92a7528-b041-40ad-83fa-49e13de5af79",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What happens when a Spark Structured Streaming pipeline operating with Kafka as source is subject to a machine failure?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171008,
                            "question_id": 63552,
                            "option_text": "The data being processed when the failure happened will have producedpartial results that result in incorrect outputs.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889377,
                            "option_image_url": null
                        },
                        {
                            "id": 171009,
                            "question_id": 63552,
                            "option_text": "The pipeline will not be restarted automatically.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889378,
                            "option_image_url": null
                        },
                        {
                            "id": 171010,
                            "question_id": 63552,
                            "option_text": "The pipeline will be restarted automatically by Spark which is able to pick upthe exact data from Kafka which was being processed at the time of error",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889379,
                            "option_image_url": null
                        },
                        {
                            "id": 171011,
                            "question_id": 63552,
                            "option_text": "Spark will throw an error and halt",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889380,
                            "option_image_url": null
                        },
                        {
                            "id": 171012,
                            "question_id": 63552,
                            "option_text": "Spark will gracefully shut down, reboot that machine which failed and start theStructured Streaming pipeline again to continue from where it left off",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889381,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63553,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 253,
                    "question_text_1": "A company with headquarters (HQ) in the Middle East operates on a Sunday-Thursday weekday schedule with Friday & Saturday as weekend days. It computes end of week revenue numbers by first computing sales for each day at 1AM local time of the next day, and then summing up the weekly sales every Sunday early morning at 3AM local time. This number gets reported to leadership every Sunday morning 9AM local time. As a result of management change, it has decided to relocate its HQ to India. Which of the following changes will need to be done to its ETL pipeline?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565215,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "c11fa144c6cec635d29c04c653c113cd",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "68260c22-1766-4b0f-a70f-03f2ff303f2d",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "A company with headquarters (HQ) in the Middle East operates on a Sunday-Thursday weekday schedule with Friday & Saturday as weekend days. It computes end of week revenue numbers by first computing sales for each day at 1AM local time of the next day, and then summing up the weekly sales every Sunday early morning at 3AM local time. This number gets reported to leadership every Sunday morning 9AM local time. As a result of management change, it has decided to relocate its HQ to India. Which of the following changes will need to be done to its ETL pipeline?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171013,
                            "question_id": 63553,
                            "option_text": "No change to business time since that remains constant at 1AM local time,only the schedule for the final weekly sum operation needs to be changed to the operational timeof Monday 3AM India time instead of Sunday 3AM Middle East time.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889392,
                            "option_image_url": null
                        },
                        {
                            "id": 171014,
                            "question_id": 63553,
                            "option_text": "Since time zone has changed as well as week definition too, the definition ofbusiness time has changed. So, the ETL has to be rewritten entirely.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889393,
                            "option_image_url": null
                        },
                        {
                            "id": 171015,
                            "question_id": 63553,
                            "option_text": "Nothing needs to change since daily sales is available at 1AM Middle East timewhich is anyway behind India time and so the numbers will be available before leadership comesin at 9AM.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889394,
                            "option_image_url": null
                        },
                        {
                            "id": 171016,
                            "question_id": 63553,
                            "option_text": "Event time has changed since the event of week ending has changed indefinition, and so the ETL needs to be changed to consider the new event in the data.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889395,
                            "option_image_url": null
                        },
                        {
                            "id": 171017,
                            "question_id": 63553,
                            "option_text": "No change required.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889396,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63554,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 254,
                    "question_text_1": "You are appointed as a Data Engineer in a company that has a legacy reporting application written in Java which suffers from both performance and maintainability problems. The reporting application plots dashboards with near real-time refresh (once every minute) of key business indicators to help management take live decisions. The application reads data directly from the source database of MongoDB, aggregates using simple counts and shows them visually in a UI. The performance problem of this application comes because the source database is at times overloaded and therefore the dashboard is not able to refresh fast enough. The maintainability issue is because every time a new KPI summary is required, the source has to be changed to include it. Choose from the options below the option that best tackles both problems:",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565216,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "684fc2fca694e08deed4ac56ebf7c459",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "574867bb-54b5-4a0a-a0d9-0ad3c973d396",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are appointed as a Data Engineer in a company that has a legacy reporting application written in Java which suffers from both performance and maintainability problems. The reporting application plots dashboards with near real-time refresh (once every minute) of key business indicators to help management take live decisions. The application reads data directly from the source database of MongoDB, aggregates using simple counts and shows them visually in a UI. The performance problem of this application comes because the source database is at times overloaded and therefore the dashboard is not able to refresh fast enough. The maintainability issue is because every time a new KPI summary is required, the source has to be changed to include it. Choose from the options below the option that best tackles both problems:"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171018,
                            "question_id": 63554,
                            "option_text": "Since MongoDB is OLTP, it is not able to support business reporting. So,replace it with Hadoop which supports OLAP better",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889397,
                            "option_image_url": null
                        },
                        {
                            "id": 171019,
                            "question_id": 63554,
                            "option_text": "Extract raw data from MongoDB using Change Data Capture (CDC) once everyminute into Kafka, and then use Spark Streaming to compute the KPIs and then populate into aNoSQL DB like Redis for the UI to consume.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889398,
                            "option_image_url": null
                        },
                        {
                            "id": 171020,
                            "question_id": 63554,
                            "option_text": "Convert the application from using plain Java to using Spark Streaming in Java",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889399,
                            "option_image_url": null
                        },
                        {
                            "id": 171021,
                            "question_id": 63554,
                            "option_text": "Query MongoDB every 1 minute for new data using a check on documentinserted timestamp, use Spark Streaming to compute the KPIs with the queried data, and thenpopulate into a NoSQL DB like Redis for the UI to consume.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889400,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63555,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 255,
                    "question_text_1": "Dhoni is on the crease with a bat in hand that has sensors embedded throughout. The sensors talk to the spider cam every second. The spider cam is itself a powerful ARM-based computer which has connectivity to the cloud through the wire on which it hangs. Using this connectivity, it can send as much or as little data as required and also receive instructions from the cloud. There is a machine learning model which suggests to the batsman to loosen the grip on his bat or tighten it based on the shots played using the sensor measurements. The way the suggestion happens is using dynamic vibration intensity communicated to the sensors embedded in the bat handle. Your task is to design the data pipeline that enables such feedback to Dhoni ideally before every ball with as much accuracy as possible throughout the match. Which of the following options best satisfies the requirements?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565217,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "ac03c708c8eca886df86b05013e861ad",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "914c017e-153b-49e6-a7e2-8464935e6ab4",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Dhoni is on the crease with a bat in hand that has sensors embedded throughout. The sensors talk to the spider cam every second. The spider cam is itself a powerful ARM-based computer which has connectivity to the cloud through the wire on which it hangs. Using this connectivity, it can send as much or as little data as required and also receive instructions from the cloud. There is a machine learning model which suggests to the batsman to loosen the grip on his bat or tighten it based on the shots played using the sensor measurements. The way the suggestion happens is using dynamic vibration intensity communicated to the sensors embedded in the bat handle. Your task is to design the data pipeline that enables such feedback to Dhoni ideally before every ball with as much accuracy as possible throughout the match. Which of the following options best satisfies the requirements?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171022,
                            "question_id": 63555,
                            "option_text": "Ingest all data into Pub/Sub, process using Google Cloud Dataflow, invoke theML model, and then write back output from Cloud to the spider cam to relay to the bat.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889401,
                            "option_image_url": null
                        },
                        {
                            "id": 171023,
                            "question_id": 63555,
                            "option_text": "Compress the ML model to fit into the spider cam\u2019s available resource, andwrite pipelines to execute the model in the spider cam itself",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889402,
                            "option_image_url": null
                        },
                        {
                            "id": 171024,
                            "question_id": 63555,
                            "option_text": "Compress the data in the spider cam every 5 seconds, write to Pub/Sub thecompressed data, invoke the ML model and then write back output from Cloud to the spider camto relay to the bat.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889403,
                            "option_image_url": null
                        },
                        {
                            "id": 171025,
                            "question_id": 63555,
                            "option_text": "Compress the ML model to fit into the spider cam\u2019s available resource, andwrite pipelines to execute in the spider cam itself, with periodically data being sent to the cloud,retrain the model using Google Cloud ML and then redeploy the model to the spider cam.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889404,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63556,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 256,
                    "question_text_1": "In the class, we saw the UDF for mobilenet_v2. By definition, UDFs are scalar. In Spark, there is another class of user defined routines called UDAFs, which stand for User Defined Aggregator Functions. UDAFs are meant to provide a means to write a custom aggregation function which aggregates over a grouping of values to arrive at a single value. UDAF structure differs saliently from UDFs in that it exposes the notion of a buffer as a way of maintaining intermediate state before finalizing aggregate output. Why is a buffer required for UDAF and not for a UDF?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565218,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "8b5c40d255d279241dedbf14d53f3c01",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "c00f1214-e8ad-47f8-bc9a-7515d68cb019",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "In the class, we saw the UDF for mobilenet_v2. By definition, UDFs are scalar. In Spark, there is another class of user defined routines called UDAFs, which stand for User Defined Aggregator Functions. UDAFs are meant to provide a means to write a custom aggregation function which aggregates over a grouping of values to arrive at a single value. UDAF structure differs saliently from UDFs in that it exposes the notion of a buffer as a way of maintaining intermediate state before finalizing aggregate output. Why is a buffer required for UDAF and not for a UDF?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171026,
                            "question_id": 63556,
                            "option_text": "A UDF is a scalar operation executing on 1 row at a time and producing outputimmediately, and therefore there is no intermediate output necessary. Whereas, a UDAF operateson multiple rows which will require multiple passes for the final output thereby requiring a buffer.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889405,
                            "option_image_url": null
                        },
                        {
                            "id": 171027,
                            "question_id": 63556,
                            "option_text": "A UDF is a scalar operation executing on many rows at a time, grouped by akey and producing a single output, and therefore there is no intermediate output necessary.Whereas, a UDAF operates on multiple rows which will require multiple passes for the final outputthereby requiring a buffer.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889406,
                            "option_image_url": null
                        },
                        {
                            "id": 171028,
                            "question_id": 63556,
                            "option_text": "A UDF is a scalar operation executing on 1 row at a time and producing outputimmediately, and therefore there is no intermediate output necessary. Whereas, a UDAF operateson multiple rows of unbounded size requiring a divide-and-conquer approach for computingaggregates, which uses the intermediate buffer to store partial values before finalizing the resultaggregate.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889407,
                            "option_image_url": null
                        },
                        {
                            "id": 171029,
                            "question_id": 63556,
                            "option_text": "A UDF is a scalar operation executing on many rows at a time, grouped by akey and producing a single output, and therefore there is no intermediate output necessary.Whereas, a UDAF operates on multiple rows of unbounded size requiring a divide-and-conquerapproach for computing aggregates, which uses the intermediate buffer to store partial valuesbefore finalizing the result aggregate.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889408,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63557,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 257,
                    "question_text_1": "In the class, we saw the UDF for mobilenet_v2. Specifically, the predict() function contained the below lines of code  ",
                    "question_image_1": "aZarp2xgbmuDFIisUzU7cWoRoKeyQa5BEKuWgXTV2HHhySSX1y.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565219,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "956fd3b6660938bd2565e4912d2c0d0c",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": "In the class, we saw the UDF for mobilenet_v2. Specifically, the predict() function contained the below lines of code    Why are these lines important in that pipeline of categorizing flowers?",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "4a76d439-ccc6-4ee7-b250-24655ffbb950",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/aZarp2xgbmuDFIisUzU7cWoRoKeyQa5BEKuWgXTV2HHhySSX1y.png"
                    ],
                    "question_texts": [
                        "In the class, we saw the UDF for mobilenet_v2. Specifically, the predict() function contained the below lines of code  ",
                        "In the class, we saw the UDF for mobilenet_v2. Specifically, the predict() function contained the below lines of code    Why are these lines important in that pipeline of categorizing flowers?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171030,
                            "question_id": 63557,
                            "option_text": "They check the syntax of the model function so that there are no errors",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889409,
                            "option_image_url": null
                        },
                        {
                            "id": 171031,
                            "question_id": 63557,
                            "option_text": "They load the model into memory so that the loop iteration over each imagedoes not need to do the same operation again and again given model load times are prohibitivelyexpensive for DL models",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889410,
                            "option_image_url": null
                        },
                        {
                            "id": 171032,
                            "question_id": 63557,
                            "option_text": "They invoke PyTorch libraries that have already been setup for model scoringon GCP using APIs embedded within the function",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889411,
                            "option_image_url": null
                        },
                        {
                            "id": 171033,
                            "question_id": 63557,
                            "option_text": "They are Map-style UDFs that make it an embarrassingly parallel computationthus making the execution parallelized and fast.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889412,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63558,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 258,
                    "question_text_1": "You are given a Spark Streaming pipeline that invokes a pre-trained DL model for every image it receives as input and produces the classification result in quick time. The model with the best recall rate from the PyTorch library takes 3.1 seconds on an average to execute on a Quad CPU Spark worker machine. However, the output is expected to be produced consistently within 3 seconds from having received the input. What is the best option to meet the expectations without compromising on false negatives and minimizing the amount of money & effort that is spent further?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565220,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "ec6c2831448b245d41f5cb62254303fe",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "7bad8057-3fae-454c-b5ac-8ecdc0b96720",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are given a Spark Streaming pipeline that invokes a pre-trained DL model for every image it receives as input and produces the classification result in quick time. The model with the best recall rate from the PyTorch library takes 3.1 seconds on an average to execute on a Quad CPU Spark worker machine. However, the output is expected to be produced consistently within 3 seconds from having received the input. What is the best option to meet the expectations without compromising on false negatives and minimizing the amount of money & effort that is spent further?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171034,
                            "question_id": 63558,
                            "option_text": "Use a different DL model that is faster but has half the recall rate.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889413,
                            "option_image_url": null
                        },
                        {
                            "id": 171035,
                            "question_id": 63558,
                            "option_text": "Build a custom model that compresses the highest recall rate model justenough to be able to execute within the stipulated time, and measure recall.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889414,
                            "option_image_url": null
                        },
                        {
                            "id": 171036,
                            "question_id": 63558,
                            "option_text": "Parallelize the DL model code using Divide-and-Conquer so that it runs faster.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889415,
                            "option_image_url": null
                        },
                        {
                            "id": 171037,
                            "question_id": 63558,
                            "option_text": "Change Spark machine to use GPUs",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889416,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63559,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 259,
                    "question_text_1": "Which of the following code snippets will give a runtime error? (Note: df is a spark dataframe. It has a column called content which has images represented as byte array)",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565223,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "2c4df830e5c9aded8987ce3d3c8b7af9",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "fe391f40-f08f-45ae-bdf9-57ecf0f2f681",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of the following code snippets will give a runtime error? (Note: df is a spark dataframe. It has a column called content which has images represented as byte array)"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171038,
                            "question_id": 63559,
                            "option_text": "",
                            "option_image": "BwIi3btTsBfjw1Av8qDUqBIjUo9LD0UrqNdMHqRLpWdAuPmv38.png",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889424,
                            "option_image_url": "app/option_images/BwIi3btTsBfjw1Av8qDUqBIjUo9LD0UrqNdMHqRLpWdAuPmv38.png"
                        },
                        {
                            "id": 171039,
                            "question_id": 63559,
                            "option_text": "",
                            "option_image": "OokZ9tjSDS45gR4xr8idLSgz9NKqmFgTZeve4y9ikM9Ck86wwL.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889425,
                            "option_image_url": "app/option_images/OokZ9tjSDS45gR4xr8idLSgz9NKqmFgTZeve4y9ikM9Ck86wwL.png"
                        },
                        {
                            "id": 171040,
                            "question_id": 63559,
                            "option_text": "",
                            "option_image": "llzRPsr2GmxzV6EFgKrj2U8rYMal1ORILBwcemcKKEBo73rjU7.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889426,
                            "option_image_url": "app/option_images/llzRPsr2GmxzV6EFgKrj2U8rYMal1ORILBwcemcKKEBo73rjU7.png"
                        },
                        {
                            "id": 171041,
                            "question_id": 63559,
                            "option_text": "",
                            "option_image": "lqRJTTaEBdUvtmSFHsxq7fQKt4liRz6N5dO9hihaTF6obZKP3s.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889427,
                            "option_image_url": "app/option_images/lqRJTTaEBdUvtmSFHsxq7fQKt4liRz6N5dO9hihaTF6obZKP3s.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63560,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 260,
                    "question_text_1": "Consider a Structured Streaming application running on Google Dataproc firing up every 10 seconds, consuming any number of records from Kafka available since last read, and emitting some computed answers to another Kafka topic. Consider also that apart from the functional logic, the same application is also emitting into a file the start time and end time of every batch invocation for audit purposes.  Assume there is a failure in one of the Dataproc machines that results in a failure of a specific run. For the external world (i.e. anybody consuming the outputs of this application), will they see the effect of the failure at all, or will it be as though there was no failure? Select the ones from the list below that answer this question.",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565224,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "dbf42fb1a5fac68e34688c0d60ba9b3c",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "37a1c865-de5c-4051-a47b-c6c70556afc4",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider a Structured Streaming application running on Google Dataproc firing up every 10 seconds, consuming any number of records from Kafka available since last read, and emitting some computed answers to another Kafka topic. Consider also that apart from the functional logic, the same application is also emitting into a file the start time and end time of every batch invocation for audit purposes.  Assume there is a failure in one of the Dataproc machines that results in a failure of a specific run. For the external world (i.e. anybody consuming the outputs of this application), will they see the effect of the failure at all, or will it be as though there was no failure? Select the ones from the list below that answer this question."
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171042,
                            "question_id": 63560,
                            "option_text": "Yes, the failure is visible. The only visible effect for the external world would bein the form of a slowdown in runtime for completion of that batch as Structured Streaming retriesthe batch that failed thus taking twice as much time as normal.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889428,
                            "option_image_url": null
                        },
                        {
                            "id": 171043,
                            "question_id": 63560,
                            "option_text": "Yes, the failure is visible because the side effect of emitting timestamps in abatch will be visible as 2 consecutive Start timestamps without any end timestamp as StructuredStreaming retries the failed batch.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889429,
                            "option_image_url": null
                        },
                        {
                            "id": 171044,
                            "question_id": 63560,
                            "option_text": "No, no failure is visible since Structured Streaming uses transactions andidempotence to achieve exactly-once processing.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889430,
                            "option_image_url": null
                        },
                        {
                            "id": 171045,
                            "question_id": 63560,
                            "option_text": "No, no failure is visible since Structured Streaming can process the same datain a retry resulting in the same outputs again.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889431,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63561,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 261,
                    "question_text_1": "Let us say we are using structured streaming for continuously reading data from Kafka and storing the results back into a Kafka topic. Now, instead, we decide that we need to just perform a one-time batch operation, where there is a need to read specific data from Kafka (i.e. using pre-determined offsets). How will we need to modify the code to make it work?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565225,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "26278a12295131b77c6c54d07fcd016d",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "39cb92c9-0e89-4529-8f10-4bd31869dda7",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Let us say we are using structured streaming for continuously reading data from Kafka and storing the results back into a Kafka topic. Now, instead, we decide that we need to just perform a one-time batch operation, where there is a need to read specific data from Kafka (i.e. using pre-determined offsets). How will we need to modify the code to make it work?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171046,
                            "question_id": 63561,
                            "option_text": "The read and write commands will remain the same, but the remaining codewill need to be modified, as operations on streaming dataframes are not supported on staticdataframes.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889432,
                            "option_image_url": null
                        },
                        {
                            "id": 171047,
                            "question_id": 63561,
                            "option_text": "Only the read and write commands need to be modified to specify that it's abatch operation.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889433,
                            "option_image_url": null
                        },
                        {
                            "id": 171048,
                            "question_id": 63561,
                            "option_text": "The entire code will need to be modified as the APIs for stream and batchprocessing are completely different.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889434,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63562,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 262,
                    "question_text_1": "Consider a Kafka system that has two brokers with the exact same specifications. Let\u2019s consider a topic A with a single partition, whose two copies are being maintained by Kafka. Consider the following cases:  Case 1: Both the copies reside in the same broker Case 2: Each broker has one copy of the partition  Two of the key promises of Kafka are:  (i) Availability (ii) Throughput  Regarding which of the above promises, case 1 is at a disadvantage as compared to case 2?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565226,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "60835b7b78f22252110b9481ace20eff",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "a4bc9b0f-4dfb-4731-ad4d-b18d21df0bba",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider a Kafka system that has two brokers with the exact same specifications. Let\u2019s consider a topic A with a single partition, whose two copies are being maintained by Kafka. Consider the following cases:  Case 1: Both the copies reside in the same broker Case 2: Each broker has one copy of the partition  Two of the key promises of Kafka are:  (i) Availability (ii) Throughput  Regarding which of the above promises, case 1 is at a disadvantage as compared to case 2?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171049,
                            "question_id": 63562,
                            "option_text": "Only (i)",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889435,
                            "option_image_url": null
                        },
                        {
                            "id": 171050,
                            "question_id": 63562,
                            "option_text": "Only (ii)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889436,
                            "option_image_url": null
                        },
                        {
                            "id": 171051,
                            "question_id": 63562,
                            "option_text": "Both (i) and (ii)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889437,
                            "option_image_url": null
                        },
                        {
                            "id": 171052,
                            "question_id": 63562,
                            "option_text": "Neither (i) nor (ii)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889438,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63563,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 263,
                    "question_text_1": "Kubernetes is an open-source system for automating deployment, scaling and management of containerized applications. Google Datastore and HBase are both highly-scalable NoSQL database systems for interactive, real-time applications. Consider the following pipeline choices for effecting the same outcome:  (i) Java producer on VM on GCP \u2192 Kafka VM on GCP \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 HBase on same Hadoop VMs on GCP (ii) Java producer in Google Cloud Function \u2192 Pub/Sub \u2192 Dataflow \u2192 Datastore (iii) Java producer in Kubernetes on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Google Dataproc \u2192 Datastore (iv) Java producer on VM on GCP \u2192 Pub/Sub \u2192 Dataflow \u2192 Datastore (v) Java producer in Kubernetes on GCP \u2192 Kafka VM on GCP \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 HBase on same Hadoop VMs on GCP  Which option below represents the correct order of pipeline options that has the \u201cmost IaaS\u201d entry to the left and the \u201cmost PaaS\u201d entry to the right?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565227,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "07cdfb25abf36c9f2fadd21d5bfa207b",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "75c227e7-a6f6-4da8-8a2f-609f73954a60",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Kubernetes is an open-source system for automating deployment, scaling and management of containerized applications. Google Datastore and HBase are both highly-scalable NoSQL database systems for interactive, real-time applications. Consider the following pipeline choices for effecting the same outcome:  (i) Java producer on VM on GCP \u2192 Kafka VM on GCP \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 HBase on same Hadoop VMs on GCP (ii) Java producer in Google Cloud Function \u2192 Pub/Sub \u2192 Dataflow \u2192 Datastore (iii) Java producer in Kubernetes on GCP \u2192 Pub/Sub \u2192 Spark Streaming on Google Dataproc \u2192 Datastore (iv) Java producer on VM on GCP \u2192 Pub/Sub \u2192 Dataflow \u2192 Datastore (v) Java producer in Kubernetes on GCP \u2192 Kafka VM on GCP \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 HBase on same Hadoop VMs on GCP  Which option below represents the correct order of pipeline options that has the \u201cmost IaaS\u201d entry to the left and the \u201cmost PaaS\u201d entry to the right?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171053,
                            "question_id": 63563,
                            "option_text": "(i), (v), (iii), (iv), (ii)",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889439,
                            "option_image_url": null
                        },
                        {
                            "id": 171054,
                            "question_id": 63563,
                            "option_text": "(i), (ii), (iii), (iv), (v)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889440,
                            "option_image_url": null
                        },
                        {
                            "id": 171055,
                            "question_id": 63563,
                            "option_text": "(ii), (iii), (i), (iv), (v)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889441,
                            "option_image_url": null
                        },
                        {
                            "id": 171056,
                            "question_id": 63563,
                            "option_text": "(v), (iii), (i), (iv), (ii)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889442,
                            "option_image_url": null
                        },
                        {
                            "id": 171057,
                            "question_id": 63563,
                            "option_text": "All are equally PaaS / IaaS",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889443,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63564,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 264,
                    "question_text_1": "Since it is the onset of summer, there is a surge in railway ticket bookings. The business head at IRCTC is interested in a real-time view of what are the top 10 destinations being booked in descending order of count of tickets purchased where port of origin is one of the tier-1 urban centres such as Mumbai, Delhi, Chennai etc. She wants to see this be presented in a monitor mounted in her office wall that refreshes with the latest info on an India map every 1 minute along with the time of update so that she gets the confirmation that this is the latest data, so that she can plan for new summer-special trains as required. What solution option below best solves for the need?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565230,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "cdcbeb218934cd30ffef451a8e5fe1a9",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "9b097281-5783-4ffa-80bc-ed483dd55f17",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Since it is the onset of summer, there is a surge in railway ticket bookings. The business head at IRCTC is interested in a real-time view of what are the top 10 destinations being booked in descending order of count of tickets purchased where port of origin is one of the tier-1 urban centres such as Mumbai, Delhi, Chennai etc. She wants to see this be presented in a monitor mounted in her office wall that refreshes with the latest info on an India map every 1 minute along with the time of update so that she gets the confirmation that this is the latest data, so that she can plan for new summer-special trains as required. What solution option below best solves for the need?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171058,
                            "question_id": 63564,
                            "option_text": "Route a copy of the ticket purchase to a Kafka topic, use Spark StructuredStreaming to continuously read from this topic and update the aggregates by destinations, andemit using output mode \u201cUpdate\u201d.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889452,
                            "option_image_url": null
                        },
                        {
                            "id": 171059,
                            "question_id": 63564,
                            "option_text": "Route a copy of the ticket purchase to a Kafka topic, use Spark StructuredStreaming to periodically read from this topic every 1 minute and update the aggregates bydestinations, and emit top aggregates using the output mode \u201cComplete\u201d.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889453,
                            "option_image_url": null
                        },
                        {
                            "id": 171060,
                            "question_id": 63564,
                            "option_text": "Route a copy of the ticket purchase to a Kafka topic, use Spark StructuredStreaming to periodically read from this topic every 1 minute and count the destinations in thatbatch, and emit only top aggregates in that batch using the output mode \u201cAppend\u201d.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889454,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63565,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 265,
                    "question_text_1": "Observe the below image and select the options that are true.  ",
                    "question_image_1": "p73cba1jQZeDk6TBGzz6tf7S7A2oaTqzbj5a96a7gFuc8UofZ2.png",
                    "question_type": "MCQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565222,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "35095b3476eb38a805a4bebb7e36fd9a",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": "Observe the below image and select the options that are true.  ",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "bf1ea3f5-ae09-4a47-8d91-718341459531",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/p73cba1jQZeDk6TBGzz6tf7S7A2oaTqzbj5a96a7gFuc8UofZ2.png"
                    ],
                    "question_texts": [
                        "Observe the below image and select the options that are true.  ",
                        "Observe the below image and select the options that are true.  "
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171061,
                            "question_id": 63565,
                            "option_text": "Each subscriber gets one third of all messages published into Topic 1.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889421,
                            "option_image_url": null
                        },
                        {
                            "id": 171062,
                            "question_id": 63565,
                            "option_text": "Each subscriber gets one third of all messages published into Topic 1 byPublisher 1",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889422,
                            "option_image_url": null
                        },
                        {
                            "id": 171063,
                            "question_id": 63565,
                            "option_text": "Each subscriber gets all messages published into Topic 1.",
                            "option_image": "",
                            "score": "3.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889423,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63566,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 266,
                    "question_text_1": "A Spark Streaming application is configured to execute once per minute. However, each run takes 10+ minutes consistently resulting in a never-ending backlog of work. The code in a nutshell looks as follows:  ",
                    "question_image_1": "0FFyJl7xH0GZvpTpGWi3MIcqO4yeiJVlO2kD8ir8FFlqKf4FAw.png",
                    "question_type": "MCQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565228,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "25ff4ad90ea733727aa18bc5ac98c96e",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": "A Spark Streaming application is configured to execute once per minute. However, each run takes 10+ minutes consistently resulting in a never-ending backlog of work. The code in a nutshell looks as follows:  ",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "b1796903-c0b8-484f-bc16-054fd58f2442",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/0FFyJl7xH0GZvpTpGWi3MIcqO4yeiJVlO2kD8ir8FFlqKf4FAw.png"
                    ],
                    "question_texts": [
                        "A Spark Streaming application is configured to execute once per minute. However, each run takes 10+ minutes consistently resulting in a never-ending backlog of work. The code in a nutshell looks as follows:  ",
                        "A Spark Streaming application is configured to execute once per minute. However, each run takes 10+ minutes consistently resulting in a never-ending backlog of work. The code in a nutshell looks as follows:  "
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171064,
                            "question_id": 63566,
                            "option_text": "",
                            "option_image": "PMbaK9wfCltkWJdX2VxjJhjN3BIByUkVA2qRDbdUaCuULJveLX.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889444,
                            "option_image_url": "app/option_images/PMbaK9wfCltkWJdX2VxjJhjN3BIByUkVA2qRDbdUaCuULJveLX.png"
                        },
                        {
                            "id": 171065,
                            "question_id": 63566,
                            "option_text": "",
                            "option_image": "tbHkDV2UKjom7AnvEqUFnvfynkdfvVQVKcxxQzdfNLpfWgvT92.png",
                            "score": "3.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889445,
                            "option_image_url": "app/option_images/tbHkDV2UKjom7AnvEqUFnvfynkdfvVQVKcxxQzdfNLpfWgvT92.png"
                        },
                        {
                            "id": 171066,
                            "question_id": 63566,
                            "option_text": "",
                            "option_image": "kKTdlF0NLop9xzoqsmFRzeFvzRuuTuvwG1TjdaKO2x9Z79d7ts.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889446,
                            "option_image_url": "app/option_images/kKTdlF0NLop9xzoqsmFRzeFvzRuuTuvwG1TjdaKO2x9Z79d7ts.png"
                        },
                        {
                            "id": 171067,
                            "question_id": 63566,
                            "option_text": "",
                            "option_image": "aui1vUABtNa79UcJnPqPrqUjfGuLfnynvE2A8dz1Zp2kkdVEZr.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889447,
                            "option_image_url": "app/option_images/aui1vUABtNa79UcJnPqPrqUjfGuLfnynvE2A8dz1Zp2kkdVEZr.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 63567,
                    "exam_id": 3,
                    "question_paper_id": 204,
                    "question_number": 267,
                    "question_text_1": "You are given a Spark program that runs on a Google Dataproc cluster on a daily schedule from 1AM-12PM to produce as output the total amount of purchases made by every customer the previous day. The data is coming into GCS every minute from a variety of sources as standalone files. Therefore, the business leader now feels that having to wait till 12PM the next day is no longer acceptable and instead wants approximate purchase information for each customer at least every hour. What\u2019s more, she wants to control the computation of the outcome completely with flexibility to change the schedule as she wishes without the need for you or any other developer to be involved. Which amongst the below represents the best option to achieve the above?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "3.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2024-12-19T21:08:57.000000Z",
                    "updated_at": "2024-12-19T21:08:57.000000Z",
                    "question_num_long": 640653565229,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "a8988ed8a3109842f2db51d6a093501a",
                    "course_id": 44,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "af7e1298-872f-456e-be79-eabbe130537d",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are given a Spark program that runs on a Google Dataproc cluster on a daily schedule from 1AM-12PM to produce as output the total amount of purchases made by every customer the previous day. The data is coming into GCS every minute from a variety of sources as standalone files. Therefore, the business leader now feels that having to wait till 12PM the next day is no longer acceptable and instead wants approximate purchase information for each customer at least every hour. What\u2019s more, she wants to control the computation of the outcome completely with flexibility to change the schedule as she wishes without the need for you or any other developer to be involved. Which amongst the below represents the best option to achieve the above?"
                    ],
                    "course": {
                        "id": 44,
                        "course_name": "Intro to BigData",
                        "course_code": "Intro to BigData",
                        "created_at": "2024-10-16T08:15:00.000000Z",
                        "updated_at": "2024-10-16T08:15:00.000000Z",
                        "program_id": 1,
                        "uuid": "0a6ea890-ca63-49c6-a7b3-cc71e4a81ada"
                    },
                    "options": [
                        {
                            "id": 171068,
                            "question_id": 63567,
                            "option_text": "Change the code to assume hourly data instead of daily data, changescheduler to run every hour & let her manage the execution of the code on Dataproc",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889448,
                            "option_image_url": null
                        },
                        {
                            "id": 171069,
                            "question_id": 63567,
                            "option_text": "Change the code to leverage Spark Streaming with streaming window as \u201c1hour\u201d, & let her manage the execution of the code on Dataproc",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889449,
                            "option_image_url": null
                        },
                        {
                            "id": 171070,
                            "question_id": 63567,
                            "option_text": "Change the code to leverage Spark Streaming with streaming window as \u201c1hour\u201d, convert from Dataproc to Dataflow, & let her manage the execution of the code onDataflow",
                            "option_image": "",
                            "score": "3.000",
                            "is_correct": 1,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889450,
                            "option_image_url": null
                        },
                        {
                            "id": 171071,
                            "question_id": 63567,
                            "option_text": "Write a Cloud Function to move all incoming per-minute standalone files fromGCS to Pub/Sub, change the code to leverage Spark Streaming with streaming window as \u201c1 hour\u201d,convert from Dataproc to Dataflow, point source to Pub/Sub, & let her manage the execution ofthe code on Dataflow",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2024-12-19T21:08:57.000000Z",
                            "updated_at": "2024-12-19T21:08:57.000000Z",
                            "option_number": 6406531889451,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                }
            ]
        },
        "summary": "---\n### **Core Topics, Concepts, Principles, and Equations for \"Introduction to Big Data\" Exam**\n\n#### **1. Big Data Processing Paradigms**\n- **Batch Processing vs. Streaming Processing**\n  - **Batch Processing**:\n    - Operates on **static, complete datasets**.\n    - High latency (minutes/hours).\n    - Assumes data is fully specified (e.g., Hadoop MapReduce, Spark Batch).\n    - Scales to **massively large datasets**.\n  - **Streaming Processing**:\n    - Operates on **dynamic, unbounded data** (real-time).\n    - Low latency (milliseconds/seconds).\n    - Cannot assume data completeness (e.g., Spark Streaming, Kafka Streams, Flink).\n    - Requires **state management** (e.g., checkpoints, watermarks).\n\n#### **2. Core Components of Big Data Streaming Applications**\n- **Data Retrieval & Replay**:\n  - Persistent store (e.g., Kafka) must support **message replay** (fetch data by timestamps/sequence numbers).\n- **Divide-and-Conquer Processing**:\n  - Data processing must be **splittable across machines** (parallelizable).\n  - Steps must be **composable** (modular pipelines).\n- **Resilience, Manageability, Observability**:\n  - Cloud-native properties (e.g., fault tolerance, monitoring, logging).\n- **Deployment**:\n  - Should support **public cloud PaaS** (e.g., Google Dataproc, AWS EMR).\n\n#### **3. Scaling Kafka**\n- **Bottlenecks**:\n  - **Broker limitations**: CPU, memory, disk I/O, network.\n  - **Partition limitations**: Too few partitions \u2192 underutilized brokers.\n- **Scaling Strategies**:\n  - **Add brokers** to the cluster (horizontal scaling).\n  - **Increase memory/disk** per broker (vertical scaling).\n  - **Increase partitions** for a topic (if consumers can handle parallelism).\n  - Avoid **adding topics** unless necessary (complexity overhead).\n\n#### **4. Spark SQL Performance Optimization**\n- **Problem**: Incorrect cardinality estimates \u2192 poor query plans.\n- **Solutions**:\n  - Enable **Cost-Based Optimizer (CBO)**:\n    ```sql\n    SET spark.sql.cbo.enabled=true;\n    ```\n  - Run **ANALYZE TABLE** to update statistics:\n    ```sql\n    ANALYZE TABLE table_name COMPUTE STATISTICS;\n    ```\n  - Use **native Spark SQL tables** (catalog tables) for better metadata management.\n  - **Avoid caching** unless necessary (can skew statistics).\n\n#### **5. Pub/Sub Delivery Models**\n- **Pull Delivery**:\n  - Subscriber **initially requests** messages from the Pub/Sub server.\n  - Preferred for **high throughput** (subscriber controls rate).\n- **Push Delivery**:\n  - Pub/Sub **pushes messages** to the subscriber\u2019s endpoint.\n  - Preferred for **low-latency** use cases.\n  - Subscriber must **acknowledge** messages to avoid redelivery.\n\n#### **6. Fault Tolerance in Spark Structured Streaming**\n- **Checkpointing**:\n  - Spark **automatically restarts** failed pipelines from the last checkpoint.\n  - **Exactly-once processing**: Uses **transactions + idempotent sinks** (e.g., Kafka).\n- **Failure Visibility**:\n  - **Side effects** (e.g., logging timestamps) may show duplicates if retries occur.\n  - **Output modes**:\n    - `append`: Only new results.\n    - `complete`: Full aggregated results (e.g., for top-N queries).\n    - `update`: Only changed results.\n\n#### **7. Time Handling in ETL Pipelines**\n- **Business Time vs. Event Time**:\n  - **Business time**: Fixed schedule (e.g., \"end of day\" at 1 AM local time).\n  - **Event time**: When the event actually occurred (e.g., timestamp in data).\n- **Time Zone Changes**:\n  - Adjust **scheduling** (e.g., weekly aggregation from Sunday \u2192 Monday).\n  - **No need to rewrite ETL** if business time logic is parameterized.\n\n#### **8. Real-Time Data Pipelines**\n- **Design Principles**:\n  - **Low latency**: Process data as close to the source as possible (edge computing).\n  - **Scalability**: Use **streaming frameworks** (Spark, Flink) + **message brokers** (Kafka, Pub/Sub).\n  - **Model Serving**:\n    - **Edge deployment**: Run ML models on-edge (e.g., spider cam) if latency is critical.\n    - **Cloud retraining**: Periodically update models in the cloud and redeploy.\n- **Example Pipeline**:\n  ```\n  Sensors \u2192 Spider Cam (Edge ML) \u2192 Pub/Sub \u2192 Cloud Retraining \u2192 Redeploy to Edge\n  ```\n\n#### **9. User-Defined Functions (UDFs) vs. User-Defined Aggregate Functions (UDAFs)**\n| **Feature**       | **UDF**                          | **UDAF**                          |\n|-------------------|----------------------------------|-----------------------------------|\n| **Purpose**       | Scalar operations (row-wise).   | Aggregations (group-wise).        |\n| **Input**         | Single row.                      | Multiple rows (grouped).          |\n| **State**         | Stateless.                       | **Buffer** for intermediate state.|\n| **Example**       | `x \u2192 x\u00b2`                         | `SUM(x)` over a group.            |\n| **Performance**   | Fast (embarrassingly parallel).  | Slower (requires merging).        |\n\n- **Why Buffers in UDAFs?**\n  - Aggregations require **partial results** (e.g., `SUM` needs to accumulate values).\n  - Buffers store **intermediate state** before finalizing the result.\n\n#### **10. Optimizing Spark Streaming Jobs**\n- **Common Bottlenecks**:\n  - **Joins**: Shuffling data across executors.\n  - **GroupBy/Aggregations**: Skewed keys \u2192 uneven workload.\n- **Optimizations**:\n  - **Broadcast small datasets**:\n    ```python\n    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")\n    ```\n  - **Repartition skewed data**:\n    ```python\n    df.repartition(100, \"key\")\n    ```\n  - **Avoid full shuffles**: Pre-aggregate where possible.\n  - **Use `mapPartitions`** for batch operations.\n\n#### **11. Spark Structured Streaming Output Modes**\n| **Mode**      | **Behavior**                                  | **Use Case**                          |\n|---------------|---------------------------------------------|---------------------------------------|\n| `append`      | Only new rows since last trigger.           | Streaming ETL (no aggregations).     |\n| `complete`    | Full result set (e.g., aggregated tables).  | Top-N queries, real-time dashboards.  |\n| `update`      | Only rows changed since last trigger.       | Incremental updates (e.g., KPIs).    |\n\n#### **12. Kafka Replication and Fault Tolerance**\n- **Replication Factor (RF)**:\n  - Each partition has `RF` copies (e.g., `RF=2` \u2192 2 copies).\n- **Availability**:\n  - Copies on **different brokers** \u2192 survives broker failures.\n  - Copies on **same broker** \u2192 single point of failure.\n- **Throughput**:\n  - **No impact** from replication (as long as brokers are balanced).\n\n#### **13. IaaS vs. PaaS in Big Data Pipelines**\n| **Component**       | **IaaS (Infrastructure)**               | **PaaS (Platform)**                  |\n|---------------------|-----------------------------------------|--------------------------------------|\n| **Compute**         | VMs (GCP Compute Engine)                | Serverless (Cloud Functions, Dataproc) |\n| **Messaging**       | Self-managed Kafka                      | Pub/Sub                              |\n| **Processing**      | Spark on Hadoop VMs                     | Dataflow, Dataproc                  |\n| **Storage**         | HDFS, self-managed HBase                | Datastore, BigQuery                  |\n| **Orchestration**   | Manual/Kubernetes                       | Workflows, Composer                 |\n\n- **Order from IaaS \u2192 PaaS**:\n  ```\n  (VMs + Hadoop) \u2192 (Kubernetes + HBase) \u2192 (Dataproc + Datastore) \u2192 (Dataflow + Pub/Sub) \u2192 (Cloud Functions + Datastore)\n  ```\n\n#### **14. Real-Time Aggregations for Dashboards**\n- **Requirements**:\n  - **Low latency** (e.g., 1-minute refresh).\n  - **Accurate aggregations** (e.g., top-10 destinations).\n- **Solution**:\n  - **Streaming window** (e.g., 1-hour tumbling window).\n  - **Output mode `complete`** (full results per trigger).\n  - **Sink to a database** (e.g., Redis, Datastore) for dashboard consumption.\n\n#### **15. Publish-Subscribe Model**\n- **Key Concepts**:\n  - **Topic**: Channel for messages (e.g., `Topic1`).\n  - **Subscription**: Independent stream of messages per subscriber.\n  - **Delivery Semantics**:\n    - **At-least-once**: Messages may be redelivered (requires deduplication).\n    - **Exactly-once**: No duplicates (requires transactional systems).\n- **Misconceptions**:\n  - Subscribers **do not** split messages (each gets **all messages** in a subscription).\n\n---\n### **Mermaid Knowledge Graphs**\n\n#### **Graph 1: Big Data Processing Paradigms**\n```mermaid\ngraph TD\n    A[Big Data Processing] --> B[Batch Processing]\n    A --> C[Streaming Processing]\n    B --> B1[Static Data]\n    B --> B2[High Latency]\n    B --> B3[Complete Data Assumption]\n    C --> C1[Dynamic Data]\n    C --> C2[Low Latency]\n    C --> C3[No Completeness Assumption]\n    C --> C4[State Management]\n```\n\n#### **Graph 2: Spark Structured Streaming Components**\n```mermaid\ngraph TD\n    A[Spark Structured Streaming] --> B[Source]\n    A --> C[Processing]\n    A --> D[Sink]\n    B --> B1[Kafka]\n    B --> B2[Pub/Sub]\n    C --> C1[Transformations]\n    C --> C2[Aggregations]\n    C --> C3[Joins]\n    D --> D1[Kafka]\n    D --> D2[Database]\n    D --> D3[Files]\n    A --> E[Fault Tolerance]\n    E --> E1[Checkpointing]\n    E --> E2[Exactly-Once Processing]\n```\n\n#### **Graph 3: Kafka Scaling and Replication**\n```mermaid\ngraph TD\n    A[Kafka Cluster] --> B[Broker 1]\n    A --> C[Broker 2]\n    B --> B1[Partition 1 Leader]\n    B --> B2[Partition 2 Follower]\n    C --> C1[Partition 1 Follower]\n    C --> C2[Partition 2 Leader]\n    A --> D[Scaling Strategies]\n    D --> D1[Add Brokers]\n    D --> D2[Increase Partitions]\n    D --> D3[Increase Resources]\n```\n\n#### **Graph 4: Pub/Sub Delivery Models**\n```mermaid\ngraph TD\n    A[Pub/Sub] --> B[Pull Delivery]\n    A --> C[Push Delivery]\n    B --> B1[Subscriber Initiates]\n    B --> B2[High Throughput]\n    C --> C1[Pub/Sub Pushes]\n    C --> C2[Low Latency]\n    C --> C3[Subscriber ACKs]\n```\n\n#### **Graph 5: UDF vs. UDAF in Spark**\n```mermaid\ngraph TD\n    A[User-Defined Functions] --> B[UDF]\n    A --> C[UDAF]\n    B --> B1[Scalar Operations]\n    B --> B2[Stateless]\n    B --> B3[Row-wise]\n    C --> C1[Aggregations]\n    C --> C2[Stateful]\n    C --> C3[Buffer for Intermediate State]\n    C --> C4[Group-wise]\n```\n\n#### **Graph 6: Real-Time Pipeline for Dhoni's Bat Sensors**\n```mermaid\ngraph LR\n    A[Bat Sensors] --> B[Spider Cam]\n    B --> C[Edge ML Model]\n    C --> D[Vibration Feedback]\n    B --> E[Pub/Sub]\n    E --> F[Cloud Retraining]\n    F --> C\n```\n\n---\n### **Key Equations (Mathematics/Performance)**\n1. **Spark Parallelism**:\n   - Optimal partitions = `(Total Data Size) / (Target Partition Size)`\n   - Example: For 100GB data with 128MB partitions \u2192 `100GB / 128MB \u2248 800 partitions`.\n\n2. **Kafka Throughput**:\n   - `Throughput = (Number of Partitions) \u00d7 (Messages/sec per Partition)`\n   - Scaling: Increase partitions or brokers to improve throughput.\n\n3. **Streaming Latency**:\n   - `End-to-End Latency = Ingestion Latency + Processing Latency + Sink Latency`\n   - Goal: Minimize each component (e.g., use GPUs for DL models).\n\n4. **Aggregation Buffer Size (UDAF)**:\n   - `Buffer Memory = (Number of Groups) \u00d7 (Size per Group)`\n   - Example: 1M groups with 1KB state each \u2192 `1GB buffer`.",
        "total_score": "50.00"
    },
    "url": "/question-paper/practise/44/04e274ff-ef4",
    "version": "ee3d5d44299e610bd137ea6200db5ac2"
}