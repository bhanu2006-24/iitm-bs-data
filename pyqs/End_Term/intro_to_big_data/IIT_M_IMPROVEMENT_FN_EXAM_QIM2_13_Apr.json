{
    "component": "Quizpractise/PractiseQuestionPaper",
    "props": {
        "errors": {},
        "auth": {
            "user": null
        },
        "flash": {
            "error": []
        },
        "banner": null,
        "file_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "file_do_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "question_paper": {
            "id": 268,
            "group_id": 27,
            "exam_id": 3,
            "total_score": "1.00",
            "duration": 4,
            "created_at": "2025-04-17T15:56:21.000000Z",
            "updated_at": "2025-04-17T15:56:21.000000Z",
            "question_paper_name": "IIT M IMPROVEMENT FN EXAM QIM2 13 Apr",
            "question_paper_description": "2025 Apr13: IIT M FN EXAM QIM2",
            "uuid": "9e03d1fe-3a5",
            "year": 2025,
            "is_new": 0,
            "exam": {
                "id": 3,
                "exam_name": "End Term Quiz",
                "created_at": "2024-10-16T08:08:51.000000Z",
                "updated_at": "2024-10-16T08:08:51.000000Z",
                "uuid": "7a6ff569-f50c-40e7-a08b-f5c334392600",
                "en_id": "eyJpdiI6IkllUEtNUkZCaWlYbEpudmhoQm8rOXc9PSIsInZhbHVlIjoiVnM0cTB2MjF6VDNrbFRsY0x0RVQ3UT09IiwibWFjIjoiODYzNjllMzgxNDNhNTM0NDk3NWNmZGJiZjQwZTQ1ZDEzNDI3ZDE3YzJhNTE1NjhlNzJjNWYxYWU3MDE5ZmUzNSIsInRhZyI6IiJ9"
            },
            "questions": [
                {
                    "id": 85541,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 186,
                    "question_text_1": "THIS IS QUESTION PAPER FOR THE SUBJECT <b>\"DEGREE LEVEL : INTRODUCTION TO BIG DATA </b><b>(COMPUTER BASED EXAM)\"</b><b> </b><b> </b><b>ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? </b><b>CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN. </b><b> </b><b>(IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS </b><b>REGISTERED BY YOU)</b>",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:22.000000Z",
                    "updated_at": "2025-04-17T15:56:22.000000Z",
                    "question_num_long": 6406531249980,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "4f118dea7e6ce47b64c530143bbf48b0",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "094342ab-337e-4b40-a976-042362fbbb9c",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "THIS IS QUESTION PAPER FOR THE SUBJECT <b>\"DEGREE LEVEL : INTRODUCTION TO BIG DATA </b><b>(COMPUTER BASED EXAM)\"</b><b> </b><b> </b><b>ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? </b><b>CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN. </b><b> </b><b>(IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS </b><b>REGISTERED BY YOU)</b>"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228604,
                            "question_id": 85541,
                            "option_text": "YES",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:22.000000Z",
                            "updated_at": "2025-04-17T15:56:22.000000Z",
                            "option_number": 6406534212206,
                            "option_image_url": null
                        },
                        {
                            "id": 228605,
                            "question_id": 85541,
                            "option_text": "NO",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:22.000000Z",
                            "updated_at": "2025-04-17T15:56:22.000000Z",
                            "option_number": 6406534212207,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85542,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 187,
                    "question_text_1": "Which of the following is a false characterization of \"hot potato\u201d principle?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249981,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "ce4a285875b7ca4811caa21bfef23dec",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "c469d36d-22a1-48df-ba47-5fc5ef8655fb",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of the following is a false characterization of \"hot potato\u201d principle?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228606,
                            "question_id": 85542,
                            "option_text": "It is a rule-of-thumb, not a principle, and therefore is not mandatory",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212208,
                            "option_image_url": null
                        },
                        {
                            "id": 228607,
                            "question_id": 85542,
                            "option_text": "It provides the same benefits when incorporated into the design of both batchas well as streaming data applications.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212209,
                            "option_image_url": null
                        },
                        {
                            "id": 228608,
                            "question_id": 85542,
                            "option_text": "It emphasizes minimal work in each processing step for maximum scalabilityand optimal recoverability in the event of failures.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212210,
                            "option_image_url": null
                        },
                        {
                            "id": 228609,
                            "question_id": 85542,
                            "option_text": "It is critically dependent on a message store as the via media for itsintermediate outputs.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212211,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85543,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 188,
                    "question_text_1": "A website that started 3 years ago now sees 1 Billion hits every month. The website owner wants to count average hits per customer across all months, where a customer is denoted by the IP address of the device from which the customer is accessing the website. The owner has at his disposal a minimal Hadoop cluster of 2 workers and 1 master each with 1GB of RAM. He would like to run this job every month going forward. Which of the following methods is the most likely to finish every time it is run in the months and years ahead yielding the right result?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249982,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "88afcf493426625d0f69e58449820af6",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "e606d5d4-bf28-4948-bfa6-ac7391bb52fe",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "A website that started 3 years ago now sees 1 Billion hits every month. The website owner wants to count average hits per customer across all months, where a customer is denoted by the IP address of the device from which the customer is accessing the website. The owner has at his disposal a minimal Hadoop cluster of 2 workers and 1 master each with 1GB of RAM. He would like to run this job every month going forward. Which of the following methods is the most likely to finish every time it is run in the months and years ahead yielding the right result?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228610,
                            "question_id": 85543,
                            "option_text": "Write a MapReduce program where the Map does nothing useful, Combinecomputes the aggregated hits per customer, Shuffle combines data based on IP address across workers, and the Reduce builds a hash table on each machine with hash key = IP address and hash value = running total and sum, with a final Map that emits the avg per customer.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212212,
                            "option_image_url": null
                        },
                        {
                            "id": 228611,
                            "question_id": 85543,
                            "option_text": "Write a Spark program that forms a Dataframe as grouping by IP address withcount as aggregate, followed by a take into a list in the Spark driver which further computes the average of all the individual counts in the list",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212213,
                            "option_image_url": null
                        },
                        {
                            "id": 228612,
                            "question_id": 85543,
                            "option_text": "Write a Spark program that forms a Dataframe as grouping by IP address withcount as aggregate, followed by another stage that computes the avg on top of the Dataframe of the first stage",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212214,
                            "option_image_url": null
                        },
                        {
                            "id": 228613,
                            "question_id": 85543,
                            "option_text": "Write a MapReduce program where the Map does nothing useful, Combinecomputes the aggregated hits per customer, Shuffle combines data based on IP address across workers, and the Reduce sorts the data on sort key = IP address and then calculates the final avg per customer.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212215,
                            "option_image_url": null
                        },
                        {
                            "id": 228614,
                            "question_id": 85543,
                            "option_text": "All will finish every time it is run without issues.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212216,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85544,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 189,
                    "question_text_1": "An enterprise software designer wants to leverage the best of Google cloud to minimize the number of administrative overheads associated with her payment processing pipeline while also getting on-demand scalability without sacrificing flexibility. What option should she choose to best serve these needs?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249983,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "47737f628e22965ce6d36ff37dbd7977",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "215fb15f-ec97-40c4-ad05-f15a23c9cecb",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "An enterprise software designer wants to leverage the best of Google cloud to minimize the number of administrative overheads associated with her payment processing pipeline while also getting on-demand scalability without sacrificing flexibility. What option should she choose to best serve these needs?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228615,
                            "question_id": 85544,
                            "option_text": "Build the payment processer using VMs \u2013 one for Python for the logic, one forinvoking the external payment engine, and one for storing the results",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212217,
                            "option_image_url": null
                        },
                        {
                            "id": 228616,
                            "question_id": 85544,
                            "option_text": "Build the payment processor using Python running on Google Cloud Functionswhere the results are stored on GCS",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212218,
                            "option_image_url": null
                        },
                        {
                            "id": 228617,
                            "question_id": 85544,
                            "option_text": "Build the payment processor using MapReduce with input data and results arestored on HDFS, and deploy both on Dataproc",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212219,
                            "option_image_url": null
                        },
                        {
                            "id": 228618,
                            "question_id": 85544,
                            "option_text": "Build the payment processor using Dataflow on top of data stored on GCS",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212220,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85545,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 190,
                    "question_text_1": "Consider an application that can scale from handling 1000 users to handling 100 million users by simply making copies of itself, has automation to detect task failures so that it can boot up a new task automatically, and logs operational metadata on a central logger backed by Kafka. Which of the following statements is true?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249984,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "c053f339197736790d63d834d5081242",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "65b11e04-f814-414f-b7f6-4f5993f4c22a",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider an application that can scale from handling 1000 users to handling 100 million users by simply making copies of itself, has automation to detect task failures so that it can boot up a new task automatically, and logs operational metadata on a central logger backed by Kafka. Which of the following statements is true?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228619,
                            "question_id": 85545,
                            "option_text": "This application has adopted cloud-native design",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212221,
                            "option_image_url": null
                        },
                        {
                            "id": 228620,
                            "question_id": 85545,
                            "option_text": "This application cannot be called as cloud-native since it is not observable at alltimes",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212222,
                            "option_image_url": null
                        },
                        {
                            "id": 228621,
                            "question_id": 85545,
                            "option_text": "This application cannot be called as cloud-native since it is not manageableeasily",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212223,
                            "option_image_url": null
                        },
                        {
                            "id": 228622,
                            "question_id": 85545,
                            "option_text": "This application cannot be called as cloud-native since it is not resilient",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212224,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85546,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 191,
                    "question_text_1": "Consider a file \u201cdata.bin\u201d which is formatted as follows: every data record has 10 key-value pairs of the format \u201ckey,value\u201d, with each pair separated by a comma, where the \u201ckey\u201d is the name of a field and the \u201cvalue\u201d is the value for that field in that record. Every data record occurs in its own line. You are asked to write a data processing script using Python that scales with big data. Which of the following represents your approach?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249985,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "0b9fd314e0b248f0cb8a4c6b5eecbc18",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "e1f95d48-2b01-40d8-af21-99f0d4b62c6a",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider a file \u201cdata.bin\u201d which is formatted as follows: every data record has 10 key-value pairs of the format \u201ckey,value\u201d, with each pair separated by a comma, where the \u201ckey\u201d is the name of a field and the \u201cvalue\u201d is the value for that field in that record. Every data record occurs in its own line. You are asked to write a data processing script using Python that scales with big data. Which of the following represents your approach?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228623,
                            "question_id": 85546,
                            "option_text": "Since data.bin is compliant with the RFC 4180, use PySpark\u2019s read_csv() to readthe data as is.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212225,
                            "option_image_url": null
                        },
                        {
                            "id": 228624,
                            "question_id": 85546,
                            "option_text": "Rename the file data.bin to data.csv to make it compliant with RFC 4180 andthen use PySpark\u2019s read_csv() to read the data",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212226,
                            "option_image_url": null
                        },
                        {
                            "id": 228625,
                            "question_id": 85546,
                            "option_text": "The problem cannot be solved since the file cannot be converted to a validformat for reading consistently without additional information",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212227,
                            "option_image_url": null
                        },
                        {
                            "id": 228626,
                            "question_id": 85546,
                            "option_text": "Write PySpark code to read all lines in data.bin, use string split on \u201c,\u201d asdelimiter, and then collect all column names and corresponding values into a RDD for further processing",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212228,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85547,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 192,
                    "question_text_1": "You join the data engineering team at a company that has good big data expertise already. Your first assignment is to convert Spark code written by previous engineers that used Spark 1.0 to use newer features and best practices. None of those previous engineers work in the company anymore, and nobody in the present team can tell you what data files available in the company data lake today correspond to which part of the processing done then. Which of the following most accurately captures your ability to do the task at hand?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249986,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "aa3cd878741214e4ad4c8ef0b0b23bcd",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "807ec184-6dc2-4869-9621-89462548bace",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You join the data engineering team at a company that has good big data expertise already. Your first assignment is to convert Spark code written by previous engineers that used Spark 1.0 to use newer features and best practices. None of those previous engineers work in the company anymore, and nobody in the present team can tell you what data files available in the company data lake today correspond to which part of the processing done then. Which of the following most accurately captures your ability to do the task at hand?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228627,
                            "question_id": 85547,
                            "option_text": "The task is impossible since Spark 1.0 used RDDs which cannot be convertedto Dataframes without the correct data schema.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212229,
                            "option_image_url": null
                        },
                        {
                            "id": 228628,
                            "question_id": 85547,
                            "option_text": "The task is impossible since Spark core and syntax has changed so completelythat those data files it processed then can no longer be processed by current Spark versions.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212230,
                            "option_image_url": null
                        },
                        {
                            "id": 228629,
                            "question_id": 85547,
                            "option_text": "The task is possible with multiple trial-and-error schema experimentsmatching data files with the older code.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212231,
                            "option_image_url": null
                        },
                        {
                            "id": 228630,
                            "question_id": 85547,
                            "option_text": "The task is possible since Spark has backwards compability.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212232,
                            "option_image_url": null
                        },
                        {
                            "id": 228631,
                            "question_id": 85547,
                            "option_text": "None of these, since not enough information is available.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212233,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85548,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 193,
                    "question_text_1": "What happens when a Spark Structured Streaming pipeline operating with Kafka as source and console output as target is subject to a failure of a machine in either of the Kafka cluster or the Spark cluster?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249987,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "34985fbba4738a3b51dd9e50ebf162ce",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "9bf1d12c-d97c-4a0c-8510-14d845d6eff8",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What happens when a Spark Structured Streaming pipeline operating with Kafka as source and console output as target is subject to a failure of a machine in either of the Kafka cluster or the Spark cluster?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228632,
                            "question_id": 85548,
                            "option_text": "Failure of a machine in the Kafka cluster will result in an Exception in the Sparkpipeline which will then fail and halt.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212234,
                            "option_image_url": null
                        },
                        {
                            "id": 228633,
                            "question_id": 85548,
                            "option_text": "The pipeline will be restarted automatically by Spark which is able to pick upthe exact data from Kafka which was being processed at the time of error, resulting in exactly-once semantics.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212235,
                            "option_image_url": null
                        },
                        {
                            "id": 228634,
                            "question_id": 85548,
                            "option_text": "The Spark pipeline will not be able to start again from previously committedoffset by restarting itself, resulting in at least-once processing semantics",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212236,
                            "option_image_url": null
                        },
                        {
                            "id": 228635,
                            "question_id": 85548,
                            "option_text": "Irrespective of whatever machine fails, Spark will throw an error and halt.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212237,
                            "option_image_url": null
                        },
                        {
                            "id": 228636,
                            "question_id": 85548,
                            "option_text": "Data that is being processed will not be processed again, resulting in at most-once semantics.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212238,
                            "option_image_url": null
                        },
                        {
                            "id": 228637,
                            "question_id": 85548,
                            "option_text": "Spark will be able to pick up data from Kafka from exactly that offset whichfailed but may produce duplicate output on the target resulting in at least once semantics for the consumer of the output.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212239,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85549,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 194,
                    "question_text_1": "A big data streaming application that uses Kafka as source is observed to be really lagging behind currently live data. The Kafka cluster has 2 broker nodes and this application is reading from 1 topic that has 10 partitions. On closer investigation, it was found that Kafka is not scaling to the velocity of input data coming in. What can you first try to do to scale Kafka further while incurring minimal overall costs?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249988,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "8bd6965554a6f6ea3c0ece99ffd5c8b8",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "acbde7c3-9310-4b6b-8442-128276e2a1a1",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "A big data streaming application that uses Kafka as source is observed to be really lagging behind currently live data. The Kafka cluster has 2 broker nodes and this application is reading from 1 topic that has 10 partitions. On closer investigation, it was found that Kafka is not scaling to the velocity of input data coming in. What can you first try to do to scale Kafka further while incurring minimal overall costs?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228638,
                            "question_id": 85549,
                            "option_text": "Add disks to each broker in the cluster, and disks are the cheapest computercomponent",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212240,
                            "option_image_url": null
                        },
                        {
                            "id": 228639,
                            "question_id": 85549,
                            "option_text": "Add new brokers to the cluster, even though this is more expensive than theother options this is the only foolproof way to scale.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212241,
                            "option_image_url": null
                        },
                        {
                            "id": 228640,
                            "question_id": 85549,
                            "option_text": "Create more topics and change input application to reroute data to all topicsto be able to spread input data better. This is nearly the least expensive since only developer effort is required to change application.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212242,
                            "option_image_url": null
                        },
                        {
                            "id": 228641,
                            "question_id": 85549,
                            "option_text": "Double the number of partitions for this single topic to be able to spread inputdata better. This is the least expensive since only administrator effort is required without changing application.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212243,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85550,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 195,
                    "question_text_1": "A company with headquarters (HQ) in the Middle East operates on a Sunday-Thursday weekday schedule with Friday & Saturday as weekend days. It computes end of week revenue numbers using an ETL pipeline by first computing sales for each day at 1AM local time of the next day, and then summing up the weekly sales every Sunday early morning at 3AM local time. This number gets reported to leadership every Sunday morning 9AM local time. Therefore, the ETL pipeline is scheduled to run every Sunday morning at 8AM local time. As a result of management change, the company has decided to shutdown its business in the Middle East and relocate all of its operations and business HQ to India. Which of the following changes will need to be done to its ETL pipeline to ensure the correct output continues to be produced?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249989,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "0aaa90881f57cdd9c18a8ac9dac3139c",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "afb7b0cb-7d59-456f-b330-af81ed7923bf",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "A company with headquarters (HQ) in the Middle East operates on a Sunday-Thursday weekday schedule with Friday & Saturday as weekend days. It computes end of week revenue numbers using an ETL pipeline by first computing sales for each day at 1AM local time of the next day, and then summing up the weekly sales every Sunday early morning at 3AM local time. This number gets reported to leadership every Sunday morning 9AM local time. Therefore, the ETL pipeline is scheduled to run every Sunday morning at 8AM local time. As a result of management change, the company has decided to shutdown its business in the Middle East and relocate all of its operations and business HQ to India. Which of the following changes will need to be done to its ETL pipeline to ensure the correct output continues to be produced?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228642,
                            "question_id": 85550,
                            "option_text": "The business time for the final weekly sum operation needs to be changed tothat of Monday 3AM India time instead of Sunday 3AM Middle East time.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212244,
                            "option_image_url": null
                        },
                        {
                            "id": 228643,
                            "question_id": 85550,
                            "option_text": "There is zero change needed since neither event time nor business time ischanging whereas only the operational time is changing.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212245,
                            "option_image_url": null
                        },
                        {
                            "id": 228644,
                            "question_id": 85550,
                            "option_text": "Since time zone has changed as well as week definition too, the definition ofbusiness time has changed. So, the ETL has to be rewritten entirely.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212246,
                            "option_image_url": null
                        },
                        {
                            "id": 228645,
                            "question_id": 85550,
                            "option_text": "Nothing needs to change since daily sales is available at 1AM Middle East timewhich is anyway behind India time and so the numbers will be available before leadership comes in at 9AM.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212247,
                            "option_image_url": null
                        },
                        {
                            "id": 228646,
                            "question_id": 85550,
                            "option_text": "Event time has changed since the event of week ending has changed indefinition, and so the ETL needs to be changed to consider the new event in the data.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212248,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85551,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 196,
                    "question_text_1": "You are appointed as a Data Engineer in a company that has a legacy reporting application written in Java which suffers from performance problems. The reporting application plots dashboards with daily refresh of key business indicators to help management take the best decisions. The application reads data directly from the source database of MongoDB, aggregates using simple counts and shows them visually in a UI. The performance problem of this application comes because the source database is at times overloaded and therefore the dashboard is not able to retrieve answers fast enough making the end user wait for the result. Choose the best option that gives the best performance with minimal maintenance effort:",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249990,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "eb5a4d3396a3fac6fa3a5a361e030729",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "f765e1ae-e1a3-4ecd-b633-62a99c9464ae",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are appointed as a Data Engineer in a company that has a legacy reporting application written in Java which suffers from performance problems. The reporting application plots dashboards with daily refresh of key business indicators to help management take the best decisions. The application reads data directly from the source database of MongoDB, aggregates using simple counts and shows them visually in a UI. The performance problem of this application comes because the source database is at times overloaded and therefore the dashboard is not able to retrieve answers fast enough making the end user wait for the result. Choose the best option that gives the best performance with minimal maintenance effort:"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228647,
                            "question_id": 85551,
                            "option_text": "Since MongoDB is OLTP, it is not able to support business reporting. So, bringthe data into Hadoop end of day, and run OLAP queries on it from the same UI.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212249,
                            "option_image_url": null
                        },
                        {
                            "id": 228648,
                            "question_id": 85551,
                            "option_text": "Convert the application from using plain Java to using Spark Streaming in Java",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212250,
                            "option_image_url": null
                        },
                        {
                            "id": 228649,
                            "question_id": 85551,
                            "option_text": "Extract raw data from MongoDB using Change Data Capture (CDC) once everyminute into Kafka, and then use Spark Streaming to compute the KPIs and then populate into a NoSQL DB like Redis for the UI to consume.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212251,
                            "option_image_url": null
                        },
                        {
                            "id": 228650,
                            "question_id": 85551,
                            "option_text": "Query MongoDB every 1 minute for new data using a check on documentinserted timestamp, use Spark Streaming to compute the KPIs with the queried data, and then populate into a NoSQL DB like Redis for the UI to consume.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212252,
                            "option_image_url": null
                        },
                        {
                            "id": 228651,
                            "question_id": 85551,
                            "option_text": "Convert application to using Python along with a NoSQL database for storingand retrieving the aggregated counts.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212253,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85552,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 197,
                    "question_text_1": "Fielder on the midwicket boundary is wearing a smart watch. Unlike other smart watches, this one is unique in that it helps him field. Whenever the ball is headed his direction, the watch vibrates to alert him. When there is a chance of a catch by this fielder, the watch continually whispers every second to go forward, back, left or right thus improving his chances of settling under the ball and taking the catch. The watch is connected to the spider cam. The spider cam is itself a powerful ARM-based computer which has connectivity to the Cloud, an all-seeing AI-powered superbeing, through the wire on which it hangs. Using this connectivity, it can send as much or as little data as required and also receive instructions from the cloud. Your task is to design the data pipeline that enables such feedback to the fielder for every ball that comes his way with as much accuracy as possible throughout the match. You are given two ML models: a vision model that given a frame from the spider cam computes an alert if the ball is headed to him, and an adjustment model that helps adjust catch positioning of the fielder which takes as input a continuous video feed. Which of the following options best satisfies the requirements?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249991,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "ea5998989998fea2f4cd1052f5f5f66f",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "6641706d-d00d-45c2-a34a-be9f5a886769",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Fielder on the midwicket boundary is wearing a smart watch. Unlike other smart watches, this one is unique in that it helps him field. Whenever the ball is headed his direction, the watch vibrates to alert him. When there is a chance of a catch by this fielder, the watch continually whispers every second to go forward, back, left or right thus improving his chances of settling under the ball and taking the catch. The watch is connected to the spider cam. The spider cam is itself a powerful ARM-based computer which has connectivity to the Cloud, an all-seeing AI-powered superbeing, through the wire on which it hangs. Using this connectivity, it can send as much or as little data as required and also receive instructions from the cloud. Your task is to design the data pipeline that enables such feedback to the fielder for every ball that comes his way with as much accuracy as possible throughout the match. You are given two ML models: a vision model that given a frame from the spider cam computes an alert if the ball is headed to him, and an adjustment model that helps adjust catch positioning of the fielder which takes as input a continuous video feed. Which of the following options best satisfies the requirements?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228652,
                            "question_id": 85552,
                            "option_text": "Ingest all spider cam data including video feeds into Pub/Sub, process usingGoogle Cloud Dataflow where you invoke the vision model and the adjustment model, relaying this output to the spider cam continuously.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212254,
                            "option_image_url": null
                        },
                        {
                            "id": 228653,
                            "question_id": 85552,
                            "option_text": "Compress the vision model and adjustment model to fit into the spider cam\u2019savailable resource, and write pipelines to execute the models in the spider cam itself",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212255,
                            "option_image_url": null
                        },
                        {
                            "id": 228654,
                            "question_id": 85552,
                            "option_text": "Compress the vision model and adjustment model to fit into the spider cam\u2019savailable resource, and write pipelines to execute in the spider cam itself, with periodically data being sent to the cloud for retraining the vision model using Google Cloud ML and then redeploy the model to the spider cam.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212256,
                            "option_image_url": null
                        },
                        {
                            "id": 228655,
                            "question_id": 85552,
                            "option_text": "Compress the data in the spider cam every 5 seconds, write to Pub/Sub thecompressed data, invoke the vision model and, if needed, the adjustment model, and then write back output from Cloud to the spider cam to relay to the fielder.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212257,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85553,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 198,
                    "question_text_1": null,
                    "question_image_1": "QJgfjaBlzh3dNFwAIYtRropRBh1rtY6HN47A5NwJfpxH4oJ7aU.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249992,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "864f7064bd4d7037b6765ac2efae0b3b",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "859cae29-af70-44a0-b60e-8978d2786328",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/QJgfjaBlzh3dNFwAIYtRropRBh1rtY6HN47A5NwJfpxH4oJ7aU.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228656,
                            "question_id": 85553,
                            "option_text": "Yes, since they check the syntax of the model function so that there are noerrors",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212258,
                            "option_image_url": null
                        },
                        {
                            "id": 228657,
                            "question_id": 85553,
                            "option_text": "Yes, they invoke PyTorch libraries that have already been setup for modelscoring on GCP using APIs embedded within the function",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212259,
                            "option_image_url": null
                        },
                        {
                            "id": 228658,
                            "question_id": 85553,
                            "option_text": "No, since the primary function of these lines of code is to eliminate repeatedDL model loads as DL models are large in size",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212260,
                            "option_image_url": null
                        },
                        {
                            "id": 228659,
                            "question_id": 85553,
                            "option_text": "Yes, they are Map-style UDFs that make it an embarrassingly parallelcomputation thus making the execution parallelized and fast.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212261,
                            "option_image_url": null
                        },
                        {
                            "id": 228660,
                            "question_id": 85553,
                            "option_text": "Yes, since the new model will potentially have large load times.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212262,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85554,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 199,
                    "question_text_1": "You are given a Spark Streaming pipeline that invokes a pre-trained DL model for every image it receives as input and produces the classification result in quick time. The model with the best recall rate from the PyTorch library runs in under 3 seconds on an average, when executing on a GPU-powered Spark cluster. Your management has now instructed you to reduce the cost of AI projects significantly, and has given guidance that latency of execution is not a concern since the consumer for the model output is batch oriented, and also that they would be ok with a lesser recall rate provided the drop isn\u2019t significant. What is the best option to explore first to meet the expectations?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249993,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "9e544c6e18221aa7fb103a85ad8337ba",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "dc8f9ad2-7fd2-41d4-b315-e5473b43dc3a",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are given a Spark Streaming pipeline that invokes a pre-trained DL model for every image it receives as input and produces the classification result in quick time. The model with the best recall rate from the PyTorch library runs in under 3 seconds on an average, when executing on a GPU-powered Spark cluster. Your management has now instructed you to reduce the cost of AI projects significantly, and has given guidance that latency of execution is not a concern since the consumer for the model output is batch oriented, and also that they would be ok with a lesser recall rate provided the drop isn\u2019t significant. What is the best option to explore first to meet the expectations?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228661,
                            "question_id": 85554,
                            "option_text": "Build a custom model that compresses the highest recall rate model justenough to be able to execute within a single Spark worker and thus reduce cost of the cluster.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212263,
                            "option_image_url": null
                        },
                        {
                            "id": 228662,
                            "question_id": 85554,
                            "option_text": "Use a different DL model from PyTorch that has better efficiency at lesserrecall levels so that the cluster size can be minimized.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212264,
                            "option_image_url": null
                        },
                        {
                            "id": 228663,
                            "question_id": 85554,
                            "option_text": "Remove complexity associated with Spark Streaming, remove GPUs to savesignificant costs, and convert the model execution pipeline into a single threaded Python application using traditional ML running on a CPU-only machine.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212265,
                            "option_image_url": null
                        },
                        {
                            "id": 228664,
                            "question_id": 85554,
                            "option_text": "Change Spark machine to use CPUs and train a fresh pipeline to achieveobjectives.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212266,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85555,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 200,
                    "question_text_1": "Consider a Structured Streaming application running on Google Dataproc firing up every 10 seconds, consuming any number of records from Kafka available since last read, and emitting some computed answers to a NoSQL DB. Consider also that apart from the functional logic, the same application is also emitting into a file some log statements for debugging purposes meant for use by the developer of the application only in the event that something goes wrong but is otherwise not intelligible. \nAssume there is a failure in one of the Dataproc machines that results in a failure of a specific run. For anybody consuming the NoSQL DB outputs, will they see any change in output as a result of the failure at all, or will the only visible impact of failure be of slower performance for the failed-and-retried run?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249994,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "4f4adba1c66876b043a53c78106f5f1d",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "c03f8368-57ac-4226-ae38-9d296e63603a",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider a Structured Streaming application running on Google Dataproc firing up every 10 seconds, consuming any number of records from Kafka available since last read, and emitting some computed answers to a NoSQL DB. Consider also that apart from the functional logic, the same application is also emitting into a file some log statements for debugging purposes meant for use by the developer of the application only in the event that something goes wrong but is otherwise not intelligible. \nAssume there is a failure in one of the Dataproc machines that results in a failure of a specific run. For anybody consuming the NoSQL DB outputs, will they see any change in output as a result of the failure at all, or will the only visible impact of failure be of slower performance for the failed-and-retried run?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228665,
                            "question_id": 85555,
                            "option_text": "No, the failure is not visible to the consumers of NoSQL DB outputs, asStructured Streaming retries the mini-batch that failed thus taking maybe twice as much time as normal.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212267,
                            "option_image_url": null
                        },
                        {
                            "id": 228666,
                            "question_id": 85555,
                            "option_text": "No, the failure is not visible since Structured Streaming uses transactions andidempotence to achieve exactly-once processing.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212268,
                            "option_image_url": null
                        },
                        {
                            "id": 228667,
                            "question_id": 85555,
                            "option_text": "No, the failure is not visible since Structured Streaming can process the samedata in a retry resulting in the same outputs again.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212269,
                            "option_image_url": null
                        },
                        {
                            "id": 228668,
                            "question_id": 85555,
                            "option_text": "Yes, the failure is visible because the side effect of logging for debugging willbe visible as repeated entries when Structured Streaming retries the failed batch.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212270,
                            "option_image_url": null
                        },
                        {
                            "id": 228669,
                            "question_id": 85555,
                            "option_text": "Yes, the failure is visible since the logs in the backend of Spark StructuredStreaming are also logging the state of the machine.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212271,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85556,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 201,
                    "question_text_1": "Let us say we are using structured streaming for continuously reading data from Kafka and storing the results back into a Kafka topic using window function aggregates. Due to various reasons, the job did not run for 1 month. Now, we need to again continue the runs without compromising on the correctness of the results. What can you do that will take the least effort?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249995,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "db685aba7e9fd32acb3c6f3f0b233af0",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "59f73211-76d1-40d3-b522-97c58322a5bd",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Let us say we are using structured streaming for continuously reading data from Kafka and storing the results back into a Kafka topic using window function aggregates. Due to various reasons, the job did not run for 1 month. Now, we need to again continue the runs without compromising on the correctness of the results. What can you do that will take the least effort?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228670,
                            "question_id": 85556,
                            "option_text": "Code up a new batch processing job and process the 1 month of missed dataas a standalone job. Then, reactivate the structured streaming job once the latest date is caught up.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212272,
                            "option_image_url": null
                        },
                        {
                            "id": 228671,
                            "question_id": 85556,
                            "option_text": "Code up a new batch processing job and process the 1 month of missed dataas a standalone job. Then, reactivate the structured streaming job once the latest date is caught up. But copy the code over from the current structured streaming job where the read and writecommands will remain the same, but the remaining code will need to be modified, as operations on streaming dataframes are not supported on static dataframes.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212273,
                            "option_image_url": null
                        },
                        {
                            "id": 228672,
                            "question_id": 85556,
                            "option_text": "Launch the structured streaming job using starting offset as that lastsuccessfully processed before the job went on hiatus, and launch in a streaming mode with a very high time frequency of repetition. This simulates a batch execution of the same logic so as to catch up for 1 month of data processing. Once done, relaunch the structured streaming using the earlier-used configuration parameters.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212274,
                            "option_image_url": null
                        },
                        {
                            "id": 228673,
                            "question_id": 85556,
                            "option_text": "Code up a new batch processing job and process the 1 month of missed dataas a standalone job. Then, reactivate the structured streaming job once the latest date is caught up. But copy the code over from the current structured streaming job where the read and write commands need to be modified to specify that it\u2019s a batch operation. Further, the specific logic of window functions will also need to be modified since there are no time windows anymore in batch processing.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212275,
                            "option_image_url": null
                        },
                        {
                            "id": 228674,
                            "question_id": 85556,
                            "option_text": "Code up a new batch processing job and process the 1 month of missed dataas a standalone job. Then, reactivate the structured streaming job once the latest date is caught up. But copy the code over from the current structured streaming job where only the read and write commands need to be modified to specify that it's a batch operation.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212276,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85557,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 202,
                    "question_text_1": "Which one of these is not an implementation of the divide-and-conquer data processing paradigm?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249996,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "9deb207488b5253c4b9f64397e6910e7",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "bb70df21-5c47-43b6-ada0-3d6188c82f6e",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which one of these is not an implementation of the divide-and-conquer data processing paradigm?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228675,
                            "question_id": 85557,
                            "option_text": "Spark Streaming",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212277,
                            "option_image_url": null
                        },
                        {
                            "id": 228676,
                            "question_id": 85557,
                            "option_text": "Hive",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212278,
                            "option_image_url": null
                        },
                        {
                            "id": 228677,
                            "question_id": 85557,
                            "option_text": "YARN",
                            "option_image": "",
                            "score": "4.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212279,
                            "option_image_url": null
                        },
                        {
                            "id": 228678,
                            "question_id": 85557,
                            "option_text": "PySpark",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212280,
                            "option_image_url": null
                        },
                        {
                            "id": 228679,
                            "question_id": 85557,
                            "option_text": "Spark MLlib",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212281,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85558,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 203,
                    "question_text_1": "What option(s) best describe the differences between MapReduce and Spark?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249997,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "994071abb6b8201b71f2d474db4e436f",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "78485845-7a9b-4ee6-85c8-10cb43a52f81",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What option(s) best describe the differences between MapReduce and Spark?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228680,
                            "question_id": 85558,
                            "option_text": "MapReduce leverages disk heavily, while Spark optimizes for memory-basedcomputations",
                            "option_image": "",
                            "score": "1.333",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212282,
                            "option_image_url": null
                        },
                        {
                            "id": 228681,
                            "question_id": 85558,
                            "option_text": "MapReduce forces barrier synchronization after every step, while Spark usesdirected acyclic graphs to execute as many steps as possible in parallel",
                            "option_image": "",
                            "score": "1.333",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212283,
                            "option_image_url": null
                        },
                        {
                            "id": 228682,
                            "question_id": 85558,
                            "option_text": "MapReduce is comprised of both Map and Reduce steps alternatingnecessarily, while Spark allows for any combination of actions and transformations.",
                            "option_image": "",
                            "score": "1.333",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212284,
                            "option_image_url": null
                        },
                        {
                            "id": 228683,
                            "question_id": 85558,
                            "option_text": "MapReduce enables massively parallel computation, while Spark's driverprogram sequentially executes on each worker",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212285,
                            "option_image_url": null
                        },
                        {
                            "id": 228684,
                            "question_id": 85558,
                            "option_text": "MapReduce is restricted in flexibility since only Map and Reduce are possiblesteps, while Spark has a variety of Actions possible making it highly flexible",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212286,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85559,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 204,
                    "question_text_1": "You are provided with a Spark program that picks out a list of suspicious transactions. Its logic is based on both the financial value of the transaction and the geographic location of the transaction. If the financial value is higher than a threshold and the geographic location is from a set of suspected locations (provided as a 1MB file), then the program deems the transaction as suspicious. The version of the Spark program given to you is written in such a way that it pulls all the transactions from the Workers to the Driver and then applies the logic. The fraud control team that runs this program is having to constantly bother you with new threshold values since they are not able to change the Spark code themselves due to lack of technical knowledge. Which 2 changes from the list below will get you the most benefit in performance while also meeting the needs of your stakeholders?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249998,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "9f040fcbceb8b4aaaecafbf65e5f3429",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "f3167fe4-24a3-45ba-96f6-38f4957f9dbf",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are provided with a Spark program that picks out a list of suspicious transactions. Its logic is based on both the financial value of the transaction and the geographic location of the transaction. If the financial value is higher than a threshold and the geographic location is from a set of suspected locations (provided as a 1MB file), then the program deems the transaction as suspicious. The version of the Spark program given to you is written in such a way that it pulls all the transactions from the Workers to the Driver and then applies the logic. The fraud control team that runs this program is having to constantly bother you with new threshold values since they are not able to change the Spark code themselves due to lack of technical knowledge. Which 2 changes from the list below will get you the most benefit in performance while also meeting the needs of your stakeholders?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228685,
                            "question_id": 85559,
                            "option_text": "Use broadcast variables for the 1MB file",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212287,
                            "option_image_url": null
                        },
                        {
                            "id": 228686,
                            "question_id": 85559,
                            "option_text": "Hardcode threshold value as a filter condition in the Driver program so thatthe stakeholders can directly edit the Driver program without having to understand Spark",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212288,
                            "option_image_url": null
                        },
                        {
                            "id": 228687,
                            "question_id": 85559,
                            "option_text": "Reorder operations on the Driver such that geographic location is checkedfirst before filtering high value transactions",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212289,
                            "option_image_url": null
                        },
                        {
                            "id": 228688,
                            "question_id": 85559,
                            "option_text": "Hardcode threshold value as a filter condition in the Workers itself",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212290,
                            "option_image_url": null
                        },
                        {
                            "id": 228689,
                            "question_id": 85559,
                            "option_text": "Accept the threshold value as a parameter and filter using the threshold in theWorkers itself.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212291,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85560,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 205,
                    "question_text_1": null,
                    "question_image_1": "T5M2eU9WRbwsXultxzywMp5vHusIm3Y0IFEBFn1CPDp5AlZroi.png",
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531249999,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "d8ac579714f263d6b103ad1c75305dc4",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "0c7121a9-d515-41a6-b8de-e2a72b56512d",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/T5M2eU9WRbwsXultxzywMp5vHusIm3Y0IFEBFn1CPDp5AlZroi.png"
                    ],
                    "question_texts": null,
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228690,
                            "question_id": 85560,
                            "option_text": "The given programs are an example of ETL and will be bottlenecked on thestring split happening on the Python VM.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212292,
                            "option_image_url": null
                        },
                        {
                            "id": 228691,
                            "question_id": 85560,
                            "option_text": "The given programs are together an example of ELT that maximally utilizesthe Spark cluster and no further performance optimizations are required.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212293,
                            "option_image_url": null
                        },
                        {
                            "id": 228692,
                            "question_id": 85560,
                            "option_text": "By moving the string split into PySpark driver program, I can get rid of thePython program and thus make the program an ELT.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212294,
                            "option_image_url": null
                        },
                        {
                            "id": 228693,
                            "question_id": 85560,
                            "option_text": "Rewriting the string split to use native PySpark dataframe string functions willbe best suited for performance.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212295,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 85561,
                    "exam_id": 3,
                    "question_paper_id": 268,
                    "question_number": 206,
                    "question_text_1": "In a manufacturing facility, the supervisor is interested in improving efficiency of the assembly line using sensor data. While the modern machines in the facility are able to provide automated data feeds at 1Hz frequency (i.e. one data point every second) over a standard Ethernet port, the older machines have a screen showing the digital readings but no data feeds. She therefore instructs the operators to jot down readings from the screen. Which of the following are possible challenges she will have to overcome?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:56:23.000000Z",
                    "updated_at": "2025-04-17T15:56:23.000000Z",
                    "question_num_long": 6406531250000,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "efeed50d91c476eb5743a33fb4e5a7af",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "74ea2cf5-2704-464e-a62a-befbdfa2b018",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "In a manufacturing facility, the supervisor is interested in improving efficiency of the assembly line using sensor data. While the modern machines in the facility are able to provide automated data feeds at 1Hz frequency (i.e. one data point every second) over a standard Ethernet port, the older machines have a screen showing the digital readings but no data feeds. She therefore instructs the operators to jot down readings from the screen. Which of the following are possible challenges she will have to overcome?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 228694,
                            "question_id": 85561,
                            "option_text": "Ethernet is not a suitable protocol for machine data and so, for the modernmachines too, it is best to have operators read from the screen and jot down manually.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212296,
                            "option_image_url": null
                        },
                        {
                            "id": 228695,
                            "question_id": 85561,
                            "option_text": "The rate of 1Hz for data feeds is too fast for available streaming technologies.It is best to reconfigure the machines to produce at 0.5Hz or lower.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212297,
                            "option_image_url": null
                        },
                        {
                            "id": 228696,
                            "question_id": 85561,
                            "option_text": "Manual data entry is prone to error and so she cannot trust the data for theolder machines blindly.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212298,
                            "option_image_url": null
                        },
                        {
                            "id": 228697,
                            "question_id": 85561,
                            "option_text": "The rate at which operators are able to jot down the readings will not matchthe 1Hz frequency of automated data collection. So, she has to reconfigure the modern machines to produce at 0.1Hz or lower.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212299,
                            "option_image_url": null
                        },
                        {
                            "id": 228698,
                            "question_id": 85561,
                            "option_text": "The number of operators doing only data readings would be significantlyhigher if she wants to collect data at a reasonable frequency for being useful to increase efficiency.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:56:23.000000Z",
                            "updated_at": "2025-04-17T15:56:23.000000Z",
                            "option_number": 6406534212300,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                }
            ]
        },
        "summary": "Here\u2019s a structured summary of the **core topics, concepts, principles, and equations** essential for the **\"Introduction to Big Data\"** exam, along with **Mermaid knowledge graphs** to visualize relationships.\n\n---\n\n---\n\n### **Core Topics & Concepts**\n#### **1. Big Data Fundamentals**\n   - **Definition**: High-volume, high-velocity, and/or high-variety data requiring distributed processing.\n   - **4Vs**: Volume, Velocity, Variety, Veracity.\n   - **Batch vs. Streaming**:\n     - **Batch**: Process large datasets in fixed time windows (e.g., MapReduce, Hive).\n     - **Streaming**: Process data in real-time (e.g., Spark Streaming, Kafka).\n   - **Scalability**: Horizontal (adding machines) vs. vertical (upgrading hardware).\n\n#### **2. Distributed Computing Frameworks**\n   - **MapReduce**:\n     - **Phases**: Map \u2192 Shuffle \u2192 Reduce.\n     - **Limitations**: Disk-heavy, rigid structure (only Map/Reduce steps).\n     - **Fault Tolerance**: Re-executes failed tasks.\n   - **Apache Spark**:\n     - **Advantages**: In-memory processing, DAG (Directed Acyclic Graph) optimization, lazy evaluation.\n     - **Components**:\n       - **Spark Core**: RDDs (Resilient Distributed Datasets).\n       - **Spark SQL**: DataFrames/Datasets (optimized with Catalyst optimizer).\n       - **Spark Streaming**: Micro-batch processing (e.g., `window` functions).\n       - **MLlib**: Distributed machine learning.\n     - **Fault Tolerance**: Lineage-based recovery (recomputes lost partitions).\n   - **YARN**: Resource manager (not a processing framework).\n\n#### **3. Data Processing Paradigms**\n   - **Divide-and-Conquer**: Split data into chunks, process in parallel (e.g., MapReduce, Spark).\n   - **ETL vs. ELT**:\n     - **ETL**: Extract \u2192 Transform \u2192 Load (traditional, bottlenecks on transformation).\n     - **ELT**: Extract \u2192 Load \u2192 Transform (leverages distributed systems like Spark).\n   - **Hot Potato Principle**:\n     - Minimize work per step for scalability and recoverability.\n     - Avoid storing intermediate results (use message queues like Kafka).\n\n#### **4. Storage & File Formats**\n   - **HDFS**: Distributed file system (high latency, high throughput).\n   - **Cloud Storage**: GCS (Google Cloud Storage), S3 (AWS).\n   - **File Formats**:\n     - **CSV/JSON**: Human-readable but slow.\n     - **Parquet/ORC**: Columnar storage (faster analytics).\n     - **Avro**: Row-based, schema evolution support.\n\n#### **5. Streaming Systems**\n   - **Kafka**:\n     - **Topics/Partitions**: Data is divided into partitions for parallel consumption.\n     - **Offsets**: Track progress of consumers.\n     - **Scaling**: Add brokers/partitions (but rebalancing has overhead).\n   - **Spark Structured Streaming**:\n     - **Exactly-Once Semantics**: Idempotent sinks + checkpointing.\n     - **Watermarking**: Handle late data in windowed aggregations.\n   - **Failure Handling**:\n     - **At-Least-Once**: Retries may cause duplicates (e.g., Kafka + Spark without idempotence).\n     - **At-Most-Once**: No retries (may lose data).\n     - **Exactly-Once**: Transactions + offset management.\n\n#### **6. Cloud-Native Design**\n   - ** Principles**:\n     - **Scalability**: Auto-scaling (e.g., Dataproc, Cloud Functions).\n     - **Resilience**: Self-healing (e.g., Kubernetes, Spark retries).\n     - **Observability**: Logging (e.g., Stackdriver), metrics (e.g., Prometheus).\n     - **Manageability**: Infrastructure-as-code (e.g., Terraform).\n   - **Serverless**:\n     - **Cloud Functions**: Event-driven, no VM management.\n     - **Dataflow**: Managed Spark-like service (auto-scaling).\n\n#### **7. Performance Optimization**\n   - **Broadcast Variables**: Cache small datasets (e.g., 1MB lookup tables) on workers.\n   - **Partitioning**: Avoid skew (e.g., `repartition` in Spark).\n   - **Caching**: Persist intermediate RDDs/DataFrames in memory.\n   - **Predicate Pushdown**: Filter data early (e.g., in Spark SQL).\n   - **Model Optimization**:\n     - **Quantization**: Reduce model size (e.g., for edge devices).\n     - **Batch Processing**: Replace streaming with batch if latency isn\u2019t critical.\n\n#### **8. Time Handling in Data Pipelines**\n   - **Event Time**: When the event occurred (e.g., timestamp in data).\n   - **Processing Time**: When the system processes the event.\n   - **Business Time**: Align with organizational schedules (e.g., weekly sales).\n   - **Time Zones**: Critical for global operations (e.g., relocating HQ).\n\n#### **9. Machine Learning in Big Data**\n   - **Distributed Training**: Spark MLlib, TensorFlow on GPUs.\n   - **Model Serving**:\n     - **Batch**: Precompute predictions (e.g., daily).\n     - **Real-Time**: Serve via APIs (e.g., TensorFlow Serving).\n   - **Trade-offs**:\n     - **Latency vs. Cost**: GPUs (fast, expensive) vs. CPUs (slow, cheap).\n     - **Recall vs. Efficiency**: Smaller models may suffice for batch use cases.\n\n#### **10. Data Quality & Challenges**\n   - **Manual Data Entry**: Errors, low frequency (e.g., operators jotting down readings).\n   - **Automated Feeds**: High frequency but may need throttling (e.g., 1Hz sensor data).\n   - **Schema Evolution**: Handle changes in data structure (e.g., Avro schemas).\n\n---\n\n---\n\n### **Key Equations (Mathematics/Performance)**\n1. **MapReduce Word Count (Pseudocode)**:\n   ```python\n   # Mapper\n   def map(key, value):\n       for word in value.split():\n           emit(word, 1)\n\n   # Reducer\n   def reduce(key, values):\n       emit(key, sum(values))\n   ```\n\n2. **Spark RDD Lineage**:\n   - Recomputes lost partitions using transformations (e.g., `map`, `filter`).\n\n3. **Kafka Throughput**:\n   - **Partitions per Topic** = Max parallel consumers.\n   - **Throughput** \u221d (Number of Brokers \u00d7 Disk I/O).\n\n4. **Spark Streaming Batch Interval**:\n   - **Latency** \u2248 Batch interval (e.g., 10s).\n   - **Throughput** \u221d (Cluster Size / Batch Interval).\n\n5. **Data Skew Mitigation**:\n   - **Salting**: Add random prefix to keys to distribute load:\n     ```python\n     df = df.withColumn(\"salted_key\", concat(col(\"key\"), lit(\"_\"), (rand() * 10).cast(\"int\")))\n     ```\n\n6. **Broadcast Join Threshold**:\n   - Use broadcast if dataset size < `spark.sql.autoBroadcastJoinThreshold` (default: 10MB).\n\n7. **Window Aggregations (Spark SQL)**:\n   ```sql\n   SELECT window(startTime, \"10 minutes\"), deviceId, AVG(value)\n   FROM sensorData\n   GROUP BY window(startTime, \"10 minutes\"), deviceId\n   ```\n\n8. **Exactly-Once Semantics (Spark + Kafka)**:\n   - Requires:\n     - Idempotent sink (e.g., transactional DB).\n     - Checkpointing offsets.\n\n---\n\n---\n\n### **Mermaid Knowledge Graphs**\n#### **Graph 1: Big Data Ecosystem**\n```mermaid\ngraph TD\n    A[Big Data] --> B[Batch Processing]\n    A --> C[Stream Processing]\n    B --> D[MapReduce]\n    B --> E[Hive]\n    C --> F[Spark Streaming]\n    C --> G[Kafka]\n    D --> H[HDFS]\n    E --> H\n    F --> I[Spark Core]\n    G --> J[Pub/Sub]\n    I --> K[RDDs]\n    I --> L[DataFrames]\n    K --> M[Transformations]\n    L --> M\n    M --> N[Actions]\n```\n\n#### **Graph 2: Spark Components**\n```mermaid\ngraph TD\n    A[Spark] --> B[Spark Core]\n    A --> C[Spark SQL]\n    A --> D[Spark Streaming]\n    A --> E[MLlib]\n    B --> F[RDDs]\n    C --> G[DataFrames]\n    D --> H[Micro-Batches]\n    E --> I[Distributed ML]\n    F --> J[Lazy Evaluation]\n    G --> K[Catalyst Optimizer]\n    H --> L[Watermarking]\n```\n\n#### **Graph 3: Kafka Architecture**\n```mermaid\ngraph TD\n    A[Kafka Cluster] --> B[Broker 1]\n    A --> C[Broker 2]\n    B --> D[Topic 1]\n    B --> E[Topic 2]\n    D --> F[Partition 1]\n    D --> G[Partition 2]\n    F --> H[Offset 0]\n    F --> I[Offset 1]\n    G --> J[Offset 0]\n    C --> D\n    E --> K[Consumer Group]\n    K --> L[Consumer 1]\n    K --> M[Consumer 2]\n```\n\n#### **Graph 4: Cloud-Native Design**\n```mermaid\ngraph TD\n    A[Cloud-Native App] --> B[Scalability]\n    A --> C[Resilience]\n    A --> D[Observability]\n    A --> E[Manageability]\n    B --> F[Auto-Scaling]\n    C --> G[Self-Healing]\n    D --> H[Logging]\n    D --> I[Metrics]\n    E --> J[Infrastructure-as-Code]\n    F --> K[Serverless]\n    G --> L[Retries]\n    K --> M[Cloud Functions]\n    K --> N[Dataflow]\n```\n\n#### **Graph 5: ETL vs. ELT**\n```mermaid\ngraph LR\n    A[ETL] --> B[Extract]\n    B --> C[Transform]\n    C --> D[Load]\n    E[ELT] --> F[Extract]\n    F --> G[Load]\n    G --> H[Transform]\n    C --> I[Bottleneck]\n    H --> J[Distributed]\n```\n\n#### **Graph 6: Streaming Failure Modes**\n```mermaid\ngraph TD\n    A[Streaming Job] --> B[Kafka Source]\n    A --> C[Spark Processing]\n    A --> D[Sink]\n    B --> E[Broker Failure]\n    C --> F[Worker Failure]\n    E --> G[At-Least-Once]\n    F --> G\n    G --> H[Duplicate Data]\n    D --> I[Idempotent Sink]\n    I --> J[Exactly-Once]\n```\n\n---\n\n---\n\n### **Exam Tips**\n1. **MapReduce vs. Spark**:\n   - MapReduce: Disk-based, rigid (Map \u2192 Reduce).\n   - Spark: Memory-based, flexible (DAG, lazy evaluation).\n\n2. **Hot Potato Principle**:\n   - Minimize work per step; avoid storing intermediates (use Kafka).\n\n3. **Spark Optimizations**:\n   - Use `broadcast` for small datasets.\n   - Cache intermediate DataFrames (`df.cache()`).\n   - Partition data evenly (avoid skew).\n\n4. **Kafka Scaling**:\n   - Add brokers/partitions (but rebalancing has overhead).\n   - Monitor consumer lag.\n\n5. **Cloud-Native**:\n   - Serverless (e.g., Cloud Functions) reduces ops overhead.\n   - Managed services (e.g., Dataproc, Dataflow) auto-scale.\n\n6. **Time Handling**:\n   - Align processing with **business time** (e.g., weekly sales).\n   - Use **event time** for accuracy (not processing time).\n\n7. **Failure Recovery**:\n   - Spark: Recomputes from lineage (RDDs) or checkpoints (Structured Streaming).\n   - Kafka: Consumer offsets track progress.\n\n8. **Model Deployment**:\n   - Batch: Precompute predictions (cheaper).\n   - Real-Time: Serve via API (higher cost).\n\n9. **Data Quality**:\n   - Manual entry: Error-prone, low frequency.\n   - Automated: High frequency but may need throttling.\n\n10. **Cost vs. Performance**:\n    - GPUs: Faster but expensive (use for low-latency).\n    - CPUs: Cheaper but slower (use for batch).",
        "total_score": "50.00"
    },
    "url": "/question-paper/practise/94/9e03d1fe-3a5",
    "version": "ee3d5d44299e610bd137ea6200db5ac2"
}