{
    "component": "Quizpractise/PractiseQuestionPaper",
    "props": {
        "errors": {},
        "auth": {
            "user": null
        },
        "flash": {
            "error": []
        },
        "banner": null,
        "file_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "file_do_url": "https://saram.blr1.cdn.digitaloceanspaces.com",
        "question_paper": {
            "id": 248,
            "group_id": 27,
            "exam_id": 3,
            "total_score": "1.00",
            "duration": 4,
            "created_at": "2025-04-17T15:51:51.000000Z",
            "updated_at": "2025-04-17T15:51:51.000000Z",
            "question_paper_name": "IIT M DEGREE AN EXAM QDB4 13 Apr 2025",
            "question_paper_description": "2025 Apr13: IIT M AN EXAM QDB4",
            "uuid": "fb4337ae-d12",
            "year": 2025,
            "is_new": 0,
            "exam": {
                "id": 3,
                "exam_name": "End Term Quiz",
                "created_at": "2024-10-16T08:08:51.000000Z",
                "updated_at": "2024-10-16T08:08:51.000000Z",
                "uuid": "7a6ff569-f50c-40e7-a08b-f5c334392600",
                "en_id": "eyJpdiI6Ii8ydjBlcHY4bGJGUkx4cWUvdkZ0L2c9PSIsInZhbHVlIjoiWTVyU3hFQUR4REtOTGx1cFhoQ0RMQT09IiwibWFjIjoiZjYzZWNhZmI1ZTcxMzI2ZTEwOTQwNWVhNzg3ZTkxOTBhYTg5NGY5YjUwYTMzZTU0NGQ1ZTA3MDk1Nzk5OWFhNCIsInRhZyI6IiJ9"
            },
            "questions": [
                {
                    "id": 77730,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 220,
                    "question_text_1": "THIS IS QUESTION PAPER FOR THE SUBJECT <b>\"DEGREE LEVEL : INTRODUCTION TO BIG DATA </b><b>(COMPUTER BASED EXAM)\"</b><b> </b><b> </b><b>ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? </b><b>CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN. </b><b> </b><b>(IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS </b><b>REGISTERED BY YOU)</b>",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "0.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251737,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "dad8863123255a4f96ac0bc3fdae2820",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "64db2588-4481-4100-bea9-665aaabcaf00",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "THIS IS QUESTION PAPER FOR THE SUBJECT <b>\"DEGREE LEVEL : INTRODUCTION TO BIG DATA </b><b>(COMPUTER BASED EXAM)\"</b><b> </b><b> </b><b>ARE YOU SURE YOU HAVE TO WRITE EXAM FOR THIS SUBJECT? </b><b>CROSS CHECK YOUR HALL TICKET TO CONFIRM THE SUBJECTS TO BE WRITTEN. </b><b> </b><b>(IF IT IS NOT THE CORRECT SUBJECT, PLS CHECK THE SECTION AT THE TOP FOR THE SUBJECTS </b><b>REGISTERED BY YOU)</b>"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208547,
                            "question_id": 77730,
                            "option_text": "YES",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217074,
                            "option_image_url": null
                        },
                        {
                            "id": 208548,
                            "question_id": 77730,
                            "option_text": "NO",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217075,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77731,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 221,
                    "question_text_1": "At the onset of every festive season, there is a surge in quick commerce orders (i.e. orders delivered within 15 minutes) on Swiggy. The supply chain head at Swiggy for a city is interested in a real-time view of the inventory of her dark stores (i.e. stores without nameboards where the supplies are kept and used to fulfil app orders). She wants to see this be presented in a monitor mounted in her office wall that refreshes with the latest info on a city map every 1 minute. Along with the info, there also needs to be the current time so that she gets a visual confirmation that this is the latest data. This dashboard allows her to plan for new orders of specific supplies that she is running out of so that no customer is left unsatisfied. What solution option below best solves for the need?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251738,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "d46d14acedfbf84d75e4e6a9986872df",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "33e7932f-eb06-48d2-9457-d50a267e3001",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "At the onset of every festive season, there is a surge in quick commerce orders (i.e. orders delivered within 15 minutes) on Swiggy. The supply chain head at Swiggy for a city is interested in a real-time view of the inventory of her dark stores (i.e. stores without nameboards where the supplies are kept and used to fulfil app orders). She wants to see this be presented in a monitor mounted in her office wall that refreshes with the latest info on a city map every 1 minute. Along with the info, there also needs to be the current time so that she gets a visual confirmation that this is the latest data. This dashboard allows her to plan for new orders of specific supplies that she is running out of so that no customer is left unsatisfied. What solution option below best solves for the need?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208549,
                            "question_id": 77731,
                            "option_text": "Route a copy of every quick commerce item ordered to a Kafka topic, useSpark Structured Streaming to continuously read from this topic and update the current counts of supplies, and emit using output mode \u201cUpdate\u201d.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217076,
                            "option_image_url": null
                        },
                        {
                            "id": 208550,
                            "question_id": 77731,
                            "option_text": "Route a copy of every quick commerce item ordered to a Kafka topic, useSpark Structured Streaming to periodically read from this topic every 1 minute and update the current counts of supplies, and emit all aggregates using the output mode \u201cComplete\u201d.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217077,
                            "option_image_url": null
                        },
                        {
                            "id": 208551,
                            "question_id": 77731,
                            "option_text": "Route a copy of every quick commerce item ordered to a Kafka topic, useSpark Structured Streaming to periodically read from this topic every 1 minute and count the supplies ordered in that batch, and emit only all aggregates in that batch using the output mode \u201cAppend\u201d.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217078,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77732,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 222,
                    "question_text_1": "You are given a Spark program that runs on a Google Dataproc cluster on a daily schedule starting execution at 7AM running typically for 2 hours, to produce as output the total amount spent on purchases made by every customer the previous day. The input data is coming into GCS every minute from a variety of sources as standalone files. Therefore, the business leader now feels that having to wait till 9AM the next day is no longer acceptable and instead ideally wants purchase information for each customer at least every 5 minutes during the day itself. What\u2019s more, she wants to be able to change this time configuration later without involving you.  Which amongst the below represents the best option to achieve the above?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251739,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "124a1b9e8b067e3ae9dd6921029b8abf",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "f0373f99-6f78-4d73-9da2-9549e9c72649",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are given a Spark program that runs on a Google Dataproc cluster on a daily schedule starting execution at 7AM running typically for 2 hours, to produce as output the total amount spent on purchases made by every customer the previous day. The input data is coming into GCS every minute from a variety of sources as standalone files. Therefore, the business leader now feels that having to wait till 9AM the next day is no longer acceptable and instead ideally wants purchase information for each customer at least every 5 minutes during the day itself. What\u2019s more, she wants to be able to change this time configuration later without involving you.  Which amongst the below represents the best option to achieve the above?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208552,
                            "question_id": 77732,
                            "option_text": "Change the schedule to run every 5 minutes, no other change required.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217079,
                            "option_image_url": null
                        },
                        {
                            "id": 208553,
                            "question_id": 77732,
                            "option_text": "Change the code to leverage Spark Streaming with streaming window as \u201c5minutes\u201d, & let her manage the execution of the code on Google Dataproc",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217080,
                            "option_image_url": null
                        },
                        {
                            "id": 208554,
                            "question_id": 77732,
                            "option_text": "Change the code to leverage Spark Streaming with streaming window as \u201c5minutes\u201d, go from using Dataproc to Dataflow, & let her manage the execution of the code on Dataflow",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217081,
                            "option_image_url": null
                        },
                        {
                            "id": 208555,
                            "question_id": 77732,
                            "option_text": "Write a Cloud Function to move all incoming per-minute standalone files fromGCS to Pub/Sub, change the code to leverage Spark Streaming with streaming window as \u201c5 mins\u201d, convert from Dataproc to Dataflow, point source to Pub/Sub, & let her manage the execution of the code on Dataflow",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217082,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77733,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 223,
                    "question_text_1": "A Spark batch application is required to execute within 1 minute. However, a run takes 10+ minutes on an average. The code in a nutshell looks as follows: \n",
                    "question_image_1": "dNmKvEFhUHyGkkbRqnmObbLPjjwTCzwnuOXHZytybsvJ3c1Q8c.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251740,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "fdc31464132b417fab7f3be601445eb5",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": "A Spark batch application is required to execute within 1 minute. However, a run takes 10+ minutes on an average. The code in a nutshell looks as follows: \n  Your goal is to optimize the code to bring down execution to within 1 minute. Which of the following represent options that will help in this mission?",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "eb6ae01b-2672-448d-9b8f-db265ce81fa8",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/dNmKvEFhUHyGkkbRqnmObbLPjjwTCzwnuOXHZytybsvJ3c1Q8c.png"
                    ],
                    "question_texts": [
                        "A Spark batch application is required to execute within 1 minute. However, a run takes 10+ minutes on an average. The code in a nutshell looks as follows: \n",
                        "A Spark batch application is required to execute within 1 minute. However, a run takes 10+ minutes on an average. The code in a nutshell looks as follows: \n  Your goal is to optimize the code to bring down execution to within 1 minute. Which of the following represent options that will help in this mission?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208556,
                            "question_id": 77733,
                            "option_text": "",
                            "option_image": "aRljgzvI4n9qhDtU41nmXVkX7c6UPMq1edrA9EY77UtUYp71Lj.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217083,
                            "option_image_url": "app/option_images/aRljgzvI4n9qhDtU41nmXVkX7c6UPMq1edrA9EY77UtUYp71Lj.png"
                        },
                        {
                            "id": 208557,
                            "question_id": 77733,
                            "option_text": "",
                            "option_image": "SeZUrjAyRFyUNevifDInkmBD5ICTW7wLP5duSLGLACA86S0Vyi.png",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217084,
                            "option_image_url": "app/option_images/SeZUrjAyRFyUNevifDInkmBD5ICTW7wLP5duSLGLACA86S0Vyi.png"
                        },
                        {
                            "id": 208558,
                            "question_id": 77733,
                            "option_text": "",
                            "option_image": "ccrklHEKfeKjLimrGBLNPNSb3bLmt8PJGNZJb7wb4BVrdN9wf4.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217085,
                            "option_image_url": "app/option_images/ccrklHEKfeKjLimrGBLNPNSb3bLmt8PJGNZJb7wb4BVrdN9wf4.png"
                        },
                        {
                            "id": 208559,
                            "question_id": 77733,
                            "option_text": "",
                            "option_image": "J0Sk27qtVTuUfpy2DWpX9yzqUv2RBhnK2zzQHzD143RshCKnBA.png",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217086,
                            "option_image_url": "app/option_images/J0Sk27qtVTuUfpy2DWpX9yzqUv2RBhnK2zzQHzD143RshCKnBA.png"
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77734,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 224,
                    "question_text_1": "You are asked to build a recommendation engine for an offline store at the checkout counter, using a combination of cloud and point-of-sale (PoS) terminal resources. Consider the following pipeline choices for effecting the same outcome, where Kubernetes is an open-source system for automating deployment, scaling and management of containerized applications, Google Datastore and HBase are both highly-scalable NoSQL database systems for interactive, real-time applications, and Google Vertex AI is a hosted platform that lets you train and deploy ML models \n(i) Recommender model training on Python VM on GCP; Data publisher on VM in PoS \u2192 Kafka VM on GCP \u2192 Spark Streaming on Hadoop VMs on GCP -> Recommender hosted on Python VM on GCP \u2192 Relay recommendation to PoS (ii) Recommender model training on Vertex AI; Data publisher on VM in PoS \u2192 Kafka VM on GCP \u2192 Dataflow -> Recommender hosted on Vertex AI \u2192 Relay recommendation to PoS (iii) Recommender model training on Python VM on GCP; Data publisher on VM in PoS \u2192 Pub/Sub \u2192 Spark Streaming on Google Dataproc \u2192 Recommender hosted on Python VM on GCP -> Relay recommendation to PoS (iv) Recommender model training on Python VM on GCP; Data publisher on VM in PoS \u2192 Pub/Sub \u2192 Dataflow \u2192 Recommender hosted on Python VM on GCP \u2192 Relay recommendation to PoS (v) Recommender model training on Python VM on GCP; Data publisher on VM in PoS \u2192 Pub/Sub \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 Recommender hosted on Python VM on GCP \u2192 Relay recommendation to PoS \nWhich option below represents the correct order of pipeline options that has the \u201cmost IaaS\u201d entry to the left and the \u201cmost PaaS\u201d entry to the right?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251741,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "902fec90397afc0e2341c3f2a138c7ba",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "a13b55a8-b8ee-42fc-9db2-35e134cb6f3a",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are asked to build a recommendation engine for an offline store at the checkout counter, using a combination of cloud and point-of-sale (PoS) terminal resources. Consider the following pipeline choices for effecting the same outcome, where Kubernetes is an open-source system for automating deployment, scaling and management of containerized applications, Google Datastore and HBase are both highly-scalable NoSQL database systems for interactive, real-time applications, and Google Vertex AI is a hosted platform that lets you train and deploy ML models \n(i) Recommender model training on Python VM on GCP; Data publisher on VM in PoS \u2192 Kafka VM on GCP \u2192 Spark Streaming on Hadoop VMs on GCP -> Recommender hosted on Python VM on GCP \u2192 Relay recommendation to PoS (ii) Recommender model training on Vertex AI; Data publisher on VM in PoS \u2192 Kafka VM on GCP \u2192 Dataflow -> Recommender hosted on Vertex AI \u2192 Relay recommendation to PoS (iii) Recommender model training on Python VM on GCP; Data publisher on VM in PoS \u2192 Pub/Sub \u2192 Spark Streaming on Google Dataproc \u2192 Recommender hosted on Python VM on GCP -> Relay recommendation to PoS (iv) Recommender model training on Python VM on GCP; Data publisher on VM in PoS \u2192 Pub/Sub \u2192 Dataflow \u2192 Recommender hosted on Python VM on GCP \u2192 Relay recommendation to PoS (v) Recommender model training on Python VM on GCP; Data publisher on VM in PoS \u2192 Pub/Sub \u2192 Spark Streaming on Hadoop VMs on GCP \u2192 Recommender hosted on Python VM on GCP \u2192 Relay recommendation to PoS \nWhich option below represents the correct order of pipeline options that has the \u201cmost IaaS\u201d entry to the left and the \u201cmost PaaS\u201d entry to the right?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208560,
                            "question_id": 77734,
                            "option_text": "(i), (v), (iii), (iv), (ii)",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217087,
                            "option_image_url": null
                        },
                        {
                            "id": 208561,
                            "question_id": 77734,
                            "option_text": "(i), (ii), (iii), (iv), (v)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217088,
                            "option_image_url": null
                        },
                        {
                            "id": 208562,
                            "question_id": 77734,
                            "option_text": "(ii), (iii), (i), (iv), (v)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217089,
                            "option_image_url": null
                        },
                        {
                            "id": 208563,
                            "question_id": 77734,
                            "option_text": "(v), (iii), (i), (iv), (ii)",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217090,
                            "option_image_url": null
                        },
                        {
                            "id": 208564,
                            "question_id": 77734,
                            "option_text": "All are equally PaaS / IaaS",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217091,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77735,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 225,
                    "question_text_1": "Consider a file \u201cdata.bin\u201d which is formatted as follows: every data record has 10 key-value pairs of the format \u201ckey,value\u201d, with each pair separated by a comma, where the \u201ckey\u201d is the name of a field and the \u201cvalue\u201d is the value for that field in that record. Every data record occurs in its own line. You are asked to write a data processing script using Python that scales with big data. Which of the following represents your approach?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251742,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "9892fb1c169befb34b6d3ed86991e62e",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "dd6273d0-d9b9-4269-a932-a54d0a2ea3fd",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider a file \u201cdata.bin\u201d which is formatted as follows: every data record has 10 key-value pairs of the format \u201ckey,value\u201d, with each pair separated by a comma, where the \u201ckey\u201d is the name of a field and the \u201cvalue\u201d is the value for that field in that record. Every data record occurs in its own line. You are asked to write a data processing script using Python that scales with big data. Which of the following represents your approach?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208565,
                            "question_id": 77735,
                            "option_text": "Since data.bin is compliant with the RFC 4180, use PySpark\u2019s read_csv() to readthe data as is.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217092,
                            "option_image_url": null
                        },
                        {
                            "id": 208566,
                            "question_id": 77735,
                            "option_text": "Rename the file data.bin to data.csv to make it compliant with RFC 4180 andthen use PySpark\u2019s read_csv() to read the data",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217093,
                            "option_image_url": null
                        },
                        {
                            "id": 208567,
                            "question_id": 77735,
                            "option_text": "The problem cannot be solved since the file cannot be converted to a validformat for reading consistently without additional information",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217094,
                            "option_image_url": null
                        },
                        {
                            "id": 208568,
                            "question_id": 77735,
                            "option_text": "Write PySpark code to read all lines in data.bin, use string split on \u201c,\u201d asdelimiter, and then collect all column names and corresponding values into a RDD for further processing",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217095,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77736,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 226,
                    "question_text_1": "You join the data engineering team at a company that has good big data expertise already. Your first assignment is to convert Spark code written by previous engineers that used Spark 1.0 to use newer features and best practices. None of those previous engineers work in the company anymore, and nobody in the present team can tell you what data files available in the company data lake today correspond to which part of the processing done then. Which of the following most accurately captures your ability to do the task at hand?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251743,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "dfdc07dc86620df3652920b960278df0",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "55bfd6d0-171f-4d46-8ac8-f9da37f96e06",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You join the data engineering team at a company that has good big data expertise already. Your first assignment is to convert Spark code written by previous engineers that used Spark 1.0 to use newer features and best practices. None of those previous engineers work in the company anymore, and nobody in the present team can tell you what data files available in the company data lake today correspond to which part of the processing done then. Which of the following most accurately captures your ability to do the task at hand?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208569,
                            "question_id": 77736,
                            "option_text": "The task is impossible since Spark 1.0 used RDDs which cannot be convertedto Dataframes without the correct data schema.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217096,
                            "option_image_url": null
                        },
                        {
                            "id": 208570,
                            "question_id": 77736,
                            "option_text": "The task is impossible since Spark core and syntax has changed so completelythat those data files it processed then can no longer be processed by current Spark versions.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217097,
                            "option_image_url": null
                        },
                        {
                            "id": 208571,
                            "question_id": 77736,
                            "option_text": "The task is possible with multiple trial-and-error schema experimentsmatching data files with the older code.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217098,
                            "option_image_url": null
                        },
                        {
                            "id": 208572,
                            "question_id": 77736,
                            "option_text": "The task is possible since Spark has backwards compability.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217099,
                            "option_image_url": null
                        },
                        {
                            "id": 208573,
                            "question_id": 77736,
                            "option_text": "None of these,since not enough information is available.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217100,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77737,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 227,
                    "question_text_1": "What happens when a Spark Structured Streaming pipeline operating with Kafka as source and console output as target is subject to a failure of a machine in either of the Kafka cluster or the Spark cluster?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251744,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "a0a181c4b789d192c92275b626301df4",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "77b15ea0-26c5-4a3c-abd3-b1d31a544bed",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What happens when a Spark Structured Streaming pipeline operating with Kafka as source and console output as target is subject to a failure of a machine in either of the Kafka cluster or the Spark cluster?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208574,
                            "question_id": 77737,
                            "option_text": "Failure of a machine in the Kafka cluster will result in an Exception in the Sparkpipeline which will then fail and halt.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217101,
                            "option_image_url": null
                        },
                        {
                            "id": 208575,
                            "question_id": 77737,
                            "option_text": "The pipeline will be restarted automatically by Spark which is able to pick upthe exact data from Kafka which was being processed at the time of error, resulting in exactly-once semantics.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217102,
                            "option_image_url": null
                        },
                        {
                            "id": 208576,
                            "question_id": 77737,
                            "option_text": "The Spark pipeline will not be able to start again from previously committedoffset by restarting itself, resulting in at least-once processing semantics",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217103,
                            "option_image_url": null
                        },
                        {
                            "id": 208577,
                            "question_id": 77737,
                            "option_text": "Irrespective of whatever machine fails, Spark will throw an error and halt.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217104,
                            "option_image_url": null
                        },
                        {
                            "id": 208578,
                            "question_id": 77737,
                            "option_text": "Data that is being processed will not be processed again, resulting in at most-once semantics.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217105,
                            "option_image_url": null
                        },
                        {
                            "id": 208579,
                            "question_id": 77737,
                            "option_text": "Spark will be able to pick up data from Kafka from exactly that offset whichfailed but may produce duplicate output on the target resulting in at least once semantics for the consumer of the output.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217106,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77738,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 228,
                    "question_text_1": "A big data streaming application that uses Kafka as source is observed to be really lagging behind currently live data. The Kafka cluster has 2 broker nodes and this application is reading from 1 topic that has 10 partitions. On closer investigation, it was found that Kafka is not scaling to the velocity of input data coming in. What can you first try to do to scale Kafka further while incurring minimal overall costs?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251745,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "d72aefb4d554569c3bf95ad190af6946",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "1e7667e2-cc49-4752-afcd-230915ceb2aa",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "A big data streaming application that uses Kafka as source is observed to be really lagging behind currently live data. The Kafka cluster has 2 broker nodes and this application is reading from 1 topic that has 10 partitions. On closer investigation, it was found that Kafka is not scaling to the velocity of input data coming in. What can you first try to do to scale Kafka further while incurring minimal overall costs?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208580,
                            "question_id": 77738,
                            "option_text": "Add disks to each broker in the cluster, and disks are the cheapest computercomponent",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217107,
                            "option_image_url": null
                        },
                        {
                            "id": 208581,
                            "question_id": 77738,
                            "option_text": "Add new brokers to the cluster, even though this is more expensive than theother options this is the only foolproof way to scale.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217108,
                            "option_image_url": null
                        },
                        {
                            "id": 208582,
                            "question_id": 77738,
                            "option_text": "Create more topics and change input application to reroute data to all topicsto be able to spread input data better. This is nearly the least expensive since only developer effort is required to change application.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217109,
                            "option_image_url": null
                        },
                        {
                            "id": 208583,
                            "question_id": 77738,
                            "option_text": "Double the number of partitions for this single topic to be able to spread inputdata better. This is the least expensive since only administrator effort is required without changing application.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217110,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77739,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 229,
                    "question_text_1": "A company with headquarters (HQ) in the Middle East operates on a Sunday-Thursday weekday schedule with Friday & Saturday as weekend days. It computes end of week revenue numbers using an ETL pipeline by first computing sales for each day at 1AM local time of the next day, and then summing up the weekly sales every Sunday early morning at 3AM local time. This number gets reported to leadership every Sunday morning 9AM local time. Therefore, the ETL pipeline is scheduled to run every Sunday morning at 8AM local time. As a result of management change, the company has decided to shutdown its business in the Middle East and relocate all of its operations and business HQ to India. Which of the following changes will need to be done to its ETL pipeline to ensure the correct output continues to be produced?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251746,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "704389885e9ae6de5a437481f67f9fa4",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "01078afb-e8eb-4726-9dfb-3775cbe9f9bd",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "A company with headquarters (HQ) in the Middle East operates on a Sunday-Thursday weekday schedule with Friday & Saturday as weekend days. It computes end of week revenue numbers using an ETL pipeline by first computing sales for each day at 1AM local time of the next day, and then summing up the weekly sales every Sunday early morning at 3AM local time. This number gets reported to leadership every Sunday morning 9AM local time. Therefore, the ETL pipeline is scheduled to run every Sunday morning at 8AM local time. As a result of management change, the company has decided to shutdown its business in the Middle East and relocate all of its operations and business HQ to India. Which of the following changes will need to be done to its ETL pipeline to ensure the correct output continues to be produced?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208584,
                            "question_id": 77739,
                            "option_text": "The business time for the final weekly sum operation needs to be changed tothat of Monday 3AM India time instead of Sunday 3AM Middle East time.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217111,
                            "option_image_url": null
                        },
                        {
                            "id": 208585,
                            "question_id": 77739,
                            "option_text": "There is zero change needed since neither event time nor business time ischanging whereas only the operational time is changing.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217112,
                            "option_image_url": null
                        },
                        {
                            "id": 208586,
                            "question_id": 77739,
                            "option_text": "Since time zone has changed as well as week definition too, the definition ofbusiness time has changed. So, the ETL has to be rewritten entirely.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217113,
                            "option_image_url": null
                        },
                        {
                            "id": 208587,
                            "question_id": 77739,
                            "option_text": "Nothing needs to change since daily sales is available at 1AM Middle East timewhich is anyway behind India time and so the numbers will be available before leadership comes in at 9AM.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217114,
                            "option_image_url": null
                        },
                        {
                            "id": 208588,
                            "question_id": 77739,
                            "option_text": "Event time has changed since the event of week ending has changed indefinition, and so the ETL needs to be changed to consider the new event in the data.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217115,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77740,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 230,
                    "question_text_1": "You are appointed as a Data Engineer in a company that has a legacy reporting application written in Java which suffers from performance problems. The reporting application plots dashboards with daily refresh of key business indicators to help management take the best decisions. The application reads data directly from the source database of MongoDB, aggregates using simple counts and shows them visually in a UI. The performance problem of this application comes because the source database is at times overloaded and therefore the dashboard is not able to retrieve answers fast enough making the end user wait for the result. Choose the best option that gives the best performance with minimal maintenance effort:",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251747,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "e29d191f219a52d8c22d9176df0bcc33",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "81070b92-82c0-4f30-b17c-67cd96fe0fa5",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are appointed as a Data Engineer in a company that has a legacy reporting application written in Java which suffers from performance problems. The reporting application plots dashboards with daily refresh of key business indicators to help management take the best decisions. The application reads data directly from the source database of MongoDB, aggregates using simple counts and shows them visually in a UI. The performance problem of this application comes because the source database is at times overloaded and therefore the dashboard is not able to retrieve answers fast enough making the end user wait for the result. Choose the best option that gives the best performance with minimal maintenance effort:"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208589,
                            "question_id": 77740,
                            "option_text": "Since MongoDB is OLTP, it is not able to support business reporting. So, bringthe data into Hadoop end of day, and run OLAP queries on it from the same UI.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217116,
                            "option_image_url": null
                        },
                        {
                            "id": 208590,
                            "question_id": 77740,
                            "option_text": "Convert the application from using plain Java to using Spark Streaming in Java",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217117,
                            "option_image_url": null
                        },
                        {
                            "id": 208591,
                            "question_id": 77740,
                            "option_text": "Extract raw data from MongoDB using Change Data Capture (CDC) once everyminute into Kafka, and then use Spark Streaming to compute the KPIs and then populate into a NoSQL DB like Redis for the UI to consume.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217118,
                            "option_image_url": null
                        },
                        {
                            "id": 208592,
                            "question_id": 77740,
                            "option_text": "Query MongoDB every 1 minute for new data using a check on documentinserted timestamp, use Spark Streaming to compute the KPIs with the queried data, and then populate into a NoSQL DB like Redis for the UI to consume.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217119,
                            "option_image_url": null
                        },
                        {
                            "id": 208593,
                            "question_id": 77740,
                            "option_text": "Convert application to using Python along with a NoSQL database for storingand retrieving the aggregated counts.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217120,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77741,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 231,
                    "question_text_1": "Fielder on the midwicket boundary is wearing a smart watch. Unlike other smart watches, this one is unique in that it helps him field. Whenever the ball is headed his direction, the watch vibrates to alert him. When there is a chance of a catch by this fielder, the watch continually whispers every second to go forward, back, left or right thus improving his chances of settling under the ball and taking the catch. The watch is connected to the spider cam. The spider cam is itself a powerful ARM-based computer which has connectivity to the Cloud, an all-seeing AI-powered superbeing, through the wire on which it hangs. Using this connectivity, it can send as much or as little data as required and also receive instructions from the cloud. Your task is to design the data pipeline that enables such feedback to the fielder for every ball that comes his way with as much accuracy as possible throughout the match. You are given two ML models: a vision model that given a frame from the spider cam computes an alert if the ball is headed to him, and an adjustment model that helps adjust catch positioning of the fielder which takes as input a continuous video feed. Which of the following options best satisfies the requirements?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251748,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "9a33117d7dd0dae965e2b4d6588a0a25",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "f2a3985f-6195-46da-a0c7-84932417f859",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Fielder on the midwicket boundary is wearing a smart watch. Unlike other smart watches, this one is unique in that it helps him field. Whenever the ball is headed his direction, the watch vibrates to alert him. When there is a chance of a catch by this fielder, the watch continually whispers every second to go forward, back, left or right thus improving his chances of settling under the ball and taking the catch. The watch is connected to the spider cam. The spider cam is itself a powerful ARM-based computer which has connectivity to the Cloud, an all-seeing AI-powered superbeing, through the wire on which it hangs. Using this connectivity, it can send as much or as little data as required and also receive instructions from the cloud. Your task is to design the data pipeline that enables such feedback to the fielder for every ball that comes his way with as much accuracy as possible throughout the match. You are given two ML models: a vision model that given a frame from the spider cam computes an alert if the ball is headed to him, and an adjustment model that helps adjust catch positioning of the fielder which takes as input a continuous video feed. Which of the following options best satisfies the requirements?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208594,
                            "question_id": 77741,
                            "option_text": "Ingest all spider cam data including video feeds into Pub/Sub, process usingGoogle Cloud Dataflow where you invoke the vision model and the adjustment model, relaying this output to the spider cam continuously.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217121,
                            "option_image_url": null
                        },
                        {
                            "id": 208595,
                            "question_id": 77741,
                            "option_text": "Compress the vision model and adjustment model to fit into the spider cam\u2019savailable resource, and write pipelines to execute the models in the spider cam itself",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217122,
                            "option_image_url": null
                        },
                        {
                            "id": 208596,
                            "question_id": 77741,
                            "option_text": "Compress the vision model and adjustment model to fit into the spider cam\u2019savailable resource, and write pipelines to execute in the spider cam itself, with periodically data being sent to the cloud for retraining the vision model using Google Cloud ML and then redeploy the model to the spider cam.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217123,
                            "option_image_url": null
                        },
                        {
                            "id": 208597,
                            "question_id": 77741,
                            "option_text": "Compress the data in the spider cam every 5 seconds, write to Pub/Sub thecompressed data, invoke the vision model and, if needed, the adjustment model, and then write back output from Cloud to the spider cam to relay to the fielder.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217124,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77742,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 232,
                    "question_text_1": "  Suppose there\u2019s a new transformer model from Facebook that is orders of magnitude smaller than mobilenet_v2 and has a better F1 score. Will these lines of code still be required in the pipeline?",
                    "question_image_1": "HfjFIQnyg2KFUwtx68PqDUGnbTKUmXkCjKnKroXkTmTnHpXPwe.png",
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251749,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "4d908142c0971669c539bffaa306aba9",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "3ef98ff4-f1cc-4503-ac93-b384fdf27aa5",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/HfjFIQnyg2KFUwtx68PqDUGnbTKUmXkCjKnKroXkTmTnHpXPwe.png"
                    ],
                    "question_texts": [
                        "  Suppose there\u2019s a new transformer model from Facebook that is orders of magnitude smaller than mobilenet_v2 and has a better F1 score. Will these lines of code still be required in the pipeline?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208598,
                            "question_id": 77742,
                            "option_text": "Yes, since they check the syntax of the model function so that there are noerrors",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217125,
                            "option_image_url": null
                        },
                        {
                            "id": 208599,
                            "question_id": 77742,
                            "option_text": "Yes, they invoke PyTorch libraries that have already been setup for modelscoring on GCP using APIs embedded within the function",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217126,
                            "option_image_url": null
                        },
                        {
                            "id": 208600,
                            "question_id": 77742,
                            "option_text": "No, since the primary function of these lines of code is to eliminate repeatedDL model loads as DL models are large in size",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217127,
                            "option_image_url": null
                        },
                        {
                            "id": 208601,
                            "question_id": 77742,
                            "option_text": "Yes, they are Map-style UDFs that make it an embarrassingly parallelcomputation thus making the execution parallelized and fast.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217128,
                            "option_image_url": null
                        },
                        {
                            "id": 208602,
                            "question_id": 77742,
                            "option_text": "Yes, since the new model will potentially have large load times.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217129,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77743,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 233,
                    "question_text_1": "You are given a Spark Streaming pipeline that invokes a pre-trained DL model for every image it receives as input and produces the classification result in quick time. The model with the best recall rate from the PyTorch library runs in under 3 seconds on an average, when executing on a GPU-powered Spark cluster. Your management has now instructed you to reduce the cost of AI projects significantly, and has given guidance that latency of execution is not a concern since the consumer for the model output is batch oriented, and also that they would be ok with a lesser recall rate provided the drop isn\u2019t significant. What is the best option to explore first to meet the expectations?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251750,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "f036e72c9abcd9291ad71eaa88836582",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "aa58b63a-38b5-405f-b587-ac25764c02ae",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are given a Spark Streaming pipeline that invokes a pre-trained DL model for every image it receives as input and produces the classification result in quick time. The model with the best recall rate from the PyTorch library runs in under 3 seconds on an average, when executing on a GPU-powered Spark cluster. Your management has now instructed you to reduce the cost of AI projects significantly, and has given guidance that latency of execution is not a concern since the consumer for the model output is batch oriented, and also that they would be ok with a lesser recall rate provided the drop isn\u2019t significant. What is the best option to explore first to meet the expectations?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208603,
                            "question_id": 77743,
                            "option_text": "Build a custom model that compresses the highest recall rate model justenough to be able to execute within a single Spark worker and thus reduce cost of the cluster.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217130,
                            "option_image_url": null
                        },
                        {
                            "id": 208604,
                            "question_id": 77743,
                            "option_text": "Use a different DL model from PyTorch that has better efficiency at lesserrecall levels so that the cluster size can be minimized.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217131,
                            "option_image_url": null
                        },
                        {
                            "id": 208605,
                            "question_id": 77743,
                            "option_text": "Remove complexity associated with Spark Streaming, remove GPUs to savesignificant costs, and convert the model execution pipeline into a single threaded Python application using traditional ML running on a CPU-only machine.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217132,
                            "option_image_url": null
                        },
                        {
                            "id": 208606,
                            "question_id": 77743,
                            "option_text": "Change Spark machine to use CPUs and train a fresh pipeline to achieveobjectives.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217133,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77744,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 234,
                    "question_text_1": "Consider a Structured Streaming application running on Google Dataproc firing up every 10 seconds, consuming any number of records from Kafka available since last read, and emitting some computed answers to a NoSQL DB. Consider also that apart from the functional logic, the same application is also emitting into a file some log statements for debugging purposes meant for use by the developer of the application only in the event that something goes wrong but is otherwise not intelligible. \nAssume there is a failure in one of the Dataproc machines that results in a failure of a specific run. For anybody consuming the NoSQL DB outputs, will they see any change in output as a result of the failure at all, or will the only visible impact of failure be of slower performance for the failed-and-retried run?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251751,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "3f0e0ad5dbf7c80bad7ff72eb8dd1293",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "bc925a41-bd55-45fb-be58-7b48ea94d717",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Consider a Structured Streaming application running on Google Dataproc firing up every 10 seconds, consuming any number of records from Kafka available since last read, and emitting some computed answers to a NoSQL DB. Consider also that apart from the functional logic, the same application is also emitting into a file some log statements for debugging purposes meant for use by the developer of the application only in the event that something goes wrong but is otherwise not intelligible. \nAssume there is a failure in one of the Dataproc machines that results in a failure of a specific run. For anybody consuming the NoSQL DB outputs, will they see any change in output as a result of the failure at all, or will the only visible impact of failure be of slower performance for the failed-and-retried run?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208607,
                            "question_id": 77744,
                            "option_text": "No, the failure is not visible to the consumers of NoSQL DB outputs, asStructured Streaming retries the mini-batch that failed thus taking maybe twice as much time as normal.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217134,
                            "option_image_url": null
                        },
                        {
                            "id": 208608,
                            "question_id": 77744,
                            "option_text": "No, the failure is not visible since Structured Streaming uses transactions andidempotence to achieve exactly-once processing.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217135,
                            "option_image_url": null
                        },
                        {
                            "id": 208609,
                            "question_id": 77744,
                            "option_text": "No, the failure is not visible since Structured Streaming can process the samedata in a retry resulting in the same outputs again.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217136,
                            "option_image_url": null
                        },
                        {
                            "id": 208610,
                            "question_id": 77744,
                            "option_text": "Yes, the failure is visible because the side effect of logging for debugging willbe visible as repeated entries when Structured Streaming retries the failed batch.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217137,
                            "option_image_url": null
                        },
                        {
                            "id": 208611,
                            "question_id": 77744,
                            "option_text": "Yes, the failure is visible since the logs in the backend of Spark StructuredStreaming are also logging the state of the machine.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217138,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77745,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 235,
                    "question_text_1": "Let us say we are using structured streaming for continuously reading data from Kafka and storing the results back into a Kafka topic using window function aggregates. Due to various reasons, the job did not run for 1 month. Now, we need to again continue the runs without compromising on the correctness of the results. What can you do that will take the least effort?",
                    "question_image_1": null,
                    "question_type": "MCQ",
                    "total_mark": "2.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251752,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "42dc82595a677bf86e2a6dc9a2014655",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "8c39e65f-a5ad-4db9-a88c-93afabbcabd4",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Let us say we are using structured streaming for continuously reading data from Kafka and storing the results back into a Kafka topic using window function aggregates. Due to various reasons, the job did not run for 1 month. Now, we need to again continue the runs without compromising on the correctness of the results. What can you do that will take the least effort?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208612,
                            "question_id": 77745,
                            "option_text": "Code up a new batch processing job and process the 1 month of missed dataas a standalone job. Then, reactivate the structured streaming job once the latest date is caught up.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217139,
                            "option_image_url": null
                        },
                        {
                            "id": 208613,
                            "question_id": 77745,
                            "option_text": "Code up a new batch processing job and process the 1 month of missed dataas a standalone job. Then, reactivate the structured streaming job once the latest date is caught up. But copy the code over from the current structured streaming job where the read and write commands will remain the same, but the remaining code will need to be modified, as operations on streaming dataframes are not supported on static dataframes.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217140,
                            "option_image_url": null
                        },
                        {
                            "id": 208614,
                            "question_id": 77745,
                            "option_text": "Launch the structured streaming job using starting offset as that lastsuccessfully processed before the job went on hiatus, and launch in a streaming mode with a very high time frequency of repetition. This simulates a batch execution of the same logic so as to catch up for 1 month of data processing. Once done, relaunch the structured streaming using the earlier-used configuration parameters.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217141,
                            "option_image_url": null
                        },
                        {
                            "id": 208615,
                            "question_id": 77745,
                            "option_text": "Code up a new batch processing job and process the 1 month of missed dataas a standalone job. Then, reactivate the structured streaming job once the latest date is caught up.But copy the code over from the current structured streaming job where the read and write commands need to be modified to specify that it\u2019s a batch operation. Further, the specific logic of window functions will also need to be modified since there are no time windows anymore in batch processing.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217142,
                            "option_image_url": null
                        },
                        {
                            "id": 208616,
                            "question_id": 77745,
                            "option_text": "Code up a new batch processing job and process the 1 month of missed dataas a standalone job. Then, reactivate the structured streaming job once the latest date is caught up. But copy the code over from the current structured streaming job where only the read and write commands need to be modified to specify that it's a batch operation.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217143,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77746,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 236,
                    "question_text_1": "Observe the below image showing a possible implementation of Kafka or Google Pub/Sub and select the options that are true. \n",
                    "question_image_1": "N3IKT2dBVLyWc8zamVJb7CqFX5qKUFsWzZo8bBygd2ihIkqrVq.png",
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251753,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "dd725c5a9df2f31ccccda0269a373234",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": "Observe the below image showing a possible implementation of Kafka or Google Pub/Sub and select the options that are true. \n ",
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "0e2f879c-963b-491a-87bf-38ac02ddd0d7",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": [
                        "/question_images/N3IKT2dBVLyWc8zamVJb7CqFX5qKUFsWzZo8bBygd2ihIkqrVq.png"
                    ],
                    "question_texts": [
                        "Observe the below image showing a possible implementation of Kafka or Google Pub/Sub and select the options that are true. \n",
                        "Observe the below image showing a possible implementation of Kafka or Google Pub/Sub and select the options that are true. \n "
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208617,
                            "question_id": 77746,
                            "option_text": "Each of the Consumers 1,2,3 and 4 gets all messages published into Topic.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217144,
                            "option_image_url": null
                        },
                        {
                            "id": 208618,
                            "question_id": 77746,
                            "option_text": "Publisher 1 can safely send data only for Consumer Group 1\u2019s consumptionwhile preventing access to Consumer 3 & 4.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217145,
                            "option_image_url": null
                        },
                        {
                            "id": 208619,
                            "question_id": 77746,
                            "option_text": "Consumers 1 & 2 may not get every message published into Topic.",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217146,
                            "option_image_url": null
                        },
                        {
                            "id": 208620,
                            "question_id": 77746,
                            "option_text": "Each Publisher is able to have the guarantee that its messages are reliablystored in Topic and made available to every consumer / consumer group that needs it.",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217147,
                            "option_image_url": null
                        },
                        {
                            "id": 208621,
                            "question_id": 77746,
                            "option_text": "Consumer 3 gets all messages published into Topic.",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217148,
                            "option_image_url": null
                        },
                        {
                            "id": 208622,
                            "question_id": 77746,
                            "option_text": "Consumer Group 1 gets all messages published by Publisher 1.",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217149,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77747,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 237,
                    "question_text_1": "You are given the task of improving the performance of a Spark SQL program that is doing a simple count after a series of transformations. You suspect that the culprit is the main transformation job in the program. When you run EXPLAIN on that SQL, you see that Spark wrongly estimates that there are only 10 values for the key on which the main transformation job is hinged upon, whereas in reality the underlying data has a million values for that key. What actions would you perform from the below to increase the estimation accuracy?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251754,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "f255d9aa13bfeeb4981e3c4d756d3ab7",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "b804a629-503a-48b6-9653-f25bfc0a26b6",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "You are given the task of improving the performance of a Spark SQL program that is doing a simple count after a series of transformations. You suspect that the culprit is the main transformation job in the program. When you run EXPLAIN on that SQL, you see that Spark wrongly estimates that there are only 10 values for the key on which the main transformation job is hinged upon, whereas in reality the underlying data has a million values for that key. What actions would you perform from the below to increase the estimation accuracy?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208623,
                            "question_id": 77747,
                            "option_text": "Create all tables as external tables.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217150,
                            "option_image_url": null
                        },
                        {
                            "id": 208624,
                            "question_id": 77747,
                            "option_text": "Ensure cost based optimizer (CBO) is ON.",
                            "option_image": "",
                            "score": "1.333",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217151,
                            "option_image_url": null
                        },
                        {
                            "id": 208625,
                            "question_id": 77747,
                            "option_text": "Partition all tables on the same key on which the aggregate is happening.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217152,
                            "option_image_url": null
                        },
                        {
                            "id": 208626,
                            "question_id": 77747,
                            "option_text": "Run ANALYZE on all tables to ensure the right estimates are available to theoptimizer.",
                            "option_image": "",
                            "score": "1.333",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217153,
                            "option_image_url": null
                        },
                        {
                            "id": 208627,
                            "question_id": 77747,
                            "option_text": "Cache the table in a step with actions ahead of the SQL statement that is theculprit.",
                            "option_image": "",
                            "score": "1.333",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217154,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77748,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 238,
                    "question_text_1": "Which of the following are best practices associated with Streaming applications?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251755,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "901595ae602f30d8a1a32ef3cb83b7b0",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "65613e55-dedb-45f3-b98e-50b1206ae6be",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of the following are best practices associated with Streaming applications?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208628,
                            "question_id": 77748,
                            "option_text": "Use a message store that supports message replay so that no data is lost inprocessing.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217155,
                            "option_image_url": null
                        },
                        {
                            "id": 208629,
                            "question_id": 77748,
                            "option_text": "In order to get \u201cexactly once\u201d data processing, it is sufficient that the messagestore supports transactional semantics of all-or-nothing operations.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217156,
                            "option_image_url": null
                        },
                        {
                            "id": 208630,
                            "question_id": 77748,
                            "option_text": "Hadoop is best suited for executing Streaming applications.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217157,
                            "option_image_url": null
                        },
                        {
                            "id": 208631,
                            "question_id": 77748,
                            "option_text": "Use checkpointing when faced with mission-critical workloads that require100% accuracy.",
                            "option_image": "",
                            "score": "2.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217158,
                            "option_image_url": null
                        },
                        {
                            "id": 208632,
                            "question_id": 77748,
                            "option_text": "Handle state pollution by restarting the persistent store software periodically.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217159,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77749,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 239,
                    "question_text_1": "What are some capabilities common to both \u201cstreaming processing\u201d & \u201cbatch processing\u201d when using Spark for big data?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251756,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "94f4361a3648a95db54023a7ee3b9d7b",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "2581f93f-1928-45eb-98ef-d07291cae220",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "What are some capabilities common to both \u201cstreaming processing\u201d & \u201cbatch processing\u201d when using Spark for big data?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208633,
                            "question_id": 77749,
                            "option_text": "Batch operates on data that typically has a static schema but can adjust tochanges in schema, and the same syntax will also help in streaming mode",
                            "option_image": "",
                            "score": "1.333",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217160,
                            "option_image_url": null
                        },
                        {
                            "id": 208634,
                            "question_id": 77749,
                            "option_text": "Batch operates on a set of data elements taken together while streaming canalso operate on a set of data elements as determined by the window",
                            "option_image": "",
                            "score": "1.333",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217161,
                            "option_image_url": null
                        },
                        {
                            "id": 208635,
                            "question_id": 77749,
                            "option_text": "Batch processing can assume data as fully specified and complete whilestreaming cannot make that assumption",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217162,
                            "option_image_url": null
                        },
                        {
                            "id": 208636,
                            "question_id": 77749,
                            "option_text": "Batch processing is typically high latency while streaming processing isnecessary for real-time latencies",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217163,
                            "option_image_url": null
                        },
                        {
                            "id": 208637,
                            "question_id": 77749,
                            "option_text": "Batch can operate on massively large data sets, while streaming can alsooperate on massively large data but expressed as streams",
                            "option_image": "",
                            "score": "1.333",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217164,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                },
                {
                    "id": 77750,
                    "exam_id": 3,
                    "question_paper_id": 248,
                    "question_number": 240,
                    "question_text_1": "Which of the following is true?",
                    "question_image_1": null,
                    "question_type": "MSQ",
                    "total_mark": "4.00",
                    "value_start": null,
                    "value_end": null,
                    "created_at": "2025-04-17T15:51:53.000000Z",
                    "updated_at": "2025-04-17T15:51:53.000000Z",
                    "question_num_long": 6406531251757,
                    "answer_type": null,
                    "response_type": null,
                    "have_answers": 1,
                    "hash": "52dd5c4c3b734f954875b56a7fa7785c",
                    "course_id": 94,
                    "is_markdown": 0,
                    "question_text_2": null,
                    "question_image_2": null,
                    "question_image_3": null,
                    "question_image_4": null,
                    "question_image_5": null,
                    "question_image_7": null,
                    "question_image_8": null,
                    "question_image_9": null,
                    "question_image_10": null,
                    "parent_question_id": null,
                    "question_text_3": null,
                    "question_text_4": null,
                    "question_text_5": null,
                    "uuid": "de28dd0b-b95b-4de9-a1c6-3a663c8c87de",
                    "solutions_count": 0,
                    "comments_count": 0,
                    "question_image_url": null,
                    "question_texts": [
                        "Which of the following is true?"
                    ],
                    "course": {
                        "id": 94,
                        "course_name": "Intro to Big Data",
                        "course_code": "Intro to Big Data",
                        "created_at": "2024-11-27T07:27:23.000000Z",
                        "updated_at": "2024-11-27T07:27:23.000000Z",
                        "program_id": 1,
                        "uuid": "44039058-a2ca-424f-a748-bbc56b22992a"
                    },
                    "options": [
                        {
                            "id": 208638,
                            "question_id": 77750,
                            "option_text": "Subscribers to Google Pub/Sub can decide at what rate they want to consumedata without depending on or informing the publisher of the data.",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217165,
                            "option_image_url": null
                        },
                        {
                            "id": 208639,
                            "question_id": 77750,
                            "option_text": "Snapshots of source systems can be created using an event capture tool likeCDC and then replaying the events in sequence for that time period.",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217166,
                            "option_image_url": null
                        },
                        {
                            "id": 208640,
                            "question_id": 77750,
                            "option_text": "A shared-memory algorithm for a ML problem is one where the algorithmassumes that the memory available to it is shared with multiple other processes that run on the same machine.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217167,
                            "option_image_url": null
                        },
                        {
                            "id": 208641,
                            "question_id": 77750,
                            "option_text": "A data lake is a collection of data to be provided as input for data sciencealgorithms.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217168,
                            "option_image_url": null
                        },
                        {
                            "id": 208642,
                            "question_id": 77750,
                            "option_text": "Zookeeper is a library for ensuring that critical services used in big data areable to stay in sync with each other as to the health of those services.",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217169,
                            "option_image_url": null
                        },
                        {
                            "id": 208643,
                            "question_id": 77750,
                            "option_text": "Distributed model training is not practical since reducing estimation errorsrequires constant exchange of enormous information across the machines in the cluster.",
                            "option_image": "",
                            "score": "0.000",
                            "is_correct": 0,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217170,
                            "option_image_url": null
                        },
                        {
                            "id": 208644,
                            "question_id": 77750,
                            "option_text": "Spark has support for declarative programming, functional programming aswell as procedural programming",
                            "option_image": "",
                            "score": "1.000",
                            "is_correct": 1,
                            "created_at": "2025-04-17T15:51:53.000000Z",
                            "updated_at": "2025-04-17T15:51:53.000000Z",
                            "option_number": 6406534217171,
                            "option_image_url": null
                        }
                    ],
                    "parent_question": null
                }
            ]
        },
        "summary": "Here\u2019s a structured **summary of core topics, concepts, principles, and equations** essential for the **\"Introduction to Big Data\"** exam, followed by **Mermaid knowledge graphs** to visualize relationships.\n\n---\n\n---\n\n### **Core Topics & Concepts**\n#### **1. Big Data Architectures & Ecosystems**\n- **Batch vs. Streaming Processing**:\n  - Batch: Static schema, high latency, complete data (e.g., Spark SQL, Hadoop).\n  - Streaming: Dynamic schema, low latency, windowed data (e.g., Spark Structured Streaming, Kafka).\n  - *Common Capabilities*: Schema flexibility, large-scale data handling, windowed aggregations.\n- **IaaS vs. PaaS**:\n  - **IaaS (Infrastructure as a Service)**: Manual management (e.g., VMs, Hadoop clusters).\n  - **PaaS (Platform as a Service)**: Managed services (e.g., Google Dataflow, Vertex AI, Pub/Sub).\n  - *Order of Increasing PaaS*: `(i) Hadoop VMs < (v) Spark on Hadoop < (iii) Dataproc < (iv) Dataflow < (ii) Vertex AI`.\n\n- **Key Components**:\n  - **Message Brokers**: Kafka, Pub/Sub (replayability, exactly-once semantics).\n  - **Stream Processing**: Spark Structured Streaming (output modes: `Complete`, `Update`, `Append`).\n  - **Storage**: GCS (Google Cloud Storage), Hadoop HDFS, NoSQL (Redis, HBase).\n  - **Orchestration**: Kubernetes, Dataproc, Dataflow.\n\n#### **2. Spark Optimization**\n- **Performance Bottlenecks**:\n  - **Skewed Joins**: Broadcast small DataFrames (`broadcast` hint).\n  - **Partitioning**: Repartition data to avoid shuffles (e.g., `repartition` on join keys).\n  - **Caching**: Cache frequently used DataFrames (`df.cache()`).\n  - **Query Optimization**:\n    - Enable **Cost-Based Optimizer (CBO)** (`spark.sql.cbo.enabled=true`).\n    - Run `ANALYZE TABLE` to update statistics.\n    - Use `EXPLAIN` to debug query plans.\n- **Structured Streaming**:\n  - **Output Modes**:\n    - `Complete`: Full aggregates (e.g., for dashboards).\n    - `Update`: Only changed rows.\n    - `Append`: New rows only.\n  - **Fault Tolerance**:\n    - Checkpointing for stateful operations.\n    - Exactly-once semantics with Kafka + Spark (idempotent sinks).\n\n#### **3. Kafka & Pub/Sub**\n- **Scaling**:\n  - Increase **brokers** (not just partitions/disks) for throughput.\n  - Partitions \u2265 consumers in a group for parallelism.\n- **Consumer Groups**:\n  - Messages are **partitioned** across consumers in a group (not all consumers get all messages).\n  - **Guarantees**: Publishers \u2192 Topic \u2192 Consumer Group \u2192 Consumers (reliable storage, no publisher-consumer coupling).\n- **Failure Handling**:\n  - Spark Streaming retries failed batches (no data loss, but latency increases).\n  - Kafka offsets track progress (exactly-once with transactional writes).\n\n#### **4. Data Pipelines & ETL**\n- **Real-Time Dashboards**:\n  - **Pattern**: Kafka \u2192 Spark Streaming (`Complete` mode) \u2192 Visualization (e.g., 1-minute refreshes).\n  - **Time Handling**:\n    - **Event Time**: When data was generated (e.g., sales timestamp).\n    - **Processing Time**: When data is processed (e.g., 3 AM Sunday).\n    - **Business Time**: Aligns with operational schedules (e.g., weekend definitions).\n- **Legacy Migration**:\n  - **CDC (Change Data Capture)**: Capture DB changes \u2192 Stream to Kafka/Pub/Sub.\n  - **Lambda Architecture**: Batch (Hadoop) + Streaming (Spark) layers.\n\n#### **5. Machine Learning in Big Data**\n- **Model Serving**:\n  - **Edge vs. Cloud**:\n    - Edge (e.g., spider cam): Low latency, limited resources (model compression).\n    - Cloud (e.g., Vertex AI): Scalable training, high latency.\n  - **Optimizations**:\n    - Replace GPUs with CPUs if latency is not critical.\n    - Use smaller models (e.g., Facebook\u2019s tiny transformer vs. MobileNetV2).\n- **UDFs (User-Defined Functions)**:\n  - Avoid repeated model loads in Spark UDFs (cache models in memory).\n\n#### **6. Time & State Management**\n- **Watermarks & Windows**:\n  - Handle late data in streaming (e.g., 5-minute tumbling windows).\n- **Stateful Processing**:\n  - Checkpoint state for fault tolerance (e.g., Redis for KPIs).\n- **Time Zones & Scheduling**:\n  - Adjust ETL schedules for business time changes (e.g., Middle East \u2192 India).\n\n#### **7. Cost Optimization**\n- **Trade-offs**:\n  - **Latency vs. Cost**: Batch processing (cheaper) vs. streaming (real-time).\n  - **Hardware**: GPUs (expensive) vs. CPUs (cheaper for non-latency-critical tasks).\n- **Right-Sizing**:\n  - Reduce cluster size by optimizing queries (e.g., broadcast joins).\n\n---\n\n### **Key Equations & Metrics**\n1. **Spark Join Optimization**:\n   - **Broadcast Join Threshold**: `spark.sql.autoBroadcastJoinThreshold` (default: 10MB).\n   - **Shuffle Partition Size**: Aim for 100\u2013200MB per partition.\n     ```\n     spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n     ```\n\n2. **Kafka Throughput**:\n   - **Partitions per Topic**: `\u2265` number of consumers in a group.\n   - **Broker Scaling**: Add brokers (not just disks) for linear throughput growth.\n\n3. **Streaming Latency**:\n   - **Watermark**: `eventTime - threshold` (e.g., 5 minutes for late data).\n   - **Trigger Interval**: `processingTime = \"5 minutes\"` (Spark Structured Streaming).\n\n4. **Cost Metrics**:\n   - **Dataproc vs. Dataflow**:\n     - Dataproc: Pay for VMs (IaaS).\n     - Dataflow: Pay per use (PaaS, serverless).\n\n---\n\n### **Mermaid Knowledge Graphs**\n#### **1. Big Data Processing Ecosystem**\n```mermaid\ngraph TD\n    A[Data Sources] -->|Streaming| B[Kafka/PubSub]\n    A -->|Batch| C[GCS/HDFS]\n    B --> D[Spark Streaming]\n    C --> E[Spark Batch]\n    D --> F[Stateful Processing\\nCheckpoints]\n    D --> G[Output Modes:\\nComplete/Update/Append]\n    E --> H[Aggregations\\nSQL/DataFrames]\n    F --> I[NoSQL DB\\nRedis/HBase]\n    G --> I\n    H --> I\n    I --> J[Dashboards/ML]\n```\n\n#### **2. Kafka Consumer Groups**\n```mermaid\ngraph LR\n    P1[Publisher 1] --> T[Topic]\n    P2[Publisher 2] --> T\n    T --> CG[Consumer Group 1]\n    CG --> C1[Consumer 1]\n    CG --> C2[Consumer 2]\n    CG --> C3[Consumer 3]\n    CG --> C4[Consumer 4]\n```\n\n#### **3. IaaS vs. PaaS Spectrum**\n```mermaid\ngraph LR\n    I1[(i) Hadoop VMs] --> I2[(v) Spark on Hadoop]\n    I2 --> I3[(iii) Dataproc]\n    I3 --> I4[(iv) Dataflow]\n    I4 --> I5[(ii) Vertex AI]\n    style I1 fill:#f9f,stroke:#333\n    style I5 fill:#bbf,stroke:#333\n```\n\n#### **4. Spark Optimization Flow**\n```mermaid\ngraph TD\n    A[Slow Query] --> B[Run EXPLAIN]\n    B --> C{Estimate Accurate?}\n    C -->|No| D[Run ANALYZE TABLE]\n    C -->|No| E[Enable CBO]\n    C -->|No| F[Cache DataFrame]\n    D --> G[Re-run Query]\n    E --> G\n    F --> G\n    C -->|Yes| H[Check Joins]\n    H --> I[Broadcast Small Tables]\n    H --> J[Repartition Data]\n```\n\n#### **5. Real-Time Recommendation Pipeline**\n```mermaid\ngraph TD\n    A[PoS Data] --> B[Kafka/PubSub]\n    B --> C[Spark Streaming/Dataflow]\n    C --> D[ML Model\\nVertex AI/Python VM]\n    D --> E[Redis/NoSQL]\n    E --> F[Dashboard]\n    G[Spider Cam] --> H[Edge Model\\nVision/Adjustment]\n    H --> G\n```\n\n---\n\n### **Exam Tips**\n1. **Spark Streaming**:\n   - Use `Complete` mode for dashboards (full aggregates).\n   - `Update` mode for incremental updates.\n   - `Append` for new rows only (e.g., logs).\n\n2. **Kafka**:\n   - Scaling: Add **brokers** (not just partitions).\n   - Consumer groups **distribute** messages (not replicate).\n\n3. **Optimization**:\n   - Broadcast small tables (`< 10MB`).\n   - Cache repeated DataFrames.\n   - Use CBO + `ANALYZE`.\n\n4. **Fault Tolerance**:\n   - Checkpointing for stateful ops.\n   - Kafka offsets ensure exactly-once with idempotent sinks.\n\n5. **Cost**:\n   - PaaS (Dataflow/Vertex AI) > IaaS (Dataproc/Hadoop).\n   - CPUs > GPUs for non-latency-critical tasks.",
        "total_score": "50.00"
    },
    "url": "/question-paper/practise/94/fb4337ae-d12",
    "version": "ee3d5d44299e610bd137ea6200db5ac2"
}